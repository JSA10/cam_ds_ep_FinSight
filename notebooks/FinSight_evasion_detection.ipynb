{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc758a1",
   "metadata": {},
   "source": [
    "# **Evasion Detection Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d970815",
   "metadata": {},
   "source": [
    "# **1. Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1387398",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to contain the evasion detection pipeline. \n",
    "1. **Baseline Evasion score** (rule-based) is made up of three components:\n",
    "- **Cosine similarity**- similarity of the question and answer, lower similarity = more evasive\n",
    "- **Numeric specificity check**- does the question require a number, if so does the answer contain a number?, e.g. requests for financial data\n",
    "- **Evasive phrases**- does the answer contain evasive phrases?, presence = more evasive\n",
    "\n",
    "2. **LLM evasion score** (RoBERTa-MNLI) uses entailment/neutral/contradiction between the question and answer\n",
    "- Lower entailment (and higher neutral + contradiction) = more evasive\n",
    "  \n",
    "3. **Blended evasion score** combines both scores including a weight for the LLM component\n",
    "- Rationale is that baseline enforces precision while the LLM will capture semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8372746",
   "metadata": {},
   "source": [
    "# **1. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431bddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import transformers, datasets, inspect\n",
    "from llama_cpp import Llama \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score \n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set SEED.\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d4ae2",
   "metadata": {},
   "source": [
    "# **2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30edeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "1            qa              NaN            NaN  Steven Chubak   \n",
       "2            qa              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "1                             analyst    Wolfe Research LLC   \n",
       "2             Chief Financial Officer  JPMorgan Chase & Co.   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "1                                 Hey, good morning.  2023      Q1   \n",
       "2                               Good morning, Steve.  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "\n",
       "   is_pleasantry                                         source_pdf  \n",
       "0          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "1           True  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "2           True  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "3          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "4          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1411\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "all_jpm_2023_2025 = pd.read_csv('../data/processed/jpm/all_jpm_2023_2025.csv')\n",
    "\n",
    "# View dataset.\n",
    "display(all_jpm_2023_2025.head())\n",
    "\n",
    "# Number of rows.\n",
    "print('Number of rows:', all_jpm_2023_2025.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7647de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1241\n"
     ]
    }
   ],
   "source": [
    "# Remove pleasantries.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025[all_jpm_2023_2025['is_pleasantry'] == False]\n",
    "print('Number of rows:', all_jpm_2023_2025_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b9e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 23\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025_cleaned['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31340c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025_cleaned.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c042a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 0\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025_cleaned['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2752faaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay',\n",
       "       \"We're fundamentally\", 'Thanks', 'Almost no chance.'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View roles.\n",
    "all_jpm_2023_2025_cleaned['role'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc3188",
   "metadata": {},
   "source": [
    "- Some text has leaked into role column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0d690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>qa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Who knows how important politics are in all th...</td>\n",
       "      <td>We're fundamentally</td>\n",
       "      <td>as I said, I think on the press call, happy to...</td>\n",
       "      <td>little bit cautious about the pull-forward dyn...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-1q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>qa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Chief Financial Officer, JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Thanks</td>\n",
       "      <td>Glenn.</td>\n",
       "      <td>Operator: Next, we'll go to the line of Matt O...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer, JPMorgan C...</td>\n",
       "      <td>Almost no chance.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Well, but having – it's very important. While ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q3</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-3q24-earnings-conference-call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     section  question_number  answer_number  \\\n",
       "305       qa             22.0            4.0   \n",
       "309       qa             23.0            3.0   \n",
       "650       qa             10.0            3.0   \n",
       "924       qa              8.0            2.0   \n",
       "1059      qa             22.0            4.0   \n",
       "1063      qa             23.0            3.0   \n",
       "1274      qa             23.0            1.0   \n",
       "\n",
       "                                           speaker_name  \\\n",
       "305              Chief Financial Officer, JPMorganChase   \n",
       "309              Chief Financial Officer, JPMorganChase   \n",
       "650   Who knows how important politics are in all th...   \n",
       "924       Chief Financial Officer, JPMorgan Chase & Co.   \n",
       "1059             Chief Financial Officer, JPMorganChase   \n",
       "1063             Chief Financial Officer, JPMorganChase   \n",
       "1274  Chairman & Chief Executive Officer, JPMorgan C...   \n",
       "\n",
       "                                             role  \\\n",
       "305   And then some. Theres a lot of value added.   \n",
       "309                                          Okay   \n",
       "650                           We're fundamentally   \n",
       "924                                        Thanks   \n",
       "1059  And then some. Theres a lot of value added.   \n",
       "1063                                         Okay   \n",
       "1274                            Almost no chance.   \n",
       "\n",
       "                                                company  \\\n",
       "305                                       JPMorganChase   \n",
       "309                                  there you have it.   \n",
       "650   as I said, I think on the press call, happy to...   \n",
       "924                                              Glenn.   \n",
       "1059                                      JPMorganChase   \n",
       "1063                                 there you have it.   \n",
       "1274                                      JPMorganChase   \n",
       "\n",
       "                                                content  year quarter  \\\n",
       "305   Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "309   But it's not like I thought it would do badly,...  2025      Q2   \n",
       "650   little bit cautious about the pull-forward dyn...  2024      Q1   \n",
       "924   Operator: Next, we'll go to the line of Matt O...  2024      Q2   \n",
       "1059  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "1063  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "1274  Well, but having – it's very important. While ...  2024      Q3   \n",
       "\n",
       "      is_pleasantry                                         source_pdf  \n",
       "305           False  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...  \n",
       "309           False  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...  \n",
       "650           False  data/raw/jpm/jpm-1q24-earnings-call-transcript...  \n",
       "924           False  data/raw/jpm/jpm-2q24-earnings-call-transcript...  \n",
       "1059          False  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "1063          False  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "1274          False  data/raw/jpm/jpm-3q24-earnings-conference-call...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles. \n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = all_jpm_2023_2025_cleaned[~all_jpm_2023_2025_cleaned['role'].isin(valid_roles)]\n",
    "invalid_roles_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2103540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the correct role information.\n",
    "all_jpm_2023_2025_cleaned.loc[[305, 309, 924, 1059, 1063], 'role'] = 'Chief Financial Officer'\n",
    "all_jpm_2023_2025_cleaned.loc[[1274], 'role'] = 'Chairman & Chief Executive Officer'\n",
    "\n",
    "# Drop nonsence row.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025_cleaned.drop(index=650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80ad9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the roles have been updated.\n",
    "all_jpm_2023_2025_cleaned['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ac15ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise role names.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Map roles.\n",
    "all_jpm_2023_2025_cleaned['role_normalised'] = all_jpm_2023_2025_cleaned['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "829a8e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "5            qa              1.0            1.0  Steven Chubak   \n",
       "6            qa              1.0            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "5                             analyst    Wolfe Research LLC   \n",
       "6  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "5  Got it. And just in terms of appetite for the ...  2023      Q1   \n",
       "6                                          Oh, yeah.  2023      Q1   \n",
       "\n",
       "   is_pleasantry                                         source_pdf  \\\n",
       "0          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "3          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "4          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "5          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "6          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "\n",
       "  role_normalised  \n",
       "0          banker  \n",
       "3         analyst  \n",
       "4          banker  \n",
       "5         analyst  \n",
       "6          banker  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1217\n"
     ]
    }
   ],
   "source": [
    "# View dataset.\n",
    "display(all_jpm_2023_2025_cleaned.head())\n",
    "print('Number of rows:', all_jpm_2023_2025_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8054baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset.\n",
    "all_jpm_2023_2025_cleaned.to_csv('../data/processed/jpm/cleaned/all_jpm_2023_2025_cleaned') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6934907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to remove duplicates within questions and answers. \n",
    "def clean_repeats(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # 1) Normalize whitespace\n",
    "    t = ' '.join(text.split()).strip()\n",
    "    if not t:\n",
    "        return t\n",
    "\n",
    "    # 2) If the whole-string is a back-to-back duplicate (A+A) = keep first half\n",
    "    mid = len(t) // 2\n",
    "    if len(t) % 2 == 0 and t[:mid] == t[mid:]:\n",
    "        t = t[:mid]\n",
    "\n",
    "    # 3) Collapse immediate repeated token spans (n-grams)\n",
    "    toks = t.split()\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        matched = False\n",
    "        max_span = min(50, len(toks) - i)  # cap span to remaining length\n",
    "        for n in range(max_span, 4, -1):  # try longer spans first: 50..5\n",
    "            if i + 2*n <= len(toks) and toks[i:i+n] == toks[i+n:i+2*n]:\n",
    "                out.extend(toks[i:i+n])  # keep one copy\n",
    "                i += 2*n                # skip the duplicate block\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            out.append(toks[i])\n",
    "            i += 1\n",
    "    t = ' '.join(out)\n",
    "\n",
    "    # 4) Remove duplicate sentences globally (order-preserving)\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', t)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for s in sents:\n",
    "        s_norm = s.strip()\n",
    "        if not s_norm:\n",
    "            continue\n",
    "        key = ' '.join(s_norm.lower().split())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(s_norm)\n",
    "    return ' '.join(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c551bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert datasets into question and answer pairs.\n",
    "def create_qa_pairs(df, min_answer_words=30):\n",
    "    # Keep only the Q&A section.\n",
    "    qa_df = df[df['section'].astype(str).str.lower() == 'qa'].copy()\n",
    "\n",
    "    # Split into roles.\n",
    "    analyst_rows = qa_df[qa_df['role_normalised'] == 'analyst'].copy()\n",
    "    banker_rows  = qa_df[qa_df['role_normalised'] == 'banker' ].copy()\n",
    "\n",
    "    # Keys to keep quarters separated\n",
    "    key_q = ['year', 'quarter', 'question_number']\n",
    "\n",
    "    # Build full question text per (year, quarter, question_number)\n",
    "    question_text_map = (\n",
    "        analyst_rows\n",
    "        .groupby(key_q, dropna=False)['content']\n",
    "        .apply(lambda parts: clean_repeats(' '.join(parts.astype(str))))\n",
    "        .rename('question')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Ensure bankers have an answer_number — sequential per (year, quarter, question_number) if missing\n",
    "    if 'answer_number' not in banker_rows.columns or banker_rows['answer_number'].isna().any():\n",
    "        banker_rows = banker_rows.sort_index().copy()\n",
    "        banker_rows['answer_number'] = (\n",
    "            banker_rows\n",
    "            .groupby(key_q, dropna=False)\n",
    "            .cumcount() + 1\n",
    "        )\n",
    "\n",
    "    # Combine multiple banker utterances belonging to the same answer\n",
    "    banker_answers = (\n",
    "        banker_rows\n",
    "        .groupby(key_q + ['answer_number'], dropna=False)\n",
    "        .agg({\n",
    "            'content':        lambda parts: clean_repeats(' '.join(parts.astype(str))),\n",
    "            'speaker_name':   'first',\n",
    "            'role':           'first',\n",
    "            'role_normalised':'first',\n",
    "            'source_pdf':     'first'\n",
    "        })\n",
    "        .rename(columns={'content': 'answer'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge question text back onto each answer row\n",
    "    qa_pairs = banker_answers.merge(\n",
    "        question_text_map,\n",
    "        on=key_q,\n",
    "        how='left',\n",
    "        validate='many_to_one'\n",
    "    )\n",
    "\n",
    "    # Order columns for readability\n",
    "    column_order = [\n",
    "        'year', 'quarter', 'question_number', 'answer_number',\n",
    "        'question', 'answer',\n",
    "        'speaker_name', 'role', 'role_normalised',\n",
    "        'source_pdf'\n",
    "    ]\n",
    "    qa_pairs = qa_pairs.reindex(columns=[c for c in column_order if c in qa_pairs.columns])\n",
    "\n",
    "    # Sort and reset index.\n",
    "    qa_pairs = qa_pairs.sort_values(['year', 'quarter', 'question_number', 'answer_number']).reset_index(drop=True)\n",
    "\n",
    "    # Drop duplicate answers.\n",
    "    qa_pairs = qa_pairs.drop_duplicates(subset=['answer'])\n",
    "\n",
    "    # Drop short answers below threshold to ensure quality answers.\n",
    "    qa_pairs = qa_pairs[qa_pairs['answer'].astype(str).str.split().str.len() >= int(min_answer_words)]\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8425a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create q&A pairs.\n",
    "all_jpm_2023_2025_qa = create_qa_pairs(all_jpm_2023_2025_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b82548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 309\n"
     ]
    }
   ],
   "source": [
    "# View number of examples.\n",
    "print('Number of examples:', all_jpm_2023_2025_qa.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e070042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into prediction set and validation/training/test set.\n",
    "jpm_2025_predict_qa = all_jpm_2023_2025_qa[all_jpm_2023_2025_qa['year'] == 2025]\n",
    "jpm_2023_2024_qa = all_jpm_2023_2025_qa[all_jpm_2023_2025_qa['year'].isin([2023, 2024])]\n",
    "\n",
    "# Save the datasets.\n",
    "jpm_2025_predict_qa.to_csv('../data/processed/jpm/cleaned/jpm_2025_predict_qa.csv') \n",
    "jpm_2023_2024_qa.to_csv('../data/processed/jpm/cleaned/jpm_2023_2024_qa.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c481b",
   "metadata": {},
   "source": [
    "The jpm_2023_2024_qa dataset was then manually labelled according to whether the banker's answer was deemed 'Direct' or 'Evasive'. The label was appended by a new column 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa178cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>role_normalised</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good morning. Thanks for all the comments on t...</td>\n",
       "      <td>Yeah. Matt, not particularly updating. I think...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then just separately, you bought bac...</td>\n",
       "      <td>Yeah. Good question. And I think you framed it...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thanks. Jeremy, could you give a little more c...</td>\n",
       "      <td>Yeah. Actually, John, this quarter, that's all...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then, just to follow up on the NII, ...</td>\n",
       "      <td>Sure. Yeah, happy to do that, John. So, I thin...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hey. Good morning. Maybe just to follow up in ...</td>\n",
       "      <td>Yeah. Both good questions. So let's do reprice...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year quarter  question_number  answer_number  \\\n",
       "0  2023      Q4              1.0            1.0   \n",
       "1  2023      Q4              2.0            1.0   \n",
       "2  2023      Q4              3.0            1.0   \n",
       "3  2023      Q4              4.0            1.0   \n",
       "4  2023      Q4              5.0            1.0   \n",
       "\n",
       "                                            question  \\\n",
       "0  Good morning. Thanks for all the comments on t...   \n",
       "1  Okay. And then just separately, you bought bac...   \n",
       "2  Thanks. Jeremy, could you give a little more c...   \n",
       "3  Okay. And then, just to follow up on the NII, ...   \n",
       "4  Hey. Good morning. Maybe just to follow up in ...   \n",
       "\n",
       "                                              answer   speaker_name  \\\n",
       "0  Yeah. Matt, not particularly updating. I think...  Jeremy Barnum   \n",
       "1  Yeah. Good question. And I think you framed it...  Jeremy Barnum   \n",
       "2  Yeah. Actually, John, this quarter, that's all...  Jeremy Barnum   \n",
       "3  Sure. Yeah, happy to do that, John. So, I thin...  Jeremy Barnum   \n",
       "4  Yeah. Both good questions. So let's do reprice...  Jeremy Barnum   \n",
       "\n",
       "                      role role_normalised  \\\n",
       "0  Chief Financial Officer          banker   \n",
       "1  Chief Financial Officer          banker   \n",
       "2  Chief Financial Officer          banker   \n",
       "3  Chief Financial Officer          banker   \n",
       "4  Chief Financial Officer          banker   \n",
       "\n",
       "                                          source_pdf   label  \n",
       "0  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "1  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "2  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "3  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "4  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the labelled dataset.\n",
    "jpm_2023_2024_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_2023_2024_qa_labelled.csv')\n",
    "\n",
    "# View the dataset.\n",
    "jpm_2023_2024_qa_labelled = jpm_2023_2024_qa_labelled.drop('Unnamed: 0', axis=1)\n",
    "jpm_2023_2024_qa_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a43aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split into test, training and validation datasets, preserve number of evasive cases per set.\n",
    "def train_val_test(df, group_key, test_fraction, val_fraction, random_state):\n",
    "\n",
    "    # Split test from full data.\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_fraction, random_state=random_state)\n",
    "    idx_trainval, idx_test = next(gss1.split(df, groups=df[group_key]))\n",
    "    train_and_val = df.iloc[idx_trainval].reset_index(drop=True)\n",
    "    test_set = df.iloc[idx_test].reset_index(drop=True)\n",
    "\n",
    "    # Split VAL from the remaining data (val is relative to full size)\n",
    "    val_fraction_of_remaining = val_fraction / (1.0 - test_fraction)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_fraction_of_remaining, random_state=random_state + 1)\n",
    "    idx_train, idx_val = next(gss2.split(train_and_val, groups=train_and_val[group_key]))\n",
    "    train_set = train_and_val.iloc[idx_train].reset_index(drop=True)\n",
    "    val_set = train_and_val.iloc[idx_val].reset_index(drop=True)\n",
    "\n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ba04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a group key so answers for the same question are not split between datasets.\n",
    "jpm_2023_2024_qa_labelled['group_key'] = (\n",
    "    jpm_2023_2024_qa_labelled[\"year\"].astype(str) + \"_\" +\n",
    "    jpm_2023_2024_qa_labelled[\"quarter\"].astype(str) + \"_\" +\n",
    "    jpm_2023_2024_qa_labelled[\"question_number\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d87dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test, training and validation datasets.\n",
    "jpm_train, jpm_val, jpm_test = train_val_test(\n",
    "    jpm_2023_2024_qa_labelled,\n",
    "    group_key='group_key',\n",
    "    test_fraction=0.30,\n",
    "    val_fraction=0.20,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf10685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 107 (evasive: 22)\n",
      "Number of validation examples: 43 (evasive: 11)\n",
      "Number of test examples: 65 (evasive: 9)\n"
     ]
    }
   ],
   "source": [
    "# View the split. \n",
    "print(f'Number of training examples: {jpm_train.shape[0]} (evasive: {jpm_train[jpm_train[\"label\"] == \"Evasive\"].shape[0]})')\n",
    "print(f'Number of validation examples: {jpm_val.shape[0]} (evasive: {jpm_val[jpm_val[\"label\"] == \"Evasive\"].shape[0]})')\n",
    "print(f'Number of test examples: {jpm_test.shape[0]} (evasive: {jpm_test[jpm_test[\"label\"] == \"Evasive\"].shape[0]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0adfe513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets.\n",
    "jpm_train.to_csv('../data/processed/jpm/cleaned/jpm_train.csv') \n",
    "jpm_val.to_csv('../data/processed/jpm/cleaned/jpm_val.csv') \n",
    "jpm_test.to_csv('../data/processed/jpm/cleaned/jpm_test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd7965",
   "metadata": {},
   "source": [
    "# **3. Rule-based Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698b7be",
   "metadata": {},
   "source": [
    "## **3.1 Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f012907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of evasive phrases\n",
    "EVASIVE_PHRASES = [\n",
    "    r\"\\btoo early\\b\",\n",
    "    r\"\\bcan't (?:comment|share|discuss)\\b\",\n",
    "    r\"\\bwon't (?:comment|share|provide)\\b\",\n",
    "    r\"\\bno (?:update|comment)\\b\",\n",
    "    r\"\\bwe (?:don't|do not) (?:break out|provide guidance)\\b\",\n",
    "    r\"\\bnot (?:going to|able to) (?:comment|share|provide)\\b\",\n",
    "    r\"\\bwe'll (?:come back|circle back)\\b\",\n",
    "    r\"\\bnot something we disclose\\b\",\n",
    "    r\"\\bas (?:we|I) (?:said|mentioned)\\b\",\n",
    "    r\"\\bgenerally speaking\\b\",\n",
    "    r\"\\bit's premature\\b\",\n",
    "    r\"\\bit's difficult to say\\b\",\n",
    "    r\"\\bI (?:wouldn't|won't) want to (?:speculate|get into)\\b\",\n",
    "    r\"\\bI (?:think|guess|suppose)\\b\",\n",
    "    r\"\\bkind of\\b\",\n",
    "    r\"\\bsort of\\b\",\n",
    "    r\"\\baround\\b\",\n",
    "    r\"\\broughly\\b\",\n",
    "    r\"\\bwe (?:prefer|plan) not to\\b\",\n",
    "    r\"\\bwe're not prepared to\\b\",\n",
    "]\n",
    "\n",
    "# List of words that suggest the answer needs specific financial numbers to properly answer the question.\n",
    "SPECIFICITY_TRIGGERS = [\n",
    "    \"how much\",\"how many\",\"what is\",\"what are\",\"when\",\"which\",\"where\",\"who\",\"why\",\n",
    "    \"range\",\"guidance\",\"margin\",\"capex\",\"opex\",\"revenue\",\"sales\",\"eps\",\"ebitda\",\n",
    "    \"timeline\",\"date\",\"target\",\"growth\",\"update\",\"split\",\"dividend\",\"cost\",\"price\",\n",
    "    \"units\",\"volumes\",\"gross\",\"net\",\"tax\",\"percentage\",\"utilization\",\"order book\"\n",
    "]\n",
    "\n",
    "NUMERIC_PATTERN = r\"(?:\\d+(?:\\.\\d+)?%|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b|£|\\$|€)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4585b1",
   "metadata": {},
   "source": [
    "## **3.2 Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59f8212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between question and answers.\n",
    "def cosine_sim(q, a):\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform([q, a]) # converts text to vectors \n",
    "    sim = float(cosine_similarity(vec[0], vec[1])[0, 0]) # calculate the cosine similarity between the two vectors\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38d819de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute baseline evasion score.\n",
    "def baseline_evasion_score(q, a):\n",
    "    # 1. Cosine similarity\n",
    "    sim = cosine_sim(q, a) # calculates cosine similarity using previous function\n",
    "    sim_component = (1 - sim) * 45 # less similar the answer is, the bigger the contribution to the evasion score, scaled by 45\n",
    "\n",
    "    # 2. Numerical specificity- Does the question require and answer with financial figures/ a specific answer?\n",
    "    needs_num = any(t in q.lower() for t in SPECIFICITY_TRIGGERS) # true if the question requires a numeric/ specific answer\n",
    "    has_num = bool(re.search(NUMERIC_PATTERN, a)) # true if the answer includes a number \n",
    "    numeric_component = 25 if needs_num and not has_num else 0 # score of 25 if the question needs a number but the answer doesn't give one\n",
    "\n",
    "    # 3. Evasive phrases- does the answer contain evasive phrases?\n",
    "    phrase_hits = sum(len(re.findall(p, a.lower())) for p in EVASIVE_PHRASES) # counts how many times an evasive phrase appears in the answer\n",
    "    phrase_component = min(3, phrase_hits) * 8 # max of 3 hits counted, each hit = 8 points \n",
    "\n",
    "    # Final evasion score.\n",
    "    score = min(100, sim_component + numeric_component + phrase_component) # adds components together and caps score at 100\n",
    "    \n",
    "    return score, sim, phrase_hits, needs_num, has_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25ce77",
   "metadata": {},
   "source": [
    "# **4. LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2afef3",
   "metadata": {},
   "source": [
    "## **4.1 Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09efe1",
   "metadata": {},
   "source": [
    "Small, lightweight models were selected for this to prevent memory overload and long training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de25a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base save directory.\n",
    "BASE_SAVE_DIR = \"/Users/laurenbrixey/Documents/Data Science Career Accelerator/EP Model Training\"\n",
    "os.makedirs(BASE_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "952d444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# Model names.\n",
    "distil_roberta_name = 'distilroberta-base'\n",
    "deberta_name = 'microsoft/deberta-v3-small'\n",
    "\n",
    "# Tokenizers.\n",
    "distil_roberta_tok = AutoTokenizer.from_pretrained(distil_roberta_name)\n",
    "deberta_tok = AutoTokenizer.from_pretrained (deberta_name)\n",
    "\n",
    "cfg_dr = AutoConfig.from_pretrained(distil_roberta_name, num_labels=1, problem_type=\"single_label_classification\")\n",
    "cfg_db = AutoConfig.from_pretrained(deberta_name,          num_labels=1, problem_type=\"single_label_classification\")\n",
    "\n",
    "distil_roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    distil_roberta_name, config=cfg_dr\n",
    ")\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    deberta_name, config=cfg_db\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4845a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device set-up\n",
    "import gc\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device = torch.device('mps' if use_mps else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "distil_roberta_model.to(device)\n",
    "deberta_model.to(device)\n",
    "\n",
    "def mps_gc():\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2602416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the premise from the Q&A transcripts \n",
    "def build_premise(q, a):\n",
    "    return f'[QUESTION] {q} [ANSWER] {a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e71fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset wrapper.\n",
    "class EvasionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        \n",
    "        # columns = question, answer, label (direct/evasive)\n",
    "        texts = [build_premise(q, a) for q, a in zip(df['question'].astype(str), df['answer'].astype(str))]\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=False, max_length=max_length)\n",
    "\n",
    "        # map labels (evasion=1, direct=0)\n",
    "        self.labels = (df['label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: self.encodings[k][idx] for k in self.encodings}\n",
    "        item[\"labels\"] = self.labels[idx]  # <-- key must be 'labels'\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "432287ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets.\n",
    "# distil_roberta_train = EvasionDataset(jpm_train, distil_roberta_tok, max_length=256)\n",
    "distil_roberta_val = EvasionDataset(jpm_val, distil_roberta_tok, max_length=286)\n",
    "\n",
    "# deberta_train = EvasionDataset(jpm_train, deberta_tok, max_length=256)\n",
    "deberta_val = EvasionDataset(jpm_val, deberta_tok, max_length=286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a591a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_positives(df, label_col=\"label\", pos_name=\"evasive\", factor=3):\n",
    "    pos = df[df[label_col].str.lower().str.strip() == pos_name]\n",
    "    neg = df[df[label_col].str.lower().str.strip() != pos_name]\n",
    "    df_up = pd.concat([neg, pd.concat([pos]*factor, ignore_index=True)], ignore_index=True).sample(frac=1.0, random_state=42)\n",
    "    return df_up\n",
    "\n",
    "jpm_train_up = upsample_positives(jpm_train, label_col=\"label\", pos_name=\"evasive\", factor=3)\n",
    "# rebuild datasets with jpm_train_up\n",
    "distil_roberta_train = EvasionDataset(jpm_train_up, distil_roberta_tok, max_length=286)\n",
    "deberta_train        = EvasionDataset(jpm_train_up, deberta_tok,        max_length=286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78f41f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic padding.\n",
    "from transformers import DataCollatorWithPadding\n",
    "distil_roberta_collator = DataCollatorWithPadding(tokenizer=distil_roberta_tok)\n",
    "deberta_collator = DataCollatorWithPadding(tokenizer=deberta_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5764f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model save paths.\n",
    "DISTIL_SAVE_DIR  = os.path.join(BASE_SAVE_DIR, \"distil_roberta_tuned\")\n",
    "DEBERTA_SAVE_DIR = os.path.join(BASE_SAVE_DIR, \"deberta_small_tuned\")\n",
    "os.makedirs(DISTIL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(DEBERTA_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Define early stopping.\n",
    "from transformers import EarlyStoppingCallback\n",
    "# early_stop = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)\n",
    "\n",
    "# Distil-roberta training parameters.\n",
    "distil_roberta_args = TrainingArguments(\n",
    "    output_dir=DISTIL_SAVE_DIR,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.0,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.0,\n",
    "    report_to=['none'],\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Deberta training parameters.\n",
    "deberta_args = TrainingArguments(\n",
    "    output_dir=DEBERTA_SAVE_DIR,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.0,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.0,\n",
    "    report_to=['none'],\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53799a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics during training.\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits.reshape(-1)))   # sigmoid\n",
    "    preds = (probs >= 0.5).astype(int) # threshold is 50% just to get a feel.\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    p_c, r_c, f1_c, _ = precision_recall_fscore_support(labels, preds, average=None, zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"recall_direct\": r_c[0],\n",
    "        \"recall_evasive\": r_c[1],\n",
    "        \"f1_direct\": f1_c[0],\n",
    "        \"f1_evasive\": f1_c[1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9a905b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weighted trainer.\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# class WeightedCELossTrainer(Trainer):\n",
    "#     def __init__(self, *args, class_weights=None, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.class_weights = class_weights\n",
    "\n",
    "#     # Accept extra kwargs from HF (e.g., num_items_in_batch)\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "#         # don't mutate the caller's dict\n",
    "#         inputs = inputs.copy()\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         weight = None\n",
    "#         if self.class_weights is not None:\n",
    "#             weight = self.class_weights.to(logits.device)\n",
    "\n",
    "#         loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "#         loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# class BCE1LogitTrainer(Trainer):\n",
    "#     def __init__(self, *args, pos_weight=None, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.pos_weight = pos_weight\n",
    "\n",
    "#     # 👇 accept extra kwargs from HF (e.g., num_items_in_batch)\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "#         inputs = inputs.copy()\n",
    "#         labels = inputs.pop(\"labels\").float()          # (bs,)\n",
    "#         outputs = model(**inputs)\n",
    "#         logit = outputs.logits.view(-1)                # (bs,)\n",
    "#         pw = self.pos_weight.to(logit.device) if self.pos_weight is not None else None\n",
    "#         loss = F.binary_cross_entropy_with_logits(logit, labels, pos_weight=pw, reduction=\"mean\")\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class FocalBCETrainer(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, gamma=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        inputs = inputs.copy()\n",
    "        labels = inputs.pop(\"labels\").float()         # shape: (B,)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.view(-1)              # shape: (B,)\n",
    "        pw = self.pos_weight.to(logits.device) if self.pos_weight is not None else None\n",
    "\n",
    "        # BCE per example (no reduction)\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, labels, pos_weight=pw, reduction=\"none\")\n",
    "\n",
    "        # Focal modulation\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = probs * labels + (1 - probs) * (1 - labels)   # p_t\n",
    "        loss = ((1 - pt).pow(self.gamma) * bce).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# compute class weights from TRAIN (0=Direct, 1=Evasive)\n",
    "train_y = np.array(distil_roberta_train.labels)\n",
    "n_pos = (train_y == 1).sum()\n",
    "n_neg = (train_y == 0).sum()\n",
    "scale = 3.0   # try 2.0 first; if still low recall, try 3.0\n",
    "pos_weight = torch.tensor([ (n_neg / max(1, n_pos)) * scale ], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a59e455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 01:22, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Recall Direct</th>\n",
       "      <th>Recall Evasive</th>\n",
       "      <th>F1 Direct</th>\n",
       "      <th>F1 Evasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.313457</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.341154</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.333214</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.351983</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.323538</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.387061</td>\n",
       "      <td>0.454861</td>\n",
       "      <td>0.444602</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.566251</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.572443</td>\n",
       "      <td>0.572443</td>\n",
       "      <td>0.572443</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.623155</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.372093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.793054</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.554865</td>\n",
       "      <td>0.553763</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 02:07, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Recall Direct</th>\n",
       "      <th>Recall Evasive</th>\n",
       "      <th>F1 Direct</th>\n",
       "      <th>F1 Evasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.321911</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.353468</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.418461</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.382208</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.437908</td>\n",
       "      <td>0.432540</td>\n",
       "      <td>0.451705</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>1.040521</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.514205</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>1.714527</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.450284</td>\n",
       "      <td>0.450284</td>\n",
       "      <td>0.450284</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>2.074010</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.493323</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>0.497159</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>2.160646</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.437908</td>\n",
       "      <td>0.432540</td>\n",
       "      <td>0.451705</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distil-roberta model trainer.\n",
    "distil_roberta_trainer = FocalBCETrainer(\n",
    "    model=distil_roberta_model,\n",
    "    args=distil_roberta_args,\n",
    "    train_dataset=distil_roberta_train,\n",
    "    eval_dataset=distil_roberta_val,\n",
    "    processing_class=distil_roberta_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=distil_roberta_collator,\n",
    "    pos_weight=pos_weight,\n",
    "    gamma=2.0\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Train distil_roberta.\n",
    "distil_roberta_trainer.train()\n",
    "\n",
    "# Save the tuned model.\n",
    "distil_roberta_trainer.save_model(DISTIL_SAVE_DIR)\n",
    "\n",
    "# Free up memory.\n",
    "mps_gc()\n",
    "\n",
    "# Deberta model trainer.\n",
    "deberta_trainer = FocalBCETrainer(\n",
    "    model=deberta_model,\n",
    "    args=deberta_args,\n",
    "    train_dataset=deberta_train,\n",
    "    eval_dataset=deberta_val,\n",
    "    processing_class=deberta_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=deberta_collator,\n",
    "    pos_weight=pos_weight,\n",
    "    gamma=2.0\n",
    "    # callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Train deberta.\n",
    "deberta_trainer.train()\n",
    "\n",
    "# Save the tuned models.\n",
    "deberta_trainer.save_model(DEBERTA_SAVE_DIR)\n",
    "\n",
    "# Reload the best models.\n",
    "distil_roberta_model  = AutoModelForSequenceClassification.from_pretrained(DISTIL_SAVE_DIR).to(device).eval()\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(DEBERTA_SAVE_DIR).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cdb1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "@torch.no_grad()\n",
    "def predict_probs(model, dataset, tokenizer, batch_size=32):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n",
    "    all_probs = []\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        logits = model(**batch).logits.view(-1)     # (bs,)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy() # P(evasive)\n",
    "        all_probs.append(probs)\n",
    "    return np.concatenate(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47c49fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL prob stats -> min: 0.11766578 median: 0.31766963 max: 0.8094069\n"
     ]
    }
   ],
   "source": [
    "val_probs = predict_probs(distil_roberta_model, distil_roberta_val, distil_roberta_tok)\n",
    "print(\"VAL prob stats -> min:\", val_probs.min(), \"median:\", np.median(val_probs), \"max:\", val_probs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99dd280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ThrResult:\n",
    "    thr: float\n",
    "    f1_macro: float\n",
    "    precision_macro: float\n",
    "    recall_macro: float\n",
    "    f1_evasive: float\n",
    "    recall_evasive: float\n",
    "    precision_evasive: float\n",
    "\n",
    "def eval_at_threshold(y_true, p, thr):\n",
    "    pred = (p >= thr).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, pred, average='macro', zero_division=0)\n",
    "    p_c, r_c, f1_c, _ = precision_recall_fscore_support(y_true, pred, average=None, zero_division=0)\n",
    "    return ThrResult(\n",
    "        thr=thr,\n",
    "        f1_macro=f1,\n",
    "        precision_macro=prec,\n",
    "        recall_macro=rec,\n",
    "        f1_evasive=f1_c[1],\n",
    "        recall_evasive=r_c[1],\n",
    "        precision_evasive=p_c[1]\n",
    "    )\n",
    "\n",
    "def sweep_thresholds(y_true, probs, metric=\"f1_macro\", recall_floor=None):\n",
    "    best = None\n",
    "    for thr in np.linspace(0.05, 0.95, 181):  # step 0.005\n",
    "        r = eval_at_threshold(y_true, probs, thr)\n",
    "        if recall_floor is not None and r.recall_evasive < recall_floor:\n",
    "            continue\n",
    "        if best is None or getattr(r, metric) > getattr(best, metric):\n",
    "            best = r\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da94e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DISTIL thr: 0.695 F1_macro: 0.6160714285714286 Recall_evasive: 0.2727272727272727\n",
      "Best DEBERTA thr: 0.09 F1_macro: 0.520655737704918 Recall_evasive: 0.36363636363636365\n"
     ]
    }
   ],
   "source": [
    "# CHANGE: tune for both models; use whichever you deploy\n",
    "val_labels = np.array(distil_roberta_val.labels)\n",
    "\n",
    "val_probs_distil  = predict_probs(distil_roberta_model, distil_roberta_val, distil_roberta_tok)\n",
    "best_distil       = sweep_thresholds(val_labels, val_probs_distil, metric=\"f1_macro\")\n",
    "with open(os.path.join(DISTIL_SAVE_DIR, \"threshold.json\"), \"w\") as f:\n",
    "    json.dump({\"evasive_threshold\": float(best_distil.thr)}, f)\n",
    "\n",
    "val_probs_deberta = predict_probs(deberta_model, deberta_val, deberta_tok)\n",
    "best_deberta      = sweep_thresholds(val_labels, val_probs_deberta, metric=\"f1_macro\")\n",
    "with open(os.path.join(DEBERTA_SAVE_DIR, \"threshold.json\"), \"w\") as f:\n",
    "    json.dump({\"evasive_threshold\": float(best_deberta.thr)}, f)\n",
    "\n",
    "print(\"Best DISTIL thr:\", best_distil.thr, \"F1_macro:\", best_distil.f1_macro, \"Recall_evasive:\", best_distil.recall_evasive)\n",
    "print(\"Best DEBERTA thr:\", best_deberta.thr, \"F1_macro:\", best_deberta.f1_macro, \"Recall_evasive:\", best_deberta.recall_evasive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "372e8d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deberta_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CHANGE: example with DeBERTa; swap in Distil if preferred\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_probs_deberta \u001b[38;5;241m=\u001b[39m predict_probs(deberta_model, \u001b[43mdeberta_test\u001b[49m, deberta_tok)\n\u001b[1;32m      3\u001b[0m test_labels        \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(deberta_test\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DEBERTA_SAVE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deberta_test' is not defined"
     ]
    }
   ],
   "source": [
    "# CHANGE: example with DeBERTa; swap in Distil if preferred\n",
    "test_probs_deberta = predict_probs(deberta_model, deberta_test, deberta_tok)\n",
    "test_labels        = np.array(deberta_test.labels)\n",
    "\n",
    "with open(os.path.join(DEBERTA_SAVE_DIR, \"threshold.json\")) as f:\n",
    "    thr = json.load(f)[\"evasive_threshold\"]\n",
    "\n",
    "test_res = eval_at_threshold(test_labels, test_probs_deberta, thr)\n",
    "print(\"Chosen thr:\", thr, \"| F1_macro:\", test_res.f1_macro, \"| Recall_evasive:\", test_res.recall_evasive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90ed9f",
   "metadata": {},
   "source": [
    "## **4.2 Functions for pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- blend utility ----------\n",
    "def blend_probs(p_base, p_llm, llm_weight=0.70):\n",
    "    # p_blend = w * llm + (1-w) * base\n",
    "    return llm_weight * p_llm + (1.0 - llm_weight) * p_base\n",
    "\n",
    "# ---------- compact report ----------\n",
    "def _short_report(tag, res):\n",
    "    print(f\"[{tag}] thr={res.thr:.4f} | F1_macro={res.f1_macro:.3f} | \"\n",
    "          f\"Recall_evasive={res.recall_evasive:.3f} | Precision_evasive={res.precision_evasive:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evasion_pipeline_v1(\n",
    "    # labels\n",
    "    val_labels: np.ndarray,\n",
    "    test_labels: np.ndarray,\n",
    "    # probabilities\n",
    "    val_probs_base: np.ndarray,\n",
    "    test_probs_base: np.ndarray,\n",
    "    val_probs_llm: np.ndarray,\n",
    "    test_probs_llm: np.ndarray,\n",
    "    # knobs\n",
    "    llm_weight: float = 0.70,\n",
    "    metric: str = \"f1_macro\",\n",
    "    recall_floor: float | None = None,\n",
    "    save_dirs: dict | None = None,  # e.g., {\"base\": \"...\", \"llm\": DEBERTA_SAVE_DIR, \"blend\": \"...\"}\n",
    "):\n",
    "    \"\"\"\n",
    "    Tunes thresholds on VAL for baseline, LLM, and blended probs, then applies to TEST.\n",
    "    Returns a dict of threshold + test metrics for each system.\n",
    "    \"\"\"\n",
    "    # --- blend on VAL/TEST\n",
    "    val_probs_blend  = blend_probs(val_probs_base,  val_probs_llm,  llm_weight)\n",
    "    test_probs_blend = blend_probs(test_probs_base, test_probs_llm, llm_weight)\n",
    "\n",
    "    # --- tune thresholds on VAL\n",
    "    best_base  = sweep_thresholds(val_labels,  val_probs_base,  metric=metric, recall_floor=recall_floor)\n",
    "    best_llm   = sweep_thresholds(val_labels,  val_probs_llm,   metric=metric, recall_floor=recall_floor)\n",
    "    best_blend = sweep_thresholds(val_labels,  val_probs_blend, metric=metric, recall_floor=recall_floor)\n",
    "\n",
    "    _short_report(\"VAL/Base \",  best_base)\n",
    "    _short_report(\"VAL/LLM  \",  best_llm)\n",
    "    _short_report(\"VAL/Blend\",  best_blend)\n",
    "\n",
    "    # --- evaluate on TEST using tuned thresholds\n",
    "    test_base  = eval_at_threshold(test_labels,  test_probs_base,  best_base.thr)\n",
    "    test_llm   = eval_at_threshold(test_labels,  test_probs_llm,   best_llm.thr)\n",
    "    test_blend = eval_at_threshold(test_labels,  test_probs_blend, best_blend.thr)\n",
    "\n",
    "    print(\"\\n=== TEST ===\")\n",
    "    _short_report(\"Base \",  test_base)\n",
    "    _short_report(\"LLM  \",  test_llm)\n",
    "    _short_report(\"Blend\",  test_blend)\n",
    "\n",
    "    # --- optionally save thresholds\n",
    "    if save_dirs is not None:\n",
    "        import json, os\n",
    "        os.makedirs(save_dirs.get(\"base\", \"\"), exist_ok=True)  if \"base\"  in save_dirs else None\n",
    "        os.makedirs(save_dirs.get(\"llm\", \"\"), exist_ok=True)   if \"llm\"   in save_dirs else None\n",
    "        os.makedirs(save_dirs.get(\"blend\", \"\"), exist_ok=True) if \"blend\" in save_dirs else None\n",
    "\n",
    "        if \"base\" in save_dirs:\n",
    "            with open(os.path.join(save_dirs[\"base\"], \"threshold.json\"), \"w\") as f:\n",
    "                json.dump({\"evasive_threshold\": float(best_base.thr)}, f)\n",
    "        if \"llm\" in save_dirs:\n",
    "            with open(os.path.join(save_dirs[\"llm\"], \"threshold.json\"), \"w\") as f:\n",
    "                json.dump({\"evasive_threshold\": float(best_llm.thr)}, f)\n",
    "        if \"blend\" in save_dirs:\n",
    "            with open(os.path.join(save_dirs[\"blend\"], \"threshold.json\"), \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"evasive_threshold\": float(best_blend.thr),\n",
    "                    \"llm_weight\": float(llm_weight)\n",
    "                }, f)\n",
    "\n",
    "    return {\n",
    "        \"val\":   {\"base\": best_base, \"llm\": best_llm, \"blend\": best_blend},\n",
    "        \"test\":  {\"base\": test_base, \"llm\": test_llm, \"blend\": test_blend},\n",
    "        \"thrs\":  {\"base\": best_base.thr, \"llm\": best_llm.thr, \"blend\": best_blend.thr},\n",
    "        \"weight\": llm_weight,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LLM probabilities (pick your selected model; here: deberta)\n",
    "val_probs_llm  = predict_probs(deberta_model, deberta_val,  deberta_tok)\n",
    "test_probs_llm = predict_probs(deberta_model, deberta_test, deberta_tok)\n",
    "\n",
    "# 2) Baseline probabilities:\n",
    "#    If you already have them as probs, just assign:\n",
    "#    val_probs_base  = <np.array shape [n_val]>\n",
    "#    test_probs_base = <np.array shape [n_test]>\n",
    "#\n",
    "#    If you have *logits*, convert first:\n",
    "#    val_probs_base  = 1 / (1 + np.exp(-val_logits_base))\n",
    "#    test_probs_base = 1 / (1 + np.exp(-test_logits_base))\n",
    "#\n",
    "#    If you only have hard labels from baseline, consider fitting a quick\n",
    "#    probability model for it (e.g., calibrate or use its raw score if available).\n",
    "\n",
    "# 3) Labels\n",
    "val_labels  = np.array(deberta_val.labels)   # or your dataset’s labels\n",
    "test_labels = np.array(deberta_test.labels)\n",
    "\n",
    "# 4) Run the pipeline\n",
    "results = evasion_pipeline_v1(\n",
    "    val_labels=val_labels,\n",
    "    test_labels=test_labels,\n",
    "    val_probs_base=val_probs_base,\n",
    "    test_probs_base=test_probs_base,\n",
    "    val_probs_llm=val_probs_llm,\n",
    "    test_probs_llm=test_probs_llm,\n",
    "    llm_weight=0.70,\n",
    "    metric=\"f1_macro\",        # or \"recall_macro\" / \"precision_macro\"\n",
    "    recall_floor=None,        # or e.g. 0.70 to enforce min evasive recall\n",
    "    save_dirs={\"llm\": DEBERTA_SAVE_DIR}  # add \"base\"/\"blend\" dirs if you want files\n",
    ")\n",
    "\n",
    "# 5) Access chosen thresholds\n",
    "EVASION_THRESHOLD_BASE  = results[\"thrs\"][\"base\"]\n",
    "EVASION_THRESHOLD_LLM   = results[\"thrs\"][\"llm\"]\n",
    "EVASION_THRESHOLD_BLEND = results[\"thrs\"][\"blend\"]\n",
    "LLM_WEIGHT              = results[\"weight\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fde2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisions_from_probs(probs, thr):\n",
    "    return (probs >= thr).astype(int)\n",
    "\n",
    "test_pred_base  = decisions_from_probs(test_probs_base,  EVASION_THRESHOLD_BASE)\n",
    "test_pred_llm   = decisions_from_probs(test_probs_llm,   EVASION_THRESHOLD_LLM)\n",
    "test_pred_blend = decisions_from_probs(\n",
    "    blend_probs(test_probs_base, test_probs_llm, LLM_WEIGHT),\n",
    "    EVASION_THRESHOLD_BLEND\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af43fd",
   "metadata": {},
   "source": [
    "# **5. Evasion Detection Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac468677",
   "metadata": {},
   "source": [
    "## **5.1 Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label 'Direct' or 'Evasive' based on the score.\n",
    "def label_from_score(score, threshold):\n",
    "    return 'Evasive' if score >= threshold else 'Direct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4555b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth (1 = Evasive, 0 = Direct)\n",
    "def extract_y_true(df):\n",
    "    return (df['label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cba814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the logits margin.\n",
    "def compute_logits_margin(model, tokenizer, df, batch_size=32, max_length=512):\n",
    "    model.eval()\n",
    "    margins = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            b = df.iloc[i:i+batch_size]\n",
    "            texts = [build_premise(q, a) for q, a in zip(b['question'].astype(str), b['answer'].astype(str))]\n",
    "            enc = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            out = model(**enc).logits  # [B,2]\n",
    "            margins.extend((out[:,1] - out[:,0]).detach().cpu().numpy().tolist())\n",
    "    return np.array(margins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5871f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_evasion_score(question, answer, model, tokenizer, platt_model, max_length=512):\n",
    "    text = build_premise(question, answer)\n",
    "    enc = tokenizer(text, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits.squeeze(0)  # [2]\n",
    "    margin = float((logits[1] - logits[0]).detach().cpu().numpy())\n",
    "    p_ev = float(platt_model.predict_proba([[margin]])[0,1])\n",
    "    return {'p_evasive': p_ev, 'p_direct': 1.0 - p_ev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute blended evasion score and return all scores.\n",
    "def compute_all_evasion_scores(q, a, *, models_and_tokenizers, device, LLM_WEIGHT=0.30):\n",
    "    \n",
    "    # Compute baseline evasion score.\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # Individual LLM scores.\n",
    "    llm_scores = {}\n",
    "    for key, (_, m, t, platt) in models_and_tokenizers.items():\n",
    "        scores = llm_evasion_score(q, a, m, t, platt)\n",
    "        llm_scores[key] = float(100.0 * scores['p_evasive'])\n",
    "\n",
    "    # Ensemble LLM score.\n",
    "    llm_avg = float(np.mean(list(llm_scores.values()))) if llm_scores else 0.0\n",
    "\n",
    "    # Compute blended score.\n",
    "    blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * llm_avg, 0.0, 100.0))\n",
    "\n",
    "    return {\n",
    "        'baseline': base_score,\n",
    "        'llm_individual': llm_scores,\n",
    "        'llm_avg': llm_avg,\n",
    "        'blended': blended_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607eb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main evasion Pipeline\n",
    "def evasion_pipeline(df, models_and_tokenizers, device, LLM_WEIGHT, EVASION_THRESHOLD_BASE, EVASION_THRESHOLD_LLM, EVASION_THRESHOLD_BLENDED):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row['question']), str(row['answer'])\n",
    "        out = compute_all_evasion_scores(q=q, a=a, LLM_WEIGHT=LLM_WEIGHT, models_and_tokenizers=models_and_tokenizers, device=device)\n",
    "\n",
    "        rec = {\n",
    "            'question_number': row.get('question_number'),\n",
    "            'question': q, 'answer': a,\n",
    "            'evasion_score_baseline': int(out['baseline']),\n",
    "            'evasion_score_llm_avg': int(out['llm_avg']),\n",
    "            'evasion_score_blended': int(out['blended']),\n",
    "            'prediction_baseline': label_from_score(out['baseline'], EVASION_THRESHOLD_BASE),\n",
    "            'prediction_llm_avg': label_from_score(out['llm_avg'], EVASION_THRESHOLD_LLM),\n",
    "            'prediction_blended': label_from_score(out['blended'], EVASION_THRESHOLD_BLENDED),\n",
    "        }\n",
    "        # add individual models dynamically\n",
    "        for model_name, score in out['llm_individual'].items():\n",
    "            rec[f'evasion_score_{model_name}'] = int(score)\n",
    "            rec[f'prediction_{model_name}'] = label_from_score(score, EVASION_THRESHOLD_LLM)\n",
    "\n",
    "        records.append(rec)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58f2c8",
   "metadata": {},
   "source": [
    "## **5.2 Threshold Tuning & Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed308658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate LLM labelling threshold on validation set.\n",
    "y_val = extract_y_true(jpm_val)\n",
    "\n",
    "distil_roberta_val_margins = compute_logits_margin(distil_roberta_model, distil_roberta_tok, jpm_val)\n",
    "deberta_val_margins = compute_logits_margin(deberta_model, deberta_tok, jpm_val)\n",
    "\n",
    "distil_roberta_platt = LogisticRegression(solver='lbfgs').fit(distil_roberta_val_margins.reshape(1, -1), y_val)\n",
    "derberta_platt = LogisticRegression(solver='lbfgs').fit(deberta_val_margins.reshape(1, -1), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99783b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models.\n",
    "models_and_tokenizers = {\n",
    "    'distil_roberta': ('distil_roberta', distil_roberta_model, distil_roberta_tok, distil_roberta_platt)\n",
    "    'deberta': ('deberta', deberta_model, deberta_tok, deberta_platt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine tune the threshold \n",
    "def tune_threshold(df, score_col, thr_grid):\n",
    "    y_true = extract_y_true(df)\n",
    "    scores = df[score_col].astype(float).values\n",
    "    rows = []\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (scores >= thr).astype(int)\n",
    "        rows.append({\n",
    "            'threshold': float(thr),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall':    recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1':        f1_score(y_true, y_pred, zero_division=0),\n",
    "            'accuracy':  accuracy_score(y_true, y_pred)\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(by=['f1','recall'], ascending=[False, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an initial run with preliminary threshold values.\n",
    "LLM_WEIGHT = 0.30\n",
    "EVASION_THRESHOLD_BASE = 30.0\n",
    "EVASION_THRESHOLD_LLM = 30.0\n",
    "EVASION_THRESHOLD_BLENDED = 30.0\n",
    "\n",
    "jpm_val_scores = evasion_pipeline(\n",
    "    jpm_val, \n",
    "    models_and_tokenizers, \n",
    "    device, \n",
    "    LLM_WEIGHT, \n",
    "    EVASION_THRESHOLD_BASE, \n",
    "    EVASION_THRESHOLD_LLM, \n",
    "    EVASION_THRESHOLD_BLENDED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results and reappend the label.\n",
    "jpm_val_scores['label'] = jpm_val_qa_labelled['label'].values\n",
    "jpm_val_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold grid.\n",
    "thr_grid = np.arange(30, 85, 5)\n",
    "\n",
    "# Tune all thresholds.\n",
    "base_results = tune_threshold(jpm_val_scores, 'evasion_score_baseline', thr_grid)\n",
    "llm_avg_results = tune_threshold(jpm_val_scores, 'evasion_score_llm_avg', thr_grid)\n",
    "blend_results = tune_threshold(jpm_val_scores, 'evasion_score_blended', thr_grid)\n",
    "\n",
    "best_base_thr    = float(base_results.loc[0, 'threshold'])\n",
    "best_llm_avg_thr = float(llm_avg_results.loc[0, 'threshold'])\n",
    "best_blend_thr   = float(blend_results.loc[0, 'threshold'])\n",
    "best_model_thrs  = {k: float(v.loc[0, 'threshold']) for k, v in per_model_results.items()}\n",
    "\n",
    "print(\"=== Best thresholds (VAL) ===\")\n",
    "print(\"Baseline:\", best_base_thr)\n",
    "print(\"LLM Avg:\", best_llm_avg_thr)\n",
    "print(\"Blended:\", best_blend_thr)\n",
    "for k, thr in best_model_thrs.items():\n",
    "    print(f\"{k}: {thr}\")\n",
    "\n",
    "# View top configs. \n",
    "print(\"\\nTop 5 baseline:\\n\", base_results.head())\n",
    "print(\"\\nTop 5 llm_avg:\\n\", llm_avg_results.head())\n",
    "print(\"\\nTop 5 blended:\\n\", blend_results.head())\n",
    "for k, dfres in per_model_results.items():\n",
    "    print(f\"\\nTop 5 {k}:\\n\", dfres.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c08171",
   "metadata": {},
   "source": [
    "## **5.2 Optimised Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ef6c7",
   "metadata": {},
   "source": [
    "## **5.3 2025 Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 7) Pick ONE LLM and run TEST with baseline\n",
    "# ==============================================\n",
    "# Choose which LLM you want to carry forward\n",
    "SELECTED_LLM = 'deberta_small'   # or 'distilroberta'\n",
    "\n",
    "# Use its tuned threshold + the tuned baseline and blended thresholds\n",
    "LLM_WEIGHT = 0.70  # keep your preferred blend weight\n",
    "\n",
    "EVASION_THRESHOLD_BASE    = best_base_thr\n",
    "EVASION_THRESHOLD_LLM_SEL = best_model_thrs[SELECTED_LLM]\n",
    "EVASION_THRESHOLD_BLEND   = best_blend_thr  # or retune blend after fixing SELECTED_LLM if you like\n",
    "\n",
    "# Run v1 on TEST (still computes all models, but we'll read only baseline + selected)\n",
    "test_scores_v1 = evasion_pipeline(\n",
    "    jpm_test_qa_labelled,\n",
    "    models_and_tokenizers,\n",
    "    device,\n",
    "    LLM_WEIGHT,\n",
    "    EVASION_THRESHOLD_BASE,\n",
    "    EVASION_THRESHOLD_LLM_SEL,   # used for individual preds\n",
    "    EVASION_THRESHOLD_BLEND\n",
    ")\n",
    "test_scores_v1['label'] = jpm_test_qa_labelled['label'].values\n",
    "\n",
    "# Compact evaluation for baseline + SELECTED_LLM (+ blended if desired)\n",
    "def _to_bin(pred_series): return (pred_series.astype(str).str.lower() == 'evasive').astype(int).values\n",
    "y_true = (test_scores_v1['label'].astype(str).str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "y_pred_base = _to_bin(test_scores_v1['prediction_baseline'])\n",
    "y_pred_sel  = _to_bin(test_scores_v1[f'prediction_{SELECTED_LLM}'])\n",
    "y_pred_blnd = _to_bin(test_scores_v1['prediction_blended'])\n",
    "\n",
    "print(\"\\n=== TEST: BASELINE ===\")\n",
    "print(classification_report(y_true, y_pred_base, target_names=[\"Direct\",\"Evasive\"], digits=3, zero_division=0))\n",
    "print(confusion_matrix(y_true, y_pred_base))\n",
    "\n",
    "print(f\"\\n=== TEST: {SELECTED_LLM.upper()} ===\")\n",
    "print(classification_report(y_true, y_pred_sel, target_names=[\"Direct\",\"Evasive\"], digits=3, zero_division=0))\n",
    "print(confusion_matrix(y_true, y_pred_sel))\n",
    "\n",
    "print(\"\\n=== TEST: BLENDED ===\")\n",
    "print(classification_report(y_true, y_pred_blnd, target_names=[\"Direct\",\"Evasive\"], digits=3, zero_division=0))\n",
    "print(confusion_matrix(y_true, y_pred_blnd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
