{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b820ac6",
   "metadata": {},
   "source": [
    "# **Summarisation & Evasion Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e2962",
   "metadata": {},
   "source": [
    "# **Handover Notes:** [delete after]\n",
    "- Library imports and versions are saved in environments/summarisation_evasion_env.txt\n",
    "- This notebook was originally built for a macbook pro M3 chip so some settings may need to be altered depending on your machine\n",
    "- All files related/ generated by this notebook can be found in notebooks/summarisation_evasion_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b76a8",
   "metadata": {},
   "source": [
    "### **Work progress**\n",
    "1. **Complete**\n",
    "- Summarise banker answers using baseline model.\n",
    "- Use Local RAG pipeline to bring in relevant external documents (PRA risk definitions) to create PRA aligned summaries.\n",
    "- Developed a evasion detection prototype that generates evasion scores based on bankers answers (uses baseline model, LLM- natural language inference using RoBERTa and a blended score)\n",
    "- Used jpm_2025 transcripts to get the pipeline working. Validated the evasion pipeline using jpm-23-1q data (involved human labelling the answer as Direct or Evasive- file saved in notebooks/summarisation_evasion_files).\n",
    "\n",
    "2. **Not complete**\n",
    "- Need to test pipleine on larger data set (e.g. jpm 2023-2025) and check against HSBC to make conclusions & comment on generalisability (answering research question: How does one bank’s tone and thematic profile compare to peers? Are divergences systemic or firm specific?)\n",
    "- Summarisation pipeline could be improved using a two-stage pipeline: by first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned.\n",
    "- Post-processing on the output file for the PRA aligned summaries by Mistral model so they are clearer- can this output be fed into another model to extract more insights/ detect evasion or risk?\n",
    "- Increase the size of the validation set for the evasion pipeline prototype (e.g. more human labelling)\n",
    "- Need to fine tune the evasion pipeline to increase accuracy\n",
    "- Optional extensions e.g. using Agents, more complex RAG pipeline (including more useful context for the model), validation of instances of evasion using external news sources)\n",
    "\n",
    "[we can also look at some of this in Week 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e874e23",
   "metadata": {},
   "source": [
    "# 1. **Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670282",
   "metadata": {},
   "source": [
    "# **2. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5f5884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "import math\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "import spacy\n",
    "from llama_cpp import Llama \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "from bert_score import score as bertscore \n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import faiss\n",
    "import chromadb\n",
    "import langchain\n",
    "import llama_index\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set SEED.\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91bb8c",
   "metadata": {},
   "source": [
    "# **3. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "743f6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "jpm_2025_df = pd.read_csv('../data/processed/jpm/all_jpm_2025.csv')\n",
    "\n",
    "# View the data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75628c",
   "metadata": {},
   "source": [
    "# **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b70dc",
   "metadata": {},
   "source": [
    "- Used all_jpm_2025.csv dataset\n",
    "- Preliminary preprocessing to label roles as analyst vs banker (invalid roles were corrected) to make downstream analysis easier. Created a new column 'role_normalised'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc49b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ec70c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_number  answer_number                            speaker_name  \\\n",
       "201               35            5.0  Chief Financial Officer, JPMorganChase   \n",
       "205               36            3.0  Chief Financial Officer, JPMorganChase   \n",
       "\n",
       "                                            role             company  \\\n",
       "201  And then some. Theres a lot of value added.       JPMorganChase   \n",
       "205                                         Okay  there you have it.   \n",
       "\n",
       "                                               content  year quarter  \\\n",
       "201  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "205  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "\n",
       "                                            source_pdf  \n",
       "201  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "205  data/raw/jpm/jpm-2q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles.\n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = jpm_2025_df[~jpm_2025_df['role'].isin(valid_roles)]\n",
    "\n",
    "# Number of rows with invalid roles.\n",
    "print('Number of rows:', invalid_roles_df.shape[0])\n",
    "\n",
    "# View the rows.\n",
    "invalid_roles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e31cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the correct role information.\n",
    "jpm_2025_df.at[205, 'role'] = 'Chief Financial Officer'\n",
    "jpm_2025_df.at[209, 'role'] = 'Chief Financial Officer'\n",
    "\n",
    "# Verify the roles have been updated.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e44043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_2025_df['role_normalised'] = jpm_2025_df['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "755f26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf565",
   "metadata": {},
   "source": [
    "# **5. Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401764b",
   "metadata": {},
   "source": [
    "## **5.1 Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee15b",
   "metadata": {},
   "source": [
    "- Initial model exploration using BART and mistral-7B-instruct to summarise banker's answers (no additional context given to model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92395bc",
   "metadata": {},
   "source": [
    "### **5.1.1 BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "671f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the w\n"
     ]
    }
   ],
   "source": [
    "# Filter data to banker answers only.\n",
    "banker_answers = jpm_2025_df[jpm_2025_df['role_normalised'] == 'banker']['content'].tolist()\n",
    "print(banker_answers[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54559054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (BART)\n",
    "bart = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "sample_text = banker_answers[0]\n",
    "summary_bart = bart(sample_text, max_length=80, min_length=30, do_sample=False)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f0d9b",
   "metadata": {},
   "source": [
    "- bart was able to extract ket ideas, focussing on fronloading of spending and tariff policy. \n",
    "- Compressed the response into two sentences and the summary is coherent, removing filler phrases.\n",
    "- However, the summary is not fully neutral (e.g. includes ironically) and preserves tone\n",
    "- Also there is a loss of context- e.g. consumer side vs wholesale side distinction is no longer explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: Corporates are taking a wait-and-see approach to tariff policy. Some sectors are going to be much more exposed than others. Small business and smaller corporates are probably a little more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Prompt conditioning to make PRA relevant.\n",
    "prompt = \"Summarise this answer, focusing on risk, capital and evasion of detail: \" + sample_text\n",
    "summary_bart_prompted = bart(prompt, max_length=80, min_length=30)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart_prompted[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e7996",
   "metadata": {},
   "source": [
    "- Prompted summary shifts emphasis and includes interpretation around risk, even though those words were no explicit in the original\n",
    "- This version is more aligned to evasion detection but moves away from concrete detail \n",
    "- Improved approach would be to have a two stage-pipeline: first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaabd6",
   "metadata": {},
   "source": [
    "### **5.1.2 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f4b90",
   "metadata": {},
   "source": [
    "- Mistral model: mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Mistral-7B-Instruct model download: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF?show_file_info=mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Also saved in shared team folder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a368c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides. On the consumer side, there has been some frontloading of spending ahead of expected price increases from tariffs, which may distort the data and make it difficult to draw larger conclusions. On the corporate side, clients are reacting to changes in tariff policy by shifting their focus towards short-term work and optimizing supply chains. The speaker characterizes the attitude of corporate clients as a wait-and-see attitude, with smaller clients and smaller corporates being more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with basic prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral = output[\"choices\"][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec467d",
   "metadata": {},
   "source": [
    "- Preserves details and nuance and is more contextual and interpretive than the BART baseline model.\n",
    "- However, the result is longer with heavier phrasing and includes phrases like 'distort the data' which is not explicit in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides of their business. On the consumer side, they have observed some frontloading of spending ahead of expected price increases from tariffs, which may distort data and make it difficult to draw larger conclusions. On the corporate side, clients are shifting their focus towards optimizing supply chains and responding to the current environment, rather than prioritizing more strategic work. The speaker notes that smaller clients and smaller corporates may be more challenged than larger ones, which have more experience dealing with these types of changes and more resources to manage them. Overall, the speaker suggests that it is difficult to make long-term decisions at this time due to the uncertainty surrounding the current environment.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with more detailed prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions. Focus on risk, capital and evasion of detail.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral_prompted = output[\"choices\"][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral_prompted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1e33d",
   "metadata": {},
   "source": [
    "- This summary brings in risk- language and is closer to the task objective.\n",
    "- However, some interpretations are generated by the model rather than explicitly detailed in the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813acc2",
   "metadata": {},
   "source": [
    "## **5.2 Adding Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1b7a",
   "metadata": {},
   "source": [
    "Retrieve PRA risk categories to give greater PRA focus to summaries (local RAG loop).\n",
    "- measure cosine similarity between transcript chunks and PRA risk categories (vectors)\n",
    "- retrieve the top 2-3 most relevant risk categories \n",
    "- prepend them to the summarisation prompt to make summaries PRA-aligned instead of just summarised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2bd1",
   "metadata": {},
   "source": [
    "- Attempting to use BART resulted in prompt echoing.\n",
    "- New attempt using Mistral-7B-Instruct.\n",
    "- Using sentence-BERT vs TF-IDF for vectorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9dab",
   "metadata": {},
   "source": [
    "### **5.2.1 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56464c24",
   "metadata": {},
   "source": [
    "**Process**\n",
    "- Performed some light cleaning of the transcript to remove whitespace.\n",
    "- Split the transcript into smaller chunks that the model can summarise to avoid truncation\n",
    "- Loaded the PRA categories csv file (contains category and definition)\n",
    "- Embedded the PRA categories and chunks, evaluated the similarity to extract the PRA risk categories that were relevant to the text\n",
    "- Summarised the chunk using detailed prompted and relevant PRA categories as additional context. \n",
    "\n",
    "**Output File**:\n",
    "- The output file of this can be found in notebooks/summarisation_evasion_files, name = jpm_mistral_pra_summary.json\n",
    "- It is in the format: summary, evidence, PRA category that relates to summary and reasoning for selecting these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e695d9e",
   "metadata": {},
   "source": [
    "- Needed to use a lot of fine tuning for the prompt and set strict rules for the model\n",
    "- Need to be very clear about the output expected or else the model deviates a lot, especially as it processes more data.\n",
    "- Include lines about lack of evidence if not the model may hallucinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove whitespace in text.\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the transcript into smaller chunks.\n",
    "def chunk_text(text: str, max_chars: int = 6000):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split into sentences \n",
    "    chunks, current_chunk, current_len = [], [], 0 # list of chunks, sentences collecting for current chunk, character count for current chunk\n",
    "\n",
    "    for s in sentences:\n",
    "        if current_len + len(s) + 1 <= max_chars: # if the characters of current chunk + new sentence is below the limit:\n",
    "            current_chunk.append(s) # add sentence to current chunk \n",
    "            current_len += len(s) + 1 # update running character count \n",
    "        \n",
    "        else: # if the characters is above the limit:\n",
    "            chunks.append(' '.join(current_chunk)) # add the current chunk to the final chunk list\n",
    "            current_chunk, current_len = [s], len(s) # start a new chunk containing the sentence and update current len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk)) # add any sentences in current chunk after loop ends \n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PRA categories and definitions from CSV.\n",
    "def load_pra_categories(path: Path):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            (row.get('category', '').strip(), [row.get('definition', '').strip()])\n",
    "            for row in reader if row.get('category')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sentence-BERT embedding index for PRA categories.\n",
    "def build_embedding_index(pra_categories):\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    docs = [f\"{name} {' '.join(defs)}\" for name, defs in pra_categories]\n",
    "    pra_risk_embeddings = embedder.encode(docs, batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "    return embedder, np.asarray(pra_risk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the relevant PRA categories to the transcript chunks.\n",
    "def find_rel_categories(chunk, pra_categories, embedder, pra_risk_embeddings, top_k=2):\n",
    "    query_vec = embedder.encode([chunk], normalize_embeddings=True) # turns chunk into embedding\n",
    "    sims = cosine_similarity(query_vec, pra_risk_embeddings).ravel() # compares the chunk to each category doc \n",
    "    top_indices = np.argsort(-sims)[:top_k] # sorts scores descending and selected top k cateogories \n",
    "\n",
    "    return [pra_categories[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON\n",
    "def parse_tagged_json(raw):\n",
    "    m = re.search(r\"<json>\\s*(\\{[\\s\\S]*?\\})\\s*</json>\", raw, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise the text chunks.\n",
    "def summarise_chunk(model, chunk, relevant_categories, max_evidence=5):\n",
    "\n",
    "    # Build PRA notes (limit to 2 bullets per category)\n",
    "    lines = []\n",
    "    for name, definition in relevant_categories:\n",
    "        lines.append(f\"- {name}:\")\n",
    "        for d in list(definition)[:2]:\n",
    "            lines.append(f\"- {d}\")\n",
    "    notes_block = \"\\n\".join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a careful data extraction model. \"\n",
    "        \"Return ONLY valid JSON wrapped in <json>...</json> tags.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "TRANSCRIPT:\n",
    "{chunk}\n",
    "\n",
    "PRA NOTES:\n",
    "{notes_block}\n",
    "\n",
    "TASK:\n",
    "Return JSON ONLY, wrapped exactly like this:\n",
    "<json>{{\"summary\": \"...\", \"evidence\": [\"...\"], \"pra_categories\": [{{\"category\":\"...\",\"why\":\"...\"}}]}}</json>\n",
    "\n",
    "RULES:\n",
    "- 4-6 sentence neutral summary.\n",
    "- Up to {max_evidence} evidence bullets (quotes/facts).\n",
    "- 1-3 pra_categories objects.\n",
    "- If evidence is lacking, use a single bullet \"Insufficient evidence\".\n",
    "- Only choose categories supported by the evidence.\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = model.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=700,\n",
    "        repeat_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    raw = (response[\"choices\"][0][\"message\"][\"content\"] or \"\").strip()\n",
    "\n",
    "    # Parse the tagged JSON\n",
    "    parsed = parse_tagged_json(raw)\n",
    "\n",
    "    # Fallback if model didn’t follow instructions\n",
    "    if not parsed:\n",
    "        return (\n",
    "            {\"summary\": \"\", \"evidence\": [\"Insufficient evidence\"], \"pra_categories\": []},\n",
    "            raw,\n",
    "        )\n",
    "\n",
    "    # Light coercion to guarantee keys exist\n",
    "    result = {\n",
    "        \"summary\": parsed.get(\"summary\", \"\") or \"\",\n",
    "        \"evidence\": parsed.get(\"evidence\", []) or [],\n",
    "        \"pra_categories\": parsed.get(\"pra_categories\", []) or []\n",
    "    }\n",
    "    return result, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables.\n",
    "MODEL_PATH = '/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "PRA_NOTES_PATH = '../data/RAG-resources/PRA_risk_categories.csv'\n",
    "TRANSCRIPT_PATH = '../data/processed/jpm/all_jpm_2025.csv'\n",
    "OUTPUT_PATH = pathlib.Path(\"jpm_mistral_pra_summary_raw.json\")\n",
    "TOP_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 8456 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 110 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2495.33 MiB, ( 4951.36 / 10922.67)\n",
      "load_tensors: offloading 20 repeating layers to GPU\n",
      "load_tensors: offloaded 20/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  2495.33 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x394882ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x394882d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x394883f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x3948844e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x37e62ebe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x37e62eeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x394884a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x394885020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x3948855c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x394885880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x394886030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x393106ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x394885b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x393105730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x394415bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x394415f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x3944164b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x394786a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x394786e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x3947872c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x394886b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x394416870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x394787b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x394787f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x394886680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x394887610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x394887bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x394887e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x394888950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x394889200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x394889b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x394787710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x39488a350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x394417200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x39488aa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x39488b2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x393104080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x393104340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x39009ceb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x394417740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x394417f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x39488bad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x39488c2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x3944185a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x394789590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x394418860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x394418da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x39009d170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x39009d430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x39488c840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x39488cde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x394786760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x394789c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x39488d380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x39488d640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x39009d6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x39009d9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x39488ddb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x394419060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x39478a210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x390078b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x39003de40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x39008de40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x3900edf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x390054410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x39007bf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x3900c20d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x390087cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x39478a630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x390087f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x390088250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x39478aa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x39478ae70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x39478b290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x3900e3990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x39488e630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x39478b6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x390072c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x39478bae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x39488ebd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x39488f200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x39488f7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x39478bf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x3900af410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x3900ff8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x394419600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x3944199c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x394419d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x39004b180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x39478c550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x39478c810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x39488fdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x39441a140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x39441a400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x39441a9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x39441ac60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x39441af20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x39478cad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x3948903a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x39441b850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x39004b440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x39004b700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3900da110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x39441bdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x39441c390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x390090390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x39005fa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x3900c5990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x39441c930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x39478ce90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x39478d290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x394890c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x394890f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x39003caa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x3900d7950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x39005a5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x3948911e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x3948917c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x394891e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x394892b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x3948931e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x394893830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x394893eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x39478d550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x39441ced0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x3900a92d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x39478d860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x39004a7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x3900cfb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x39478db20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x39009a1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x39441d190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x39478e0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x39478e660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x390067590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x3948943b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x394894da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x3948953c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x390067850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x390067b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x394895680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x394896300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x394896920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x394896fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x394897260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x394897bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x394898230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x394898830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x394898eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x3948994b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x3900b4490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x3900b4750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x3900b4a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x3900bbdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x3900495d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x39441d7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x3900735e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x3900c9650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x394899b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x3900efa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x39489a450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x39441dd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x39489aaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x39489b0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11062aaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x39441e300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x3900a1a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x390061d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x39441e8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x39009c220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x39441ee40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x39478e920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x39489b4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x39441f3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x39489c1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x39441f980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x39441ff20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x39003c4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x39006dee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x39478ebe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x39478f350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x39478f8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x39478fe90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x394790430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x3947906f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x394790d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x3900bd170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x3900a25a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x39489c860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x39004c870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x39489cc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x39489d1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x394791300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x3947918a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x394791e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x3947923e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x3947926a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x3944201e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x394420780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x39489d760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x39489da20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x394420d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x39489e230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x394420fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x3944216e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x39489ea90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x39489f140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x39489f740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x3900f0610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x39489fdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x394421c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x390085990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x3900d2e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x3948a01b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x394792960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x3900eb790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x394421f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x394792c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x390063140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x3944227b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x394792f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x3947932c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x3900b12b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x3948a0470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x3948a09c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x3948a16f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x390044b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x3900e1e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x394422310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x394422a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x394422e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x394423db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x394424350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x3944248f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x394424bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x39000ed50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x39000f010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x39000f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x3947938d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x394424e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x394425890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x394425e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x3948a19b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x39000f590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x3948a2120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x39000fb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x39000fe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x3900100d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x390010390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x390010650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x390010910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x390010bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x3944263d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x3948a1cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x394793f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x394794430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x390010e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x394795940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x394426970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x394426f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x390011150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x390011410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x394795c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x394796260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x3944271d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x3944275d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x394427f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x3948a26a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x3900116d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x390011990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x394427ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x394428570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x394428f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x3944294b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x394796650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x394796dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x394797360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x394797900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x394429770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x394429eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x3948a2b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x3948a3460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x3948a3fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3948a3880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x394797bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x394798370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x390011c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x390011f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x394797ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x3947989c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x394798f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x3900121d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x3948a42f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x3948a4a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x390012490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x390012750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x394429a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x39442a550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x394799330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x394799a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x39479a450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x39479a9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x39479af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x3948a5500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x39479b250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x390012a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x390012cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x390012f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x390013250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x390013510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x39479b510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x39479bb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x3900137d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x390013a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x39479c090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x39479c610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x39479cfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x390013d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x39442afe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x39442b600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x390014010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x3948a5b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x3948a5e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x39479d550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x39479daf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x3900142d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x390014590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x39479ddb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x390014850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x390014b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x390014dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x390015090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x390015350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x390015610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x3948a6150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x3948a6910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x3948a73c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x3948a7a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x39442bd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x39442c380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x39442c9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x39479e620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x3900158d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x39442cd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x39442d190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x39442dc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x39442e2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x39442e5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x3948a82e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x3948a8900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x39479e180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x39479ed20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x39479e8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x39479f220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x39479f6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x39442eb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x39442f130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x39442f760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x39442fd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x394430310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x3944308b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x3948a8f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x3947a0720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x3948a9530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x3947a0cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x3947a0f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x3948a97f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x394430ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x394431490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x394431a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x3947a17b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x3947a1350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x3948aa030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x3947a22a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x3947a2840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x390015b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3947a2de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x3947a3600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x3948aa870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x3948ab1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x390015e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x3948aba00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x3948abfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x3948ac540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x3948acae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x390016110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x3948ad110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x3948ad6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x3947a3ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x3900163d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x394432120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x3944326c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x394432c60 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   300.01 MiB\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 171 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   20255.56 ms /  1764 tokens (   11.48 ms per token,    87.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17992.06 ms /   279 runs   (   64.49 ms per token,    15.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   38329.99 ms /  2043 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1707 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   19732.90 ms /  1707 tokens (   11.56 ms per token,    86.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24856.17 ms /   343 runs   (   72.47 ms per token,    13.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   44700.02 ms /  2050 tokens\n",
      "llama_perf_context_print:    graphs reused =        331\n",
      "Llama.generate: 11 prefix-match hit, remaining 1996 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   22237.88 ms /  1996 tokens (   11.14 ms per token,    89.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29923.92 ms /   414 runs   (   72.28 ms per token,    13.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   52305.92 ms /  2410 tokens\n",
      "llama_perf_context_print:    graphs reused =        400\n",
      "Llama.generate: 11 prefix-match hit, remaining 1923 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   26616.81 ms /  1923 tokens (   13.84 ms per token,    72.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32318.37 ms /   373 runs   (   86.64 ms per token,    11.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   59062.71 ms /  2296 tokens\n",
      "llama_perf_context_print:    graphs reused =        360\n",
      "Llama.generate: 11 prefix-match hit, remaining 1799 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   21311.83 ms /  1799 tokens (   11.85 ms per token,    84.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23779.01 ms /   352 runs   (   67.55 ms per token,    14.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   45189.78 ms /  2151 tokens\n",
      "llama_perf_context_print:    graphs reused =        340\n",
      "Llama.generate: 11 prefix-match hit, remaining 2023 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   21654.70 ms /  2023 tokens (   10.70 ms per token,    93.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33248.65 ms /   484 runs   (   68.70 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   55059.11 ms /  2507 tokens\n",
      "llama_perf_context_print:    graphs reused =        468\n",
      "Llama.generate: 12 prefix-match hit, remaining 2044 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   22293.43 ms /  2044 tokens (   10.91 ms per token,    91.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32051.93 ms /   487 runs   (   65.82 ms per token,    15.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   54493.50 ms /  2531 tokens\n",
      "llama_perf_context_print:    graphs reused =        471\n",
      "Llama.generate: 12 prefix-match hit, remaining 1918 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   19743.02 ms /  1918 tokens (   10.29 ms per token,    97.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24733.86 ms /   385 runs   (   64.24 ms per token,    15.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   44580.65 ms /  2303 tokens\n",
      "llama_perf_context_print:    graphs reused =        372\n",
      "Llama.generate: 11 prefix-match hit, remaining 1770 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   18161.10 ms /  1770 tokens (   10.26 ms per token,    97.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25457.92 ms /   399 runs   (   63.80 ms per token,    15.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   43727.99 ms /  2169 tokens\n",
      "llama_perf_context_print:    graphs reused =        385\n",
      "Llama.generate: 11 prefix-match hit, remaining 1715 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   17573.37 ms /  1715 tokens (   10.25 ms per token,    97.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20106.08 ms /   319 runs   (   63.03 ms per token,    15.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   37758.54 ms /  2034 tokens\n",
      "llama_perf_context_print:    graphs reused =        308\n",
      "Llama.generate: 11 prefix-match hit, remaining 1789 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   18096.17 ms /  1789 tokens (   10.12 ms per token,    98.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20648.13 ms /   283 runs   (   72.96 ms per token,    13.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   38817.50 ms /  2072 tokens\n",
      "llama_perf_context_print:    graphs reused =        273\n",
      "Llama.generate: 11 prefix-match hit, remaining 1959 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   21069.44 ms /  1959 tokens (   10.76 ms per token,    92.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57511.00 ms /   699 runs   (   82.28 ms per token,    12.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   78893.52 ms /  2658 tokens\n",
      "llama_perf_context_print:    graphs reused =        676\n",
      "Llama.generate: 11 prefix-match hit, remaining 1748 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   19044.37 ms /  1748 tokens (   10.89 ms per token,    91.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19626.25 ms /   301 runs   (   65.20 ms per token,    15.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   38748.99 ms /  2049 tokens\n",
      "llama_perf_context_print:    graphs reused =        290\n",
      "Llama.generate: 11 prefix-match hit, remaining 1856 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   19227.70 ms /  1856 tokens (   10.36 ms per token,    96.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23937.41 ms /   334 runs   (   71.67 ms per token,    13.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   43260.64 ms /  2190 tokens\n",
      "llama_perf_context_print:    graphs reused =        323\n",
      "Llama.generate: 11 prefix-match hit, remaining 1881 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   21171.26 ms /  1881 tokens (   11.26 ms per token,    88.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20173.49 ms /   279 runs   (   72.31 ms per token,    13.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   41420.83 ms /  2160 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1938 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   25756.49 ms /  1938 tokens (   13.29 ms per token,    75.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50736.08 ms /   699 runs   (   72.58 ms per token,    13.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   76780.30 ms /  2637 tokens\n",
      "llama_perf_context_print:    graphs reused =        676\n",
      "Llama.generate: 11 prefix-match hit, remaining 1842 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   25234.27 ms /  1842 tokens (   13.70 ms per token,    73.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21686.00 ms /   313 runs   (   69.28 ms per token,    14.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   47007.09 ms /  2155 tokens\n",
      "llama_perf_context_print:    graphs reused =        302\n",
      "Llama.generate: 11 prefix-match hit, remaining 1898 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   21977.18 ms /  1898 tokens (   11.58 ms per token,    86.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21434.42 ms /   317 runs   (   67.62 ms per token,    14.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   43499.10 ms /  2215 tokens\n",
      "llama_perf_context_print:    graphs reused =        306\n",
      "Llama.generate: 11 prefix-match hit, remaining 1983 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   24457.86 ms /  1983 tokens (   12.33 ms per token,    81.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25619.11 ms /   364 runs   (   70.38 ms per token,    14.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   50184.13 ms /  2347 tokens\n",
      "llama_perf_context_print:    graphs reused =        352\n",
      "Llama.generate: 11 prefix-match hit, remaining 2050 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   29033.69 ms /  2050 tokens (   14.16 ms per token,    70.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27738.64 ms /   394 runs   (   70.40 ms per token,    14.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   56891.10 ms /  2444 tokens\n",
      "llama_perf_context_print:    graphs reused =        381\n",
      "Llama.generate: 11 prefix-match hit, remaining 1804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   23631.80 ms /  1804 tokens (   13.10 ms per token,    76.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24957.30 ms /   382 runs   (   65.33 ms per token,    15.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   48696.98 ms /  2186 tokens\n",
      "llama_perf_context_print:    graphs reused =        369\n",
      "Llama.generate: 11 prefix-match hit, remaining 2062 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =   21342.61 ms /  2062 tokens (   10.35 ms per token,    96.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24528.84 ms /   372 runs   (   65.94 ms per token,    15.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   45970.17 ms /  2434 tokens\n",
      "llama_perf_context_print:    graphs reused =        359\n",
      "Llama.generate: 11 prefix-match hit, remaining 853 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20258.85 ms\n",
      "llama_perf_context_print: prompt eval time =    8393.95 ms /   853 tokens (    9.84 ms per token,   101.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15106.81 ms /   254 runs   (   59.48 ms per token,    16.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   23558.87 ms /  1107 tokens\n",
      "llama_perf_context_print:    graphs reused =        246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote final JSON to: /Users/laurenbrixey/Documents/GitHub Repositories/cam_ds_ep_FinSight/notebooks/jpm_mistral_pra_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Runner code.\n",
    "pra_categories = load_pra_categories(Path(PRA_NOTES_PATH))\n",
    "embedder, category_embeddings = build_embedding_index(pra_categories)\n",
    "\n",
    "# Load and chunk transcript\n",
    "transcript_text = Path(TRANSCRIPT_PATH).read_text(encoding=\"utf-8\")\n",
    "transcript_chunks = chunk_text(transcript_text)\n",
    "\n",
    "n_threads = max(4, (os.cpu_count() or 8) - 2)\n",
    "\n",
    "# Define the model.\n",
    "model = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=20,\n",
    "    chat_format=\"mistral-instruct\",\n",
    "    n_threads=n_threads,\n",
    ")\n",
    "\n",
    "raw_outputs = []\n",
    "\n",
    "for i, chunk in enumerate(transcript_chunks, 1):\n",
    "    try:\n",
    "        top_categories = find_rel_categories(\n",
    "            chunk, pra_categories, embedder, category_embeddings, top_k=TOP_K\n",
    "        )\n",
    "        _, raw = summarise_chunk(\n",
    "            model, chunk, top_categories, i, max_evidence=5\n",
    "        )\n",
    "        raw_outputs.append({\"chunk\": i, \"raw\": raw})\n",
    "\n",
    "    except Exception:\n",
    "        raw_outputs.append({\"chunk\": i, \"raw\": \"\"})\n",
    "\n",
    "final_output = {\"raw_outputs\": raw_outputs}\n",
    "\n",
    "OUTPUT_PATH.write_text(json.dumps(final_output, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(f\"Wrote final JSON to: {OUTPUT_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ff6d7",
   "metadata": {},
   "source": [
    "- Need to preprocess the output so it is visually clearer (summary, evidence, PRA categories (name & why the model chose this))\n",
    "- Can this information be fed to the model again and can it detect any early PRA risk indicators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9d93f",
   "metadata": {},
   "source": [
    "# **6. Evasion Scoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654e09d",
   "metadata": {},
   "source": [
    "- Use LLM to summarise answer and then tag with an evasion score.\n",
    "- Detect evasiveness of bankers in relation to analyst questions and give an evasiveness score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb359de2",
   "metadata": {},
   "source": [
    "## **6.1 Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair each analyst question with all the banker's answers.\n",
    "def create_qa_pairs(df):\n",
    "    questions = df[df['role_normalised'] == 'analyst']\n",
    "    answers = df[df['role_normalised'] == 'banker']\n",
    "\n",
    "    qa_pairs = []\n",
    "\n",
    "    for q_num, q_row in questions.groupby('question_number'):\n",
    "        q_text = ' '.join(q_row['content'].astype(str))\n",
    "        a_rows = answers[answers['question_number'] == q_num]\n",
    "        if not a_rows.empty:\n",
    "            a_text = ' '.join(a_rows['content'].astype(str))\n",
    "            qa_pairs.append({\n",
    "                'question_number': q_num, \n",
    "                'question': q_text,\n",
    "                'answer': a_text\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8ff9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes. Good morning. This question is for Jamie....</td>\n",
       "      <td>I just – before Jamie answers that, Erika, I j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Got it. And a second follow-up question. And I...</td>\n",
       "      <td>Yeah, Erika, it's a good question. But the tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you. Operator: Thank you. Our next quest...</td>\n",
       "      <td>Thanks, Erika. Operator: I apologize. Our next...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  Good morning, Jeremy. Wondering if you could s...   \n",
       "1                2  Yeah. And just one question on the NII ex. Mar...   \n",
       "2                3  Yes. Good morning. This question is for Jamie....   \n",
       "3                4  Got it. And a second follow-up question. And I...   \n",
       "4                5  Thank you. Operator: Thank you. Our next quest...   \n",
       "\n",
       "                                              answer  \n",
       "0  Sure, Ken. So I mean, at a high level, I would...  \n",
       "1  Yeah, that's a good question, Ken. You're righ...  \n",
       "2  I just – before Jamie answers that, Erika, I j...  \n",
       "3  Yeah, Erika, it's a good question. But the tru...  \n",
       "4  Thanks, Erika. Operator: I apologize. Our next...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create qa pairs.\n",
    "jpm_2025_qa_pairs_df = create_qa_pairs(jpm_2025_df)\n",
    "\n",
    "# View the results.\n",
    "jpm_2025_qa_pairs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f97b9",
   "metadata": {},
   "source": [
    "## **6.2 Evasion Detection (prototype)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031e1b4",
   "metadata": {},
   "source": [
    "1. **Baseline Evasion score** (rule-based) is made up of three components:\n",
    "- **Cosine similarity**- similarity of the question and answer, lower similarity = more evasive\n",
    "- **Numeric specificity check**- does the question require a number, if so does the answer contain a number?, e.g. requests for financial data\n",
    "- **Evasive phrases**- does the answer contain evasive phrases?, presence = more evasive\n",
    "\n",
    "2. **LLM evasion score** (RoBERTa-MNLI) uses entailment/neutral/contradiction between the question and answer\n",
    "- Lower entailment (and higher neutral + contradiction) = more evasive\n",
    "  \n",
    "3. **Blended evasion score** combines both scores including a weight for the LLM component\n",
    "- Rationale is that baseline enforces precision while the LLM will capture semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a717495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import model and tokenizer.\n",
    "model_name = \"roberta-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# RoBERTa has 3 labels\n",
    "assert model.config.num_labels == 3\n",
    "\n",
    "# Label order for roberta-large-mnli\n",
    "id2label = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (change this if not using a macbook) \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Metal\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c60cf",
   "metadata": {},
   "source": [
    "### **Baseline evasion score functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c22283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of evasive phrases\n",
    "EVASIVE_PHRASES = [\n",
    "    r\"\\btoo early\\b\",\n",
    "    r\"\\bcan't (?:comment|share|discuss)\\b\",\n",
    "    r\"\\bwon't (?:comment|share|provide)\\b\",\n",
    "    r\"\\bno (?:update|comment)\\b\",\n",
    "    r\"\\bwe (?:don't|do not) (?:break out|provide guidance)\\b\",\n",
    "    r\"\\bnot (?:going to|able to) (?:comment|share|provide)\\b\",\n",
    "    r\"\\bwe'll (?:come back|circle back)\\b\",\n",
    "    r\"\\bnot something we disclose\\b\",\n",
    "    r\"\\bas (?:we|I) (?:said|mentioned)\\b\",\n",
    "    r\"\\bgenerally speaking\\b\",\n",
    "    r\"\\bit's premature\\b\",\n",
    "    r\"\\bit's difficult to say\\b\",\n",
    "    r\"\\bI (?:wouldn't|won't) want to (?:speculate|get into)\\b\",\n",
    "    r\"\\bI (?:think|guess|suppose)\\b\",\n",
    "    r\"\\bkind of\\b\",\n",
    "    r\"\\bsort of\\b\",\n",
    "    r\"\\baround\\b\",\n",
    "    r\"\\broughly\\b\",\n",
    "    r\"\\bwe (?:prefer|plan) not to\\b\",\n",
    "    r\"\\bwe're not prepared to\\b\",\n",
    "]\n",
    "\n",
    "# List of words that suggest the answer needs specific financial numbers to properly answer the question.\n",
    "SPECIFICITY_TRIGGERS = [\n",
    "    \"how much\",\"how many\",\"what is\",\"what are\",\"when\",\"which\",\"where\",\"who\",\"why\",\n",
    "    \"range\",\"guidance\",\"margin\",\"capex\",\"opex\",\"revenue\",\"sales\",\"eps\",\"ebitda\",\n",
    "    \"timeline\",\"date\",\"target\",\"growth\",\"update\",\"split\",\"dividend\",\"cost\",\"price\",\n",
    "    \"units\",\"volumes\",\"gross\",\"net\",\"tax\",\"percentage\",\"utilization\",\"order book\"\n",
    "]\n",
    "\n",
    "NUMERIC_PATTERN = r\"(?:\\d+(?:\\.\\d+)?%|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b|£|\\$|€)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between question and answers.\n",
    "def cosine_sim(q, a):\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform([q, a]) # converts text to vectors \n",
    "    sim = float(cosine_similarity(vec[0], vec[1])[0, 0]) # calculate the cosine similarity between the two vectors\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute baseline evasion score.\n",
    "def baseline_evasion_score(q, a):\n",
    "    # 1. Cosine similarity\n",
    "    sim = cosine_sim(q, a) # calculates cosine similarity using previous function\n",
    "    sim_component = (1 - sim) * 45 # less similar the answer is, the bigger the contribution to the evasion score, scaled by 45\n",
    "\n",
    "    # 2. Numerical specificity- Does the question require and answer with financial figures/ a specific answer?\n",
    "    needs_num = any(t in q.lower() for t in SPECIFICITY_TRIGGERS) # true if the question requires a numeric/ specific answer\n",
    "    has_num = bool(re.search(NUMERIC_PATTERN, a)) # true if the answer includes a number \n",
    "    numeric_component = 25 if needs_num and not has_num else 0 # score of 25 if the question needs a number but the answer doesn't give one\n",
    "\n",
    "    # 3. Evasive phrases- does the answer contain evasive phrases?\n",
    "    phrase_hits = sum(len(re.findall(p, a.lower())) for p in EVASIVE_PHRASES) # counts how many times an evasive phrase appears in the answer\n",
    "    phrase_component = min(3, phrase_hits) * 8 # max of 3 hits counted, each hit = 8 points \n",
    "\n",
    "    # Final evasion score.\n",
    "    score = min(100, sim_component + numeric_component + phrase_component) # adds components together and caps score at 100\n",
    "    \n",
    "    return score, sim, phrase_hits, needs_num, has_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f5e81",
   "metadata": {},
   "source": [
    "### **LLM and blended evasion score functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f83a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute llm label scores: entailment, contradiction and neutral\n",
    "def llm_label_scores(question, answer):\n",
    "\n",
    "    # Function to calculate probabilities based on RoBERTa-MNLI labels.\n",
    "    def probs(premise, hypothesis):\n",
    "        inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, max_length=512).to(device)  # tokenize the input\n",
    "        with torch.no_grad():  # disable gradient tracking \n",
    "            logits = model(**inputs).logits  # runs the model and outputs raw scores\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).squeeze().tolist()  # converts raw scores to probabilities\n",
    "\n",
    "        # RoBERTa-MNLI label order: [contradiction, neutral, entailment]\n",
    "        # contradiction = contradicts the question, neutral = related but not committing, entailment = directly answers the question\n",
    "        return {\"contradiction\": probs[0], \"neutral\": probs[1], \"entailment\": probs[2]}\n",
    "\n",
    "    # Get probability in both directions (e.g. does answer entail question, does question entail answer?)\n",
    "    pA, pB = probs(answer, question), probs(question, answer)\n",
    "\n",
    "    # Calculates scores.\n",
    "    entail = math.sqrt(max(1e-9, pA[\"entailment\"] * pB[\"entailment\"]))\n",
    "    neutral = 0.5 * (pA[\"neutral\"] + pB[\"neutral\"])\n",
    "    contradiction = 0.5 * (pA[\"contradiction\"] + pB[\"contradiction\"])\n",
    "\n",
    "    # Normalises the scores.\n",
    "    s = entail + neutral + contradiction\n",
    "    entail, neutral, contradiction = entail/s, neutral/s, contradiction/s\n",
    "    return {\n",
    "        \"entailment\": entail,\n",
    "        \"neutral\": neutral,\n",
    "        \"contradiction\": contradiction\n",
    "    }\n",
    "\n",
    "# Function to compute LLM evasion score from the label scores.\n",
    "def llm_evasion_score(entail, neutral, contradiction):\n",
    "    # Maps (entail, neutral, contradiction) -> evasion score in range 0..100\n",
    "    evasion = (1 - entail) * 60 + neutral * 30 + contradiction * 10\n",
    "    return max(0.0, min(100.0, evasion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute blended evasion score and return all scores.\n",
    "def compute_all_evasion_scores(q, a, LLM_WEIGHT=0.30):\n",
    "    \n",
    "    # Compute baseline evasion score.\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # Compute LLM evasion score.\n",
    "    llm_label = llm_label_scores(q, a)\n",
    "    llm_score = llm_evasion_score(llm_label['entailment'], llm_label['neutral'], llm_label['contradiction'])\n",
    "\n",
    "    # Compute blended score.\n",
    "    blended_score = max(0.0, min(100.0, base_score + LLM_WEIGHT * llm_score))\n",
    "\n",
    "    return {\n",
    "        'baseline': float(base_score),\n",
    "        'llm_only': float(llm_score),\n",
    "        'blended': float(blended_score)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7618e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label based on the score.\n",
    "def label_from_score(score, threshold):\n",
    "    return 'Evasive' if score >= threshold else 'Direct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1fd47",
   "metadata": {},
   "source": [
    "### **Main Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f819db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds.\n",
    "LLM_WEIGHT = 0.30\n",
    "EVASION_THRESHOLD_BASE = 60.0\n",
    "EVASION_THRESHOLD_LLM = 50.0\n",
    "EVASION_THRESHOLD_BLENDED = 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evasion Pipeline.\n",
    "def evasion_pipeline(df):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row[\"question\"]), str(row[\"answer\"])\n",
    "        out = compute_all_evasion_scores(q, a)\n",
    "\n",
    "        pred_base = label_from_score(out[\"baseline\"], EVASION_THRESHOLD_BASE)\n",
    "        pred_llm = label_from_score(out[\"llm_only\"], EVASION_THRESHOLD_LLM)\n",
    "        pred_blended = label_from_score(out[\"blended\"], EVASION_THRESHOLD_BLENDED)\n",
    "\n",
    "        records.append({\n",
    "            \"question_number\": row.get(\"question_number\"),\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "\n",
    "            # Evasion Scores\n",
    "            \"evasion_score_baseline\": int(out[\"baseline\"]),\n",
    "            \"evasion_score_llm\": int(out[\"llm_only\"]),\n",
    "            \"evasion_score_blended\": int(out[\"blended\"]),\n",
    "\n",
    "            # Predicted labels.\n",
    "            \"prediction_baseline\": pred_base,\n",
    "            \"prediction_llm\": pred_llm,\n",
    "            \"prediction_blended\": pred_blended,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cfd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_llm</th>\n",
       "      <th>prediction_blended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>44</td>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes. Good morning. This question is for Jamie....</td>\n",
       "      <td>I just – before Jamie answers that, Erika, I j...</td>\n",
       "      <td>35</td>\n",
       "      <td>75</td>\n",
       "      <td>57</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Got it. And a second follow-up question. And I...</td>\n",
       "      <td>Yeah, Erika, it's a good question. But the tru...</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you. Operator: Thank you. Our next quest...</td>\n",
       "      <td>Thanks, Erika. Operator: I apologize. Our next...</td>\n",
       "      <td>67</td>\n",
       "      <td>73</td>\n",
       "      <td>89</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  Good morning, Jeremy. Wondering if you could s...   \n",
       "1                2  Yeah. And just one question on the NII ex. Mar...   \n",
       "2                3  Yes. Good morning. This question is for Jamie....   \n",
       "3                4  Got it. And a second follow-up question. And I...   \n",
       "4                5  Thank you. Operator: Thank you. Our next quest...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Sure, Ken. So I mean, at a high level, I would...                      64   \n",
       "1  Yeah, that's a good question, Ken. You're righ...                      44   \n",
       "2  I just – before Jamie answers that, Erika, I j...                      35   \n",
       "3  Yeah, Erika, it's a good question. But the tru...                      88   \n",
       "4  Thanks, Erika. Operator: I apologize. Our next...                      67   \n",
       "\n",
       "   evasion_score_llm  evasion_score_blended prediction_baseline  \\\n",
       "0                 70                     85             Evasive   \n",
       "1                 73                     66              Direct   \n",
       "2                 75                     57              Direct   \n",
       "3                 81                    100             Evasive   \n",
       "4                 73                     89             Evasive   \n",
       "\n",
       "  prediction_llm prediction_blended  \n",
       "0        Evasive            Evasive  \n",
       "1        Evasive            Evasive  \n",
       "2        Evasive             Direct  \n",
       "3        Evasive            Evasive  \n",
       "4        Evasive            Evasive  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evasion pipeline.\n",
    "jpm_2025_evasion_results = evasion_pipeline(jpm_2025_qa_pairs_df)\n",
    "\n",
    "# View results.\n",
    "jpm_2025_evasion_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c0cfa",
   "metadata": {},
   "source": [
    "### **Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43c32e",
   "metadata": {},
   "source": [
    "- Built a test set of 26 examples from 2023 1q jpm results, human-labelled these evasive or direct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f492696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2023 transcript.\n",
    "jpm_1q_23_df = pd.read_csv('../data/processed/jpm/jpm-1q23-earnings-call-transcript_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_1q_23_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22775960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "1            qa              NaN            NaN  Steven Chubak   \n",
       "2            qa              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "1                             analyst    Wolfe Research LLC   \n",
       "2             Chief Financial Officer  JPMorgan Chase & Co.   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "1                                 Hey, good morning.  2023      Q1   \n",
       "2                               Good morning, Steve.  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "\n",
       "   is_pleasantry role_normalised  \n",
       "0          False          banker  \n",
       "1           True         analyst  \n",
       "2           True          banker  \n",
       "3          False         analyst  \n",
       "4          False          banker  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_1q_23_df['role_normalised'] = jpm_1q_23_df['role'].map(role_map)\n",
    "\n",
    "# View the dataset.\n",
    "jpm_1q_23_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955b5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>...elevated macro uncertainties.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matt O'Connor</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Deutsche Bank Securities, Inc.</td>\n",
       "      <td>Okay. And then just separately to squeeze in –...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>That'll be every quarter for the rest of our l...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>qa</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matt O'Connor</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Deutsche Bank Securities, Inc.</td>\n",
       "      <td>Okay. All right. Thank you. Operator: We have ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   section  question_number  answer_number   speaker_name  \\\n",
       "3       qa              1.0            NaN  Steven Chubak   \n",
       "4       qa              1.0            1.0    Jamie Dimon   \n",
       "5       qa              1.0            1.0  Steven Chubak   \n",
       "6       qa              1.0            2.0    Jamie Dimon   \n",
       "7       qa              1.0            2.0  Steven Chubak   \n",
       "..     ...              ...            ...            ...   \n",
       "93      qa             26.0            NaN  Matt O'Connor   \n",
       "94      qa             26.0            1.0    Jamie Dimon   \n",
       "95      qa             26.0            2.0  Jeremy Barnum   \n",
       "96      qa             26.0            3.0    Jamie Dimon   \n",
       "97      qa             27.0            NaN  Matt O'Connor   \n",
       "\n",
       "                                  role                         company  \\\n",
       "3                              analyst              Wolfe Research LLC   \n",
       "4   Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "5                              analyst              Wolfe Research LLC   \n",
       "6   Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "7                              analyst              Wolfe Research LLC   \n",
       "..                                 ...                             ...   \n",
       "93                             analyst  Deutsche Bank Securities, Inc.   \n",
       "94  Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "95             Chief Financial Officer            JPMorgan Chase & Co.   \n",
       "96  Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "97                             analyst  Deutsche Bank Securities, Inc.   \n",
       "\n",
       "                                              content  year quarter  \\\n",
       "3   So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4   Well, I think you were already kind of complet...  2023      Q1   \n",
       "5   Got it. And just in terms of appetite for the ...  2023      Q1   \n",
       "6                                           Oh, yeah.  2023      Q1   \n",
       "7                    ...elevated macro uncertainties.  2023      Q1   \n",
       "..                                                ...   ...     ...   \n",
       "93  Okay. And then just separately to squeeze in –...  2023      Q1   \n",
       "94  That'll be every quarter for the rest of our l...  2023      Q1   \n",
       "95                                             Cheap.  2023      Q1   \n",
       "96                                             Cheap.  2023      Q1   \n",
       "97  Okay. All right. Thank you. Operator: We have ...  2023      Q1   \n",
       "\n",
       "    is_pleasantry role_normalised  \n",
       "3           False         analyst  \n",
       "4           False          banker  \n",
       "5           False         analyst  \n",
       "6           False          banker  \n",
       "7           False         analyst  \n",
       "..            ...             ...  \n",
       "93          False         analyst  \n",
       "94          False          banker  \n",
       "95          False          banker  \n",
       "96          False          banker  \n",
       "97          False         analyst  \n",
       "\n",
       "[85 rows x 11 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out presention and is_pleasantry == True\n",
    "jpm_1q_23_df = jpm_1q_23_df[jpm_1q_23_df['section'] == 'qa']\n",
    "jpm_1q_23_df = jpm_1q_23_df[jpm_1q_23_df['is_pleasantry'] == False]\n",
    "\n",
    "# View the dataset.\n",
    "jpm_1q_23_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0              1.0  So, Jamie, I was actually hoping to get your p...   \n",
       "1              2.0  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2              3.0  Yeah, and as a follow-up on the point about ra...   \n",
       "3              4.0  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4              5.0  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  \n",
       "0  Well, I think you were already kind of complet...  \n",
       "1  Yeah, sure. So let me just summarize the drive...  \n",
       "2  Well first of all, I don't quite believe it. S...  \n",
       "3  Yeah. John, it's a really good question, and w...  \n",
       "4  Yeah. I wouldn't use the word credit crunch if...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 26\n"
     ]
    }
   ],
   "source": [
    "# Create qa pairs.\n",
    "jpm_1q_23_qa_pairs_df = create_qa_pairs(jpm_1q_23_df)\n",
    "\n",
    "# View dataset.\n",
    "display(jpm_1q_23_qa_pairs_df.head())\n",
    "\n",
    "# View shape.\n",
    "print('Number of samples:', jpm_1q_23_qa_pairs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eacd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test set. \n",
    "jpm_1q_23_test_set_df = jpm_1q_23_qa_pairs_df.copy()\n",
    "\n",
    "# Create a blank label column and export to CSV for human to label.\n",
    "jpm_1q_23_test_set_df['label'] = ''  # fill with Direct or Evasive\n",
    "jpm_1q_23_test_set_df.to_csv('jpm_1q_23_test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacd1e4",
   "metadata": {},
   "source": [
    "- The test set was human labelled with either 'direct' or 'evasive'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer    label  \n",
       "0  Well, I think you were already kind of complet...  Evasive  \n",
       "1  Yeah, sure. So let me just summarize the drive...   Direct  \n",
       "2  Well first of all, I don't quite believe it. S...   Direct  \n",
       "3  Yeah. John, it's a really good question, and w...  Evasive  \n",
       "4  Yeah. I wouldn't use the word credit crunch if...   Direct  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the labelled test set. \n",
    "jpm_1q_23_test_set_labelled_df = pd.read_csv('../notebooks/summarisation_evasion_files/jpm_1q_23_test_set_labelled.csv')\n",
    "\n",
    "# View dataset.\n",
    "jpm_1q_23_test_set_labelled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43f3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_llm</th>\n",
       "      <th>prediction_blended</th>\n",
       "      <th>human_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>100</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>99</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Well, I think you were already kind of complet...                      55   \n",
       "1  Yeah, sure. So let me just summarize the drive...                      80   \n",
       "2  Well first of all, I don't quite believe it. S...                      40   \n",
       "3  Yeah. John, it's a really good question, and w...                      78   \n",
       "4  Yeah. I wouldn't use the word credit crunch if...                      55   \n",
       "\n",
       "   evasion_score_llm  evasion_score_blended prediction_baseline  \\\n",
       "0                 79                     79              Direct   \n",
       "1                 78                    100             Evasive   \n",
       "2                 83                     65              Direct   \n",
       "3                 73                     99             Evasive   \n",
       "4                 81                     80              Direct   \n",
       "\n",
       "  prediction_llm prediction_blended human_label  \n",
       "0        Evasive            Evasive     Evasive  \n",
       "1        Evasive            Evasive      Direct  \n",
       "2        Evasive            Evasive      Direct  \n",
       "3        Evasive            Evasive     Evasive  \n",
       "4        Evasive            Evasive      Direct  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evasion pipeline.\n",
    "jpm_1q_23_evasion_results = evasion_pipeline(jpm_1q_23_test_set_labelled_df)\n",
    "\n",
    "# Reappend the human label.\n",
    "jpm_1q_23_evasion_results['human_label'] = jpm_1q_23_test_set_labelled_df['label']\n",
    "\n",
    "# View results.\n",
    "jpm_1q_23_evasion_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9831d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the evasion scores.\n",
    "def evaluate_evasion_scores(df, thr_base=EVASION_THRESHOLD_BASE, thr_llm=EVASION_THRESHOLD_LLM, thr_blend=EVASION_THRESHOLD_BLENDED):\n",
    "\n",
    "    # Ground truth: 1 = Evasive, 0 = Direct (using 'human_label')\n",
    "    y_true = (df[\"human_label\"].astype(str).str.strip().str.lower() == \"evasive\").astype(int).values\n",
    "\n",
    "    # Convert predicted label strings to binary (1 = Evasive, 0 = Direct)\n",
    "    def to_binary(pred_series):\n",
    "        return (pred_series.astype(str).str.strip().str.lower() == \"evasive\").astype(int).values\n",
    "\n",
    "    y_pred_base  = to_binary(df[\"prediction_baseline\"])\n",
    "    y_pred_llm   = to_binary(df[\"prediction_llm\"])\n",
    "    y_pred_blend = to_binary(df[\"prediction_blended\"])\n",
    "\n",
    "    return {\n",
    "        'baseline': {\n",
    "            'classification_report': classification_report(y_true, y_pred_base, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_base)\n",
    "        },\n",
    "        'llm': {\n",
    "            'classification_report': classification_report(y_true, y_pred_llm, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_llm)\n",
    "        },\n",
    "        'blended': {\n",
    "            'classification_report': classification_report(y_true, y_pred_blend, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_blend) \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results.\n",
    "eval_dict = evaluate_evasion_scores(jpm_1q_23_evasion_results)\n",
    "baseline_eval, llm_eval, blended_eval = eval_dict['baseline'], eval_dict['llm'], eval_dict['blended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.588     0.714     0.645        14\n",
      "     Evasive      0.556     0.417     0.476        12\n",
      "\n",
      "    accuracy                          0.577        26\n",
      "   macro avg      0.572     0.565     0.561        26\n",
      "weighted avg      0.573     0.577     0.567        26\n",
      "\n",
      "[[10  4]\n",
      " [ 7  5]]\n"
     ]
    }
   ],
   "source": [
    "# View baseline results.\n",
    "base_cr, base_cm = baseline_eval['classification_report'], baseline_eval['confusion_matrix']\n",
    "\n",
    "print(base_cr)\n",
    "print(base_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.000     0.000     0.000        14\n",
      "     Evasive      0.462     1.000     0.632        12\n",
      "\n",
      "    accuracy                          0.462        26\n",
      "   macro avg      0.231     0.500     0.316        26\n",
      "weighted avg      0.213     0.462     0.291        26\n",
      "\n",
      "[[ 0 14]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "# View llm results.\n",
    "llm_cr, llm_cm = llm_eval['classification_report'], llm_eval['confusion_matrix']\n",
    "\n",
    "print(llm_cr)\n",
    "print(llm_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      1.000     0.071     0.133        14\n",
      "     Evasive      0.480     1.000     0.649        12\n",
      "\n",
      "    accuracy                          0.500        26\n",
      "   macro avg      0.740     0.536     0.391        26\n",
      "weighted avg      0.760     0.500     0.371        26\n",
      "\n",
      "[[ 1 13]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "# View blended results.\n",
    "blended_cr, blended_cm = blended_eval['classification_report'], blended_eval['confusion_matrix']\n",
    "\n",
    "print(blended_cr)\n",
    "print(blended_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80e12c",
   "metadata": {},
   "source": [
    "- Need to fine tune threshold using grid search to improve accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
