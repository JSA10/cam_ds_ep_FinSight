{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2891bcd0-6579-4b69-91da-7b1d13eba18a",
   "metadata": {},
   "source": [
    "# JPMorgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfabdd40-8613-4696-abd8-8a41844cf6fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T13:43:54.387980Z",
     "iopub.status.busy": "2025-09-08T13:43:54.387583Z",
     "iopub.status.idle": "2025-09-08T13:43:54.661563Z",
     "shell.execute_reply": "2025-09-08T13:43:54.661309Z",
     "shell.execute_reply.started": "2025-09-08T13:43:54.387929Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"scripts\")  # folder that contains jpm_earnings_qa_parser.py\n",
    "\n",
    "from jpm_earnings_qa_parser import batch_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08f1a61-581c-4c6e-a170-4be3a6374efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-08T13:43:54.661940Z",
     "iopub.status.busy": "2025-09-08T13:43:54.661841Z",
     "iopub.status.idle": "2025-09-08T13:44:05.688939Z",
     "shell.execute_reply": "2025-09-08T13:44:05.688727Z",
     "shell.execute_reply.started": "2025-09-08T13:43:54.661933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing: data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earnings-call-transcript-checkpoint.pdf\n",
      "Extraction method for jpm-1q23-earnings-call-transcript-checkpoint.pdf: pdfminer\n",
      "  -> 98 rows written to data/processed/jpm/jpm-1q23-earnings-call-transcript-checkpoint_qa.csv\n",
      "Parsing: data/raw/jpm/.ipynb_checkpoints/jpm-1q25-earnings-call-transcript-checkpoint.pdf\n",
      "Extraction method for jpm-1q25-earnings-call-transcript-checkpoint.pdf: pdfminer\n",
      "  -> 112 rows written to data/processed/jpm/jpm-1q25-earnings-call-transcript-checkpoint_qa.csv\n",
      "Parsing: data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earnings-call-transcript-checkpoint.pdf\n",
      "Extraction method for jpm-2q25-earnings-call-transcript-checkpoint.pdf: pdfminer\n",
      "  -> 149 rows written to data/processed/jpm/jpm-2q25-earnings-call-transcript-checkpoint_qa.csv\n",
      "Parsing: data/raw/jpm/.ipynb_checkpoints/jpm-3q23-earnings-call-transcript-checkpoint.pdf\n",
      "Extraction method for jpm-3q23-earnings-call-transcript-checkpoint.pdf: pdfminer\n",
      "  -> 89 rows written to data/processed/jpm/jpm-3q23-earnings-call-transcript-checkpoint_qa.csv\n",
      "Parsing: data/raw/jpm/.ipynb_checkpoints/jpm-4q24-earnings-transcript-checkpoint.pdf\n",
      "Extraction method for jpm-4q24-earnings-transcript-checkpoint.pdf: pdfminer\n",
      "  -> 64 rows written to data/processed/jpm/jpm-4q24-earnings-transcript-checkpoint_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-1q23-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-1q23-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 98 rows written to data/processed/jpm/jpm-1q23-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-1q24-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-1q24-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 76 rows written to data/processed/jpm/jpm-1q24-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-1q25-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-1q25-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 112 rows written to data/processed/jpm/jpm-1q25-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-2q23-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-2q23-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 99 rows written to data/processed/jpm/jpm-2q23-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-2q24-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-2q24-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 67 rows written to data/processed/jpm/jpm-2q24-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-2q25-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-2q25-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 149 rows written to data/processed/jpm/jpm-2q25-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-3q23-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-3q23-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 89 rows written to data/processed/jpm/jpm-3q23-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-3q24-earnings-conference-call-transcript.pdf\n",
      "Extraction method for jpm-3q24-earnings-conference-call-transcript.pdf: pdfminer\n",
      "  -> 84 rows written to data/processed/jpm/jpm-3q24-earnings-conference-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-4q23-earnings-call-transcript.pdf\n",
      "Extraction method for jpm-4q23-earnings-call-transcript.pdf: pdfminer\n",
      "  -> 61 rows written to data/processed/jpm/jpm-4q23-earnings-call-transcript_qa.csv\n",
      "Parsing: data/raw/jpm/jpm-4q24-earnings-transcript.pdf\n",
      "Extraction method for jpm-4q24-earnings-transcript.pdf: pdfminer\n",
      "  -> 64 rows written to data/processed/jpm/jpm-4q24-earnings-transcript_qa.csv\n",
      "Wrote combined CSV (1411 rows) -> data/processed/jpm/all_jpm_2023_2025.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/processed/jpm/all_jpm_2023_2025.csv')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch a folder (writes per-file CSVs and a combined CSV)\n",
    "batch_process(\n",
    "    input_path=Path(\"data/raw/jpm\"),\n",
    "    outdir=Path(\"data/processed/jpm\"),\n",
    "    recursive=True,\n",
    "    pattern=\"*.pdf\",\n",
    "    combined_out=Path(\"data/processed/jpm/all_jpm_2023_2025.csv\"),\n",
    "    prefer=\"pymupdf\",\n",
    "    ocr=True,\n",
    "    include_presentation=\"single_row\",\n",
    "    pleasantry_mode=\"label_and_resequence\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3eefa5-cbc3-4370-815d-a0857dbd9ee6",
   "metadata": {},
   "source": [
    "# HSBC\n",
    "\n",
    "HSBC transcripts required more work to process as they were not as cleanly structured as the JPM ones\n",
    "\n",
    "NOTE: The below code was a standalone notebook ran on 2025-09-09, have combined into one notebook for submission, but not rerun the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b07c2-a428-4ef5-b3bf-fee6b0bdeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "packages = ['PyPDF2', 'pandas', 'pathlib']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "print(\"‚úÖ All required packages are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30d007-c78e-4237-80b9-f77430f09037",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"scripts\") \n",
    "# Import the parser \n",
    "from hsbc_earnings_qa_parser import HSBCEarningsParser\n",
    "\n",
    "# Initialize the parser\n",
    "parser = HSBCEarningsParser()\n",
    "print(\"‚úÖ Parser initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed33791-e8f1-4ff5-9908-4f8f438e09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple approach - navigate from where your notebook is\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# If your notebook is in the same project folder, try this:\n",
    "project_root = Path.cwd()\n",
    "while project_root.name != \"cam_ds_ep_FinSight\" and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if project_root.name == \"cam_ds_ep_FinSight\":\n",
    "    input_dir = project_root / \"data\" / \"raw\" / \"hsbc\"\n",
    "    output_dir = project_root / \"data\" / \"processed\" / \"hsbc\"\n",
    "    print(f\"‚úÖ Found project root: {project_root}\")\n",
    "else:\n",
    "    # Fallback to manual path\n",
    "    input_dir = \"Documents/2. learn_data-science/cam_ds_course_4_ep/cam_ds_ep_FinSight/data/raw/hsbc\"  # Replace with your actual path\n",
    "    output_dir = \"Documents/2. learn_data-science/cam_ds_course_4_ep/cam_ds_ep_FinSight/data/processed/hsbc\"  # Replace with your actual path\n",
    "\n",
    "print(f\"üìÅ Input directory: {input_dir}\")\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "\n",
    "# Test the path\n",
    "if os.path.exists(str(input_dir)):\n",
    "    pdf_files = [f for f in os.listdir(str(input_dir)) if f.lower().endswith('.pdf')]\n",
    "    print(f\"‚úÖ Found {len(pdf_files)} PDF files!\")\n",
    "else:\n",
    "    print(f\"‚ùå Still can't find the directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbbf34-0a1c-4174-b10d-8675c2bc49d6",
   "metadata": {},
   "source": [
    " Found project root: /Users/jerome.ahye/Documents/2. learn_data-science/cam_ds_course_4_ep/cam_ds_ep_FinSight\n",
    " \n",
    "üìÅ Input directory: /Users/jerome.ahye/Documents/2. learn_data-science/cam_ds_course_4_ep/cam_ds_ep_FinSight/data/raw/hsbc\n",
    "\n",
    "üìÅ Output directory: /Users/jerome.ahye/Documents/2. learn_data-science/cam_ds_course_4_ep/cam_ds_ep_FinSight/data/processed/hsbc\n",
    "\n",
    "‚úÖ Found 10 PDF files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed6e30-19ce-43cb-992d-d1c598b1b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the corrected speaker extraction\n",
    "parser = HSBCEarningsParser()\n",
    "\n",
    "# Test HSBC executive (should have role=title, company=HSBC)\n",
    "test1 = \"NOEL QUINN, GROUP CHIEF EXECUTIVE: Thank you for joining us today.\"\n",
    "result1 = parser.extract_speaker_info(test1)\n",
    "print(\"HSBC Executive test:\", result1)\n",
    "\n",
    "# Test analyst (should have role=Analyst, company=firm)  \n",
    "test2 = \"MANUS COSTELLO, AUTONOMOUS: Hi, thanks for taking my question.\"\n",
    "result2 = parser.extract_speaker_info(test2)\n",
    "print(\"Analyst test:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d39c5-42ee-455d-9784-f03f039618e3",
   "metadata": {},
   "source": [
    "HSBC Executive test: {'speaker_name': 'Noel Quinn', 'role': 'Group Chief Executive', 'company': 'HSBC', 'content_start': 'Thank you for joining us today.'}\n",
    "\n",
    "Analyst test: {'speaker_name': 'Manus Costello', 'role': 'Analyst', 'company': 'Autonomous', 'content_start': 'Hi, thanks for taking my question.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906efe9-7a71-425f-b203-9f62d9e5fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all PDF files in the input directory\n",
    "if pdf_files:\n",
    "    print(\"üöÄ Starting processing...\\n\")\n",
    "    \n",
    "    # Run the parser\n",
    "    df_combined = parser.process_directory(input_dir, output_dir)\n",
    "    \n",
    "    if not df_combined.empty:\n",
    "        print(\"\\n‚úÖ Processing completed successfully!\")\n",
    "        print(f\"\\nüìä Summary Statistics:\")\n",
    "        print(f\"   Total records: {len(df_combined):,}\")\n",
    "        print(f\"   Data shape: {df_combined.shape}\")\n",
    "        print(f\"   Years covered: {sorted(df_combined['year'].unique())}\")\n",
    "        print(f\"   Quarters covered: {sorted(df_combined['quarter'].unique())}\")\n",
    "        print(f\"   Sections: {df_combined['section'].value_counts().to_dict()}\")\n",
    "        print(f\"   Speakers: {df_combined['speaker_name'].nunique()} unique speakers\")\n",
    "        print(f\"   Companies: {df_combined['company'].nunique()} unique companies\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No data was successfully processed. Please check the PDF files and try again.\")\n",
    "else:\n",
    "    print(\"‚ùå No PDF files to process. Please add files to the input directory first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e650c-f7aa-4dd9-9265-5b7af71a7e7d",
   "metadata": {},
   "source": [
    "üöÄ Starting processing...\n",
    "\n",
    "\n",
    "‚úÖ Processing completed successfully!\n",
    "\n",
    "üìä Summary Statistics:\n",
    "   Total records: 376\n",
    "   \n",
    "   Data shape: (376, 11)\n",
    "  \n",
    "   Years covered: [np.int64(2023), np.int64(2024), np.int64(2025)]\n",
    "   \n",
    "   Quarters covered: ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "   \n",
    "   Sections: {'qa': 354, 'presentation': 22}\n",
    "   \n",
    "   Speakers: 41 unique speakers\n",
    "   \n",
    "   Companies: 23 unique companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35055201-3cdc-4476-b214-0fa7320dce2a",
   "metadata": {},
   "source": [
    "# Parser python files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17fa60-ef40-40ea-ba04-9d59aff8f662",
   "metadata": {},
   "source": [
    "## JPMorgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4287a-471a-4f80-9dd4-b29681cd2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "JPM (and similar) earnings call PDF ‚Üí tidy CSV parser.\n",
    "\n",
    "Includes:\n",
    "- Robust text extraction (PyMuPDF ‚Üí pdfminer.six ‚Üí PyPDF2 ‚Üí pdftotext, with optional OCR)\n",
    "- Q&A parsing with speaker/role/company attribution\n",
    "- Presentation extraction before the Q&A:\n",
    "    --include-presentation {none|single_row|per_speaker}  (default: single_row)\n",
    "    * Recognizes headings: \"PRESENTATION\", \"PREPARED REMARKS\", and\n",
    "      JPM's \"MANAGEMENT DISCUSSION SECTION\"\n",
    "- Pleasantry labeling + resequencing so pleasantries never create new Qs:\n",
    "    --pleasantry-mode {keep_raw|label_only|label_and_resequence}  (default: label_and_resequence)\n",
    "\n",
    "Outputs columns:\n",
    "    section, question_number, answer_number, speaker_name, role, company, content, year, quarter,\n",
    "    is_pleasantry, is_intro  (the last two depend on pleasantry-mode)\n",
    "\n",
    "Batch mode:\n",
    "    Write one CSV per PDF (and optional combined CSV with a source_pdf column).\n",
    "\n",
    "Install:\n",
    "    pip install pymupdf pdfminer.six PyPDF2 pandas pillow pytesseract\n",
    "    # If using --ocr, also install the Tesseract binary (brew/apt/choco).\n",
    "\n",
    "Examples:\n",
    "    # Single file (keep presentation + label/resequence pleasantries)\n",
    "    python jpm_earnings_qa_parser.py ./jpm-2q25-earnings-call-transcript.pdf\n",
    "\n",
    "    # Batch a folder recursively with combined CSV\n",
    "    python jpm_earnings_qa_parser.py ./transcripts \\\n",
    "      --batch --recursive --outdir ./qa_csvs --combined-out ./all_qna.csv \\\n",
    "      --prefer pymupdf --ocr --max-ocr-pages 6 \\\n",
    "      --include-presentation single_row \\\n",
    "      --pleasantry-mode label_and_resequence\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Canonical labels for prepared remarks (presentation) ---\n",
    "PRESENTATION_SPEAKER_NAME = \"Jeremy Barnum\"\n",
    "PRESENTATION_ROLE = \"Chief Financial Officer\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Text extraction\n",
    "# ------------------------------------------------------------\n",
    "def _extract_with_pymupdf(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            t = page.get_text(\"text\") or page.get_text(\"block\") or \"\"\n",
    "            parts.append(t)\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_with_pdfminer(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "        return pdfminer_extract_text(str(pdf_path)) or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_with_pypdf2(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        import PyPDF2\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            parts = []\n",
    "            for p in reader.pages:\n",
    "                try:\n",
    "                    parts.append(p.extract_text() or \"\")\n",
    "                except Exception:\n",
    "                    parts.append(\"\")\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _extract_with_pdftotext(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"pdftotext\", \"-layout\", str(pdf_path), \"-\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=False,\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.decode(\"utf-8\", errors=\"ignore\")\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _ocr_with_tesseract(pdf_path: Path, max_pages: int = 6, lang: str = \"eng\") -> str:\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        from PIL import Image\n",
    "        import pytesseract\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    texts = []\n",
    "    pages_to_ocr = min(len(doc), max_pages)\n",
    "    for i in range(pages_to_ocr):\n",
    "        page = doc[i]\n",
    "        pix = page.get_pixmap(dpi=200)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        try:\n",
    "            t = pytesseract.image_to_string(img, lang=lang)\n",
    "        except Exception:\n",
    "            t = \"\"\n",
    "        texts.append(t)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(\n",
    "    pdf_path: Path,\n",
    "    prefer: Optional[str] = None,\n",
    "    ocr: bool = False,\n",
    "    ocr_lang: str = \"eng\",\n",
    "    max_ocr_pages: int = 6,\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (text, method_used). prefer ‚àà {'pymupdf','pdfminer','pypdf2','pdftotext'}.\n",
    "    \"\"\"\n",
    "    methods = [\n",
    "        (\"pymupdf\", _extract_with_pymupdf),\n",
    "        (\"pdfminer\", _extract_with_pdfminer),\n",
    "        (\"pypdf2\", _extract_with_pypdf2),\n",
    "        (\"pdftotext\", _extract_with_pdftotext),\n",
    "    ]\n",
    "    if prefer:\n",
    "        methods.sort(key=lambda x: 0 if x[0] == (prefer or \"\").lower() else 1)\n",
    "\n",
    "    for name, fn in methods:\n",
    "        text = fn(pdf_path)\n",
    "        if text and text.strip():\n",
    "            return text, name\n",
    "\n",
    "    if ocr:\n",
    "        text = _ocr_with_tesseract(pdf_path, max_pages=max_ocr_pages, lang=ocr_lang)\n",
    "        if text and text.strip():\n",
    "            return text, f\"ocr({ocr_lang})\"\n",
    "\n",
    "    return \"\", \"none\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Common helpers\n",
    "# ------------------------------------------------------------\n",
    "def clean_lines(raw_text: str) -> List[str]:\n",
    "    text = raw_text.replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[.\\u2026]{10,}\", \"\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\")]\n",
    "    cleaned: List[str] = []\n",
    "    for ln in lines:\n",
    "        if not ln:\n",
    "            cleaned.append(\"\")\n",
    "            continue\n",
    "        if re.fullmatch(r\"\\d{1,3}\", ln):\n",
    "            cleaned.append(\"\")  # drop bare page numbers\n",
    "            continue\n",
    "        cleaned.append(ln)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def slice_to_qa_section(lines: List[str]) -> List[str]:\n",
    "    start_idx = None\n",
    "    for i, ln in enumerate(lines):\n",
    "        if ln.strip().upper() == \"QUESTION AND ANSWER SECTION\":\n",
    "            start_idx = i\n",
    "            break\n",
    "    return lines[start_idx:] if start_idx is not None else lines\n",
    "\n",
    "\n",
    "def is_label_line_eol(ln: str) -> bool:\n",
    "    return bool(re.search(r\"\\s[QA]$\", ln.strip()))\n",
    "\n",
    "\n",
    "def is_label_line_solo(ln: str) -> bool:\n",
    "    return ln.strip() in {\"Q\", \"A\"}\n",
    "\n",
    "\n",
    "def is_any_label_line(ln: str) -> bool:\n",
    "    return is_label_line_solo(ln) or is_label_line_eol(ln)\n",
    "\n",
    "\n",
    "def looks_like_name(s: str) -> bool:\n",
    "    s = s.strip()\n",
    "    if not s or \":\" in s or len(s.split()) < 2 or len(s.split()) > 6:\n",
    "        return False\n",
    "    return all(tok and tok[0].isupper() for tok in s.split())\n",
    "\n",
    "\n",
    "def prev_nonempty(lines: List[str], idx: int) -> Tuple[Optional[int], Optional[str]]:\n",
    "    j = idx\n",
    "    while j >= 0:\n",
    "        s = lines[j].strip()\n",
    "        if s:\n",
    "            return j, s\n",
    "        j -= 1\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def next_nonempty(lines: List[str], start: int) -> Optional[int]:\n",
    "    j = start\n",
    "    while j < len(lines):\n",
    "        if lines[j].strip():\n",
    "            return j\n",
    "        j += 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_company_and_role(rc_line: str, qa_flag: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    core = rc_line.strip()\n",
    "    if qa_flag == \"Q\":\n",
    "        role = \"analyst\"\n",
    "        company = None\n",
    "        if \",\" in core:\n",
    "            parts = core.split(\",\", 1)\n",
    "            company = parts[1].strip() if len(parts) > 1 else None\n",
    "        return company, role\n",
    "    else:\n",
    "        company = None\n",
    "        role = core\n",
    "        if \",\" in core:\n",
    "            role_part, company_part = core.split(\",\", 1)\n",
    "            role = role_part.strip()\n",
    "            company = company_part.strip()\n",
    "        return company, role\n",
    "\n",
    "\n",
    "def header_triple_detected(lines: List[str], j: int) -> bool:\n",
    "    j_name = next_nonempty(lines, j)\n",
    "    if j_name is None:\n",
    "        return False\n",
    "    j_rc = next_nonempty(lines, j_name + 1)\n",
    "    if j_rc is None:\n",
    "        return False\n",
    "    j_flag = next_nonempty(lines, j_rc + 1)\n",
    "    if j_flag is None:\n",
    "        return False\n",
    "    name_ok = looks_like_name(lines[j_name].strip())\n",
    "    rc_ok = (\",\" in lines[j_rc])\n",
    "    flag_ok = is_label_line_solo(lines[j_flag]) or is_label_line_eol(lines[j_flag])\n",
    "    return bool(name_ok and rc_ok and flag_ok)\n",
    "\n",
    "def postprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize output:\n",
    "      - Force speaker_name + role for presentation section\n",
    "      - Remove deprecated columns: is_intro, speaker, speaker_title\n",
    "      - Light normalization of key columns\n",
    "    \"\"\"\n",
    "    # Normalize section\n",
    "    if \"section\" in df.columns:\n",
    "        df[\"section\"] = df[\"section\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Ensure the canonical columns exist\n",
    "    if \"speaker_name\" not in df.columns and \"speaker\" in df.columns:\n",
    "        df = df.rename(columns={\"speaker\": \"speaker_name\"})\n",
    "    if \"role\" not in df.columns and \"speaker_title\" in df.columns:\n",
    "        df = df.rename(columns={\"speaker_title\": \"role\"})\n",
    "\n",
    "    # Clean strings\n",
    "    if \"speaker_name\" in df.columns:\n",
    "        df[\"speaker_name\"] = df[\"speaker_name\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    if \"role\" in df.columns:\n",
    "        df[\"role\"] = df[\"role\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Canonicalize prepared remarks\n",
    "    if \"section\" in df.columns:\n",
    "        pres_mask = df[\"section\"].eq(\"presentation\")\n",
    "        if pres_mask.any():\n",
    "            df.loc[pres_mask, \"speaker_name\"] = PRESENTATION_SPEAKER_NAME\n",
    "            df.loc[pres_mask, \"role\"] = PRESENTATION_ROLE\n",
    "\n",
    "    # Drop unused columns if they still exist\n",
    "    drop_cols = [c for c in [\"is_intro\", \"speaker\", \"speaker_title\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Presentation extraction (before Q&A)\n",
    "# ------------------------------------------------------------\n",
    "PRESENTATION_HEADINGS = {\n",
    "    \"PRESENTATION\",\n",
    "    \"PREPARED REMARKS\",\n",
    "    \"PREPARED REMARKS:\",\n",
    "    \"MANAGEMENT DISCUSSION SECTION\",\n",
    "    \"MANAGEMENT DISCUSSION SECTION:\",\n",
    "}\n",
    "\n",
    "def _find_qa_start_idx(lines: List[str]) -> Optional[int]:\n",
    "    for i, ln in enumerate(lines):\n",
    "        if ln.strip().upper() == \"QUESTION AND ANSWER SECTION\":\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_presentation_start_idx(lines: List[str]) -> int:\n",
    "    # Prefer explicit headings (now includes JPM's \"MANAGEMENT DISCUSSION SECTION\")\n",
    "    for i, ln in enumerate(lines[:300]):\n",
    "        if ln.strip().upper() in PRESENTATION_HEADINGS:\n",
    "            return i + 1\n",
    "    # Otherwise: first Name + \"Role, Company\" header pair\n",
    "    n = len(lines)\n",
    "    for i in range(min(300, n - 2)):\n",
    "        name = lines[i].strip()\n",
    "        role = lines[i + 1].strip()\n",
    "        if looks_like_name(name) and (\",\" in role):\n",
    "            return i + 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _parse_presentation_blocks(lines_before_qa: List[str], default_company=\"JPMorganChase\") -> List[Dict[str, str]]:\n",
    "    SKIP = {\n",
    "        \"COMPANY PARTICIPANTS\",\n",
    "        \"CONFERENCE CALL PARTICIPANTS\",\n",
    "        \"PRESENTATION\",\n",
    "        \"PREPARED REMARKS\",\n",
    "        \"PREPARED REMARKS:\",\n",
    "        \"MANAGEMENT DISCUSSION SECTION\",\n",
    "        \"MANAGEMENT DISCUSSION SECTION:\",\n",
    "    }\n",
    "    clean = [ln for ln in lines_before_qa if ln and ln.strip().upper() not in SKIP and not ln.startswith(\"Operator:\")]\n",
    "\n",
    "    blocks: List[Dict[str, str]] = []\n",
    "    i, n = 0, len(clean)\n",
    "    while i < n - 2:\n",
    "        name = clean[i].strip()\n",
    "        role_line = clean[i + 1].strip()\n",
    "        if looks_like_name(name) and (\",\" in role_line):\n",
    "            # role/company split\n",
    "            role, company = role_line.split(\",\", 1)\n",
    "            role, company = role.strip(), (company.strip() or default_company)\n",
    "            j = i + 2\n",
    "            content_lines: List[str] = []\n",
    "            # collect until next header (name+role) or end\n",
    "            while j < n - 1:\n",
    "                if looks_like_name(clean[j].strip()) and (\",\" in clean[j + 1].strip()):\n",
    "                    break\n",
    "                content_lines.append(clean[j])\n",
    "                j += 1\n",
    "            content = \" \".join(x.strip() for x in content_lines if x.strip())\n",
    "            if content:\n",
    "                blocks.append({\"speaker_name\": name, \"role\": role, \"company\": company, \"content\": content})\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if not blocks:\n",
    "        body = \" \".join(clean).strip()\n",
    "        if body:\n",
    "            blocks = [{\n",
    "                \"speaker_name\": \"Management\",\n",
    "                \"role\": \"prepared_remarks\",\n",
    "                \"company\": default_company,\n",
    "                \"content\": body\n",
    "            }]\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def extract_presentation_blocks_from_text(all_lines: List[str], default_company=\"JPMorganChase\") -> List[Dict[str, str]]:\n",
    "    qa_idx = _find_qa_start_idx(all_lines)\n",
    "    before_qa = all_lines[:qa_idx] if qa_idx is not None else all_lines\n",
    "    start_idx = _find_presentation_start_idx(before_qa)\n",
    "    pres_slice = before_qa[start_idx:]\n",
    "    return _parse_presentation_blocks(pres_slice, default_company=default_company)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Pleasantry labeling + resequencing\n",
    "# ------------------------------------------------------------\n",
    "QUESTION_CUES = re.compile(\n",
    "    r\"\\?|(^|\\b)(what|how|why|when|where|which|could you|can you|would you|\"\n",
    "    r\"give us|talk about|walk us|update on|help us|color on|outlook|guidance|\"\n",
    "    r\"drivers?|puts and takes|bridge|framework)\\b\", re.I\n",
    ")\n",
    "\n",
    "PLEASANTRY_START = re.compile(\n",
    "    r\"^\\s*(thanks( very much)?|thank you|appreciate|welcome|\"\n",
    "    r\"(good\\s+)?(morning|afternoon|evening)|hi|hello|hey|\"\n",
    "    r\"congrats|congratulations|great\\.?|okay\\.?|ok\\.?|sure\\.?|yeah\\.?|right\\.)\\b\", re.I\n",
    ")\n",
    "\n",
    "def _looks_substantive_question(text: str) -> bool:\n",
    "    return isinstance(text, str) and bool(QUESTION_CUES.search(text))\n",
    "\n",
    "\n",
    "def _looks_pleasantry(text: str, max_words: int = 18) -> bool:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    t = text.strip()\n",
    "    if \"?\" in t:\n",
    "        return False\n",
    "    if not PLEASANTRY_START.search(t):\n",
    "        return False\n",
    "    word_count = len(re.findall(r\"\\w+\", t))\n",
    "    return word_count <= max_words\n",
    "\n",
    "\n",
    "def label_pleasantries(df: pd.DataFrame, max_words: int = 18) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"is_pleasantry\"] = df[\"content\"].apply(lambda s: _looks_pleasantry(s, max_words))\n",
    "    return df\n",
    "\n",
    "\n",
    "def resequence_qa_ignoring_pleasantries(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recompute question_number / answer_number for section=='qa' rows while ignoring pleasantries:\n",
    "      - pleasantries never start a new question\n",
    "      - analyst pleasantries after answers are tagged with the current question but don't bump counters\n",
    "      - internal pleasantries inherit current a_num without bumping\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    mask_qa = (df[\"section\"] == \"qa\")\n",
    "    q = a = 0\n",
    "    # reset\n",
    "    df.loc[mask_qa, \"question_number\"] = pd.NA\n",
    "    df.loc[mask_qa, \"answer_number\"] = pd.NA\n",
    "\n",
    "    for i, r in df[mask_qa].iterrows():\n",
    "        role = str(r[\"role\"]).lower()\n",
    "        text = r.get(\"content\", \"\")\n",
    "        is_p = bool(r.get(\"is_pleasantry\"))\n",
    "        if role == \"analyst\":\n",
    "            if _looks_substantive_question(text) and not is_p:\n",
    "                q += 1; a = 0\n",
    "                df.at[i, \"question_number\"] = q\n",
    "            else:\n",
    "                if q > 0:\n",
    "                    df.at[i, \"question_number\"] = q\n",
    "                    if a > 0:\n",
    "                        df.at[i, \"answer_number\"] = a\n",
    "        else:\n",
    "            if q > 0:\n",
    "                if not is_p:\n",
    "                    a += 1\n",
    "                df.at[i, \"question_number\"] = q\n",
    "                if a > 0:\n",
    "                    df.at[i, \"answer_number\"] = a\n",
    "    # Cast to nullable ints\n",
    "    df[\"question_number\"] = df[\"question_number\"].astype(\"Int64\")\n",
    "    df[\"answer_number\"] = df[\"answer_number\"].astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Q&A core parsing\n",
    "# ------------------------------------------------------------\n",
    "def collect_blocks(lines: List[str]) -> List[Dict[str, Any]]:\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "    while i < n:\n",
    "        ln = lines[i].strip()\n",
    "\n",
    "        # Case 1: 'Q' or 'A' on its own line\n",
    "        if is_label_line_solo(ln):\n",
    "            qa_flag = ln\n",
    "            rc_idx, rc_line = prev_nonempty(lines, i - 1)\n",
    "            name_idx, name_line = prev_nonempty(lines, (rc_idx - 1) if rc_idx is not None else -1)\n",
    "            if not rc_line or not name_line or name_line.startswith(\"Operator:\"):\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            company, role = parse_company_and_role(rc_line, qa_flag)\n",
    "\n",
    "            content_lines: List[str] = []\n",
    "            j = i + 1\n",
    "            while j < n:\n",
    "                if is_any_label_line(lines[j]):\n",
    "                    break\n",
    "                if header_triple_detected(lines, j):\n",
    "                    break\n",
    "                if lines[j].strip() in {name_line.strip(), rc_line.strip()}:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                content_lines.append(lines[j])\n",
    "                j += 1\n",
    "\n",
    "            content = \" \".join(x.strip() for x in content_lines if x.strip())\n",
    "            content = re.sub(r\"\\s{2,}\", \" \", content).strip()\n",
    "\n",
    "            items.append(\n",
    "                {\n",
    "                    \"type\": qa_flag,\n",
    "                    \"speaker_name\": name_line.strip(),\n",
    "                    \"role\": \"analyst\" if qa_flag == \"Q\" else (role or \"\"),\n",
    "                    \"company\": (company or (\"JPMorganChase\" if qa_flag == \"A\" else None)),\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            )\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # Case 2: ' Q' or ' A' at end of line\n",
    "        if is_label_line_eol(ln):\n",
    "            qa_flag = ln[-1]\n",
    "            name_idx, name_line = prev_nonempty(lines, i - 1)\n",
    "            rc_line = ln.rsplit(\" \", 1)[0]\n",
    "            if not name_line or name_line.startswith(\"Operator:\"):\n",
    "                i += 1\n",
    "                continue\n",
    "            company, role = parse_company_and_role(rc_line, qa_flag)\n",
    "\n",
    "            content_lines: List[str] = []\n",
    "            j = i + 1\n",
    "            while j < n and not is_any_label_line(lines[j]) and not header_triple_detected(lines, j):\n",
    "                if lines[j].strip() in {name_line.strip(), rc_line.strip()}:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                content_lines.append(lines[j])\n",
    "                j += 1\n",
    "            content = \" \".join(x.strip() for x in content_lines if x.strip())\n",
    "            content = re.sub(r\"\\s{2,}\", \" \", content).strip()\n",
    "\n",
    "            items.append(\n",
    "                {\n",
    "                    \"type\": qa_flag,\n",
    "                    \"speaker_name\": name_line.strip(),\n",
    "                    \"role\": \"analyst\" if qa_flag == \"Q\" else (role or \"\"),\n",
    "                    \"company\": (company or (\"JPMorganChase\" if qa_flag == \"A\" else None)),\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            )\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def merge_consecutive_questions(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    merged: List[Dict[str, Any]] = []\n",
    "    for it in items:\n",
    "        if it[\"type\"] == \"Q\" and merged and merged[-1][\"type\"] == \"Q\":\n",
    "            if merged[-1][\"speaker_name\"] == it[\"speaker_name\"]:\n",
    "                merged[-1][\"content\"] = (merged[-1][\"content\"] + \" \" + it[\"content\"]).strip()\n",
    "            else:\n",
    "                merged.append(it)\n",
    "        else:\n",
    "            merged.append(it)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def to_dataframe(items: List[Dict[str, Any]], year: int, quarter: str) -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    q_counter = 0\n",
    "    a_counter = 0\n",
    "    for it in items:\n",
    "        if it[\"type\"] == \"Q\":\n",
    "            q_counter += 1\n",
    "            a_counter = 0\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"section\": \"qa\",\n",
    "                    \"question_number\": q_counter,\n",
    "                    \"answer_number\": pd.NA,\n",
    "                    \"speaker_name\": it.get(\"speaker_name\", \"\"),\n",
    "                    \"role\": \"analyst\",\n",
    "                    \"company\": it.get(\"company\"),\n",
    "                    \"content\": it.get(\"content\", \"\"),\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            a_counter += 1\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"section\": \"qa\",\n",
    "                    \"question_number\": q_counter if q_counter > 0 else pd.NA,\n",
    "                    \"answer_number\": a_counter,\n",
    "                    \"speaker_name\": it.get(\"speaker_name\", \"\"),\n",
    "                    \"role\": it.get(\"role\", \"\"),\n",
    "                    \"company\": it.get(\"company\") or \"JPMorganChase\",\n",
    "                    \"content\": it.get(\"content\", \"\"),\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter,\n",
    "                }\n",
    "            )\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"question_number\"] = df[\"question_number\"].astype(\"Int64\")\n",
    "    df[\"answer_number\"] = df[\"answer_number\"].astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def infer_year_quarter_from_filename(path: Path) -> Tuple[Optional[int], Optional[str]]:\n",
    "    name = path.name.lower()\n",
    "    m = re.search(r'(\\d)q(\\d{2,4})', name)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    qnum = int(m.group(1))\n",
    "    yy = m.group(2)\n",
    "    year = 2000 + int(yy) if len(yy) == 2 else int(yy)\n",
    "    quarter = f\"Q{qnum}\"\n",
    "    return year, quarter\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Public API\n",
    "# ------------------------------------------------------------\n",
    "def parse_pdf_to_df(\n",
    "    pdf_path: Path,\n",
    "    year: Optional[int] = None,\n",
    "    quarter: Optional[str] = None,\n",
    "    prefer: Optional[str] = None,\n",
    "    ocr: bool = False,\n",
    "    ocr_lang: str = \"eng\",\n",
    "    max_ocr_pages: int = 6,\n",
    "    include_presentation: str = \"single_row\",   # none|single_row|per_speaker\n",
    "    pleasantry_mode: str = \"label_and_resequence\",  # keep_raw|label_only|label_and_resequence\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    # 1) Extract text once\n",
    "    text, method = extract_text_from_pdf(pdf_path, prefer=prefer, ocr=ocr, ocr_lang=ocr_lang, max_ocr_pages=max_ocr_pages)\n",
    "    if verbose:\n",
    "        print(f\"Extraction method for {pdf_path.name}: {method}\")\n",
    "    if not text or not text.strip():\n",
    "        raise RuntimeError(f\"Unable to extract text from PDF: {pdf_path}\")\n",
    "\n",
    "    # 2) Preprocess lines\n",
    "    lines = clean_lines(text)\n",
    "\n",
    "    # 3) Q&A parse\n",
    "    qa_lines = slice_to_qa_section(lines)\n",
    "    items = collect_blocks(qa_lines)\n",
    "    items = merge_consecutive_questions(items)\n",
    "\n",
    "    inf_year, inf_quarter = infer_year_quarter_from_filename(pdf_path)\n",
    "    year = year if year is not None else (inf_year if inf_year is not None else pd.NA)\n",
    "    quarter = quarter if quarter is not None else (inf_quarter if inf_quarter is not None else pd.NA)\n",
    "\n",
    "    df_qa = to_dataframe(items, year=year, quarter=quarter)\n",
    "\n",
    "    # 4) Presentation extraction (optional)\n",
    "    pres_rows: List[Dict[str, Any]] = []\n",
    "    if include_presentation and include_presentation.lower() != \"none\":\n",
    "        blocks = extract_presentation_blocks_from_text(lines, default_company=\"JPMorganChase\")\n",
    "        if include_presentation.lower() == \"per_speaker\" and len(blocks) > 1:\n",
    "            for b in blocks:\n",
    "                if not b.get(\"content\"):\n",
    "                    continue\n",
    "                pres_rows.append({\n",
    "                    \"section\": \"presentation\",\n",
    "                    \"question_number\": pd.NA,\n",
    "                    \"answer_number\": pd.NA,\n",
    "                    \"speaker_name\": b[\"speaker_name\"],\n",
    "                    \"role\": b.get(\"role\") or \"prepared_remarks\",\n",
    "                    \"company\": b.get(\"company\") or \"JPMorganChase\",\n",
    "                    \"content\": b[\"content\"],\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter,\n",
    "                })\n",
    "        else:\n",
    "            # single_row: collapse all content\n",
    "            combined = \" \".join([b[\"content\"] for b in blocks if b.get(\"content\")]).strip()\n",
    "            spk = \", \".join([b[\"speaker_name\"] for b in blocks]) if blocks else \"Management\"\n",
    "            pres_rows.append({\n",
    "                \"section\": \"presentation\",\n",
    "                \"question_number\": pd.NA,\n",
    "                \"answer_number\": pd.NA,\n",
    "                \"speaker_name\": spk or \"Management\",\n",
    "                \"role\": \"prepared_remarks\",\n",
    "                \"company\": \"JPMorganChase\",\n",
    "                \"content\": combined,\n",
    "                \"year\": year,\n",
    "                \"quarter\": quarter,\n",
    "            })\n",
    "\n",
    "    if pres_rows:\n",
    "        pres_df = pd.DataFrame(pres_rows)\n",
    "    \n",
    "        # Ensure the same columns exist as in df_qa\n",
    "        for col in df_qa.columns:\n",
    "            if col not in pres_df.columns:\n",
    "                pres_df[col] = pd.NA\n",
    "    \n",
    "        # Enforce dtypes for integer-ish columns to avoid dtype inference surprises\n",
    "        for col in [\"question_number\", \"answer_number\", \"year\"]:\n",
    "            if col in pres_df.columns and str(df_qa[col].dtype) == \"Int64\":\n",
    "                pres_df[col] = pres_df[col].astype(\"Int64\")\n",
    "    \n",
    "        # Match column order, then concat\n",
    "        pres_df = pres_df[df_qa.columns]\n",
    "        df = pd.concat([pres_df, df_qa], ignore_index=True)\n",
    "    else:\n",
    "        df = df_qa\n",
    "\n",
    "\n",
    "    # 5) Pleasantry handling\n",
    "    mode = (pleasantry_mode or \"keep_raw\").lower()\n",
    "    if mode in {\"label_only\", \"label_and_resequence\"}:\n",
    "        df = label_pleasantries(df, max_words=18)\n",
    "        # Mark intro pleasantries (those before first QA question starts)\n",
    "        first_q_idx = df.index[(df[\"section\"] == \"qa\") & df[\"question_number\"].notna()].min() \\\n",
    "                      if ((df[\"section\"] == \"qa\") & df[\"question_number\"].notna()).any() else None\n",
    "        df[\"is_intro\"] = False\n",
    "        if first_q_idx is not None:\n",
    "            intro_mask = df.index < first_q_idx\n",
    "            df.loc[intro_mask & (df[\"section\"] == \"qa\") & df[\"is_pleasantry\"].fillna(False), \"is_intro\"] = True\n",
    "\n",
    "        if mode == \"label_and_resequence\":\n",
    "            df = resequence_qa_ignoring_pleasantries(df)\n",
    "    else:\n",
    "        # ensure columns exist for schema consistency\n",
    "        if \"is_pleasantry\" not in df.columns:\n",
    "            df[\"is_pleasantry\"] = False\n",
    "        if \"is_intro\" not in df.columns:\n",
    "            df[\"is_intro\"] = False\n",
    "\n",
    "    # Final dtypes\n",
    "    if \"question_number\" in df.columns:\n",
    "        df[\"question_number\"] = df[\"question_number\"].astype(\"Int64\")\n",
    "    if \"answer_number\" in df.columns:\n",
    "        df[\"answer_number\"] = df[\"answer_number\"].astype(\"Int64\")\n",
    "\n",
    "    # Final tidy-up\n",
    "    df = postprocess_df(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Batch mode utilities\n",
    "# ------------------------------------------------------------\n",
    "def iter_pdfs(folder: Path, recursive: bool, pattern: Optional[str]) -> List[Path]:\n",
    "    if recursive:\n",
    "        files = list(folder.rglob(\"*.pdf\"))\n",
    "    else:\n",
    "        files = list(folder.glob(\"*.pdf\"))\n",
    "    if pattern:\n",
    "        import fnmatch\n",
    "        files = [f for f in files if fnmatch.fnmatch(f.name.lower(), pattern.lower())]\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def batch_process(\n",
    "    input_path: Path,\n",
    "    outdir: Optional[Path],\n",
    "    recursive: bool,\n",
    "    pattern: Optional[str],\n",
    "    combined_out: Optional[Path],\n",
    "    prefer: Optional[str] = None,\n",
    "    ocr: bool = False,\n",
    "    ocr_lang: str = \"eng\",\n",
    "    max_ocr_pages: int = 6,\n",
    "    include_presentation: str = \"single_row\",   # none|single_row|per_speaker\n",
    "    pleasantry_mode: str = \"label_and_resequence\",  # keep_raw|label_only|label_and_resequence\n",
    ") -> Path:\n",
    "    if outdir:\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_rows: List[pd.DataFrame] = []\n",
    "    if input_path.is_file():\n",
    "        pdfs = [input_path]\n",
    "    elif input_path.is_dir():\n",
    "        pdfs = iter_pdfs(input_path, recursive=recursive, pattern=pattern)\n",
    "    else:\n",
    "        raise SystemExit(f\"Path not found: {input_path}\")\n",
    "\n",
    "    if not pdfs:\n",
    "        raise SystemExit(\"No PDF files found to process.\")\n",
    "\n",
    "    for pdf in pdfs:\n",
    "        try:\n",
    "            print(f\"Parsing: {pdf}\")\n",
    "            df = parse_pdf_to_df(\n",
    "                pdf,\n",
    "                prefer=prefer,\n",
    "                ocr=ocr,\n",
    "                ocr_lang=ocr_lang,\n",
    "                max_ocr_pages=max_ocr_pages,\n",
    "                include_presentation=include_presentation,\n",
    "                pleasantry_mode=pleasantry_mode,\n",
    "                verbose=True,\n",
    "            )\n",
    "            if outdir:\n",
    "                out_path = outdir / f\"{pdf.stem}_qa.csv\"\n",
    "            else:\n",
    "                out_path = pdf.with_suffix(\"\").with_name(pdf.stem + \"_qa.csv\")\n",
    "            df.to_csv(out_path, index=False)\n",
    "            print(f\"  -> {len(df)} rows written to {out_path}\")\n",
    "            all_rows.append(df.assign(source_pdf=str(pdf)))\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Failed: {pdf} -> {e}\")\n",
    "            \n",
    "\n",
    "    if combined_out:\n",
    "        combined = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n",
    "        # Final tidy-up\n",
    "        combined = postprocess_df(combined)\n",
    "        combined.to_csv(combined_out, index=False)\n",
    "        print(f\"Wrote combined CSV ({len(combined)} rows) -> {combined_out}\")\n",
    "        return combined_out\n",
    "    return Path(\"\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CLI\n",
    "# ------------------------------------------------------------\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Parse JPM earnings-call PDFs (single file or batch folder) into CSV (+ presentation, pleasantries).\")\n",
    "    p.add_argument(\"input\", type=str, help=\"Path to a PDF or a folder of PDFs\")\n",
    "    p.add_argument(\"--out\", type=str, default=None, help=\"For single-file mode: CSV output path\")\n",
    "    p.add_argument(\"--year\", type=int, default=None, help=\"Override inferred year (e.g., 2025)\")\n",
    "    p.add_argument(\"--quarter\", type=str, default=None, help=\"Override inferred quarter (e.g., Q2)\")\n",
    "\n",
    "    # Extractor options\n",
    "    p.add_argument(\"--prefer\", type=str, default=None, choices=[\"pymupdf\", \"pdfminer\", \"pypdf2\", \"pdftotext\"], help=\"Preferred extractor to try first\")\n",
    "    p.add_argument(\"--ocr\", action=\"store_true\", help=\"Enable OCR fallback via pytesseract if text extraction fails\")\n",
    "    p.add_argument(\"--ocr-lang\", type=str, default=\"eng\", help=\"OCR language code (default 'eng')\")\n",
    "    p.add_argument(\"--max-ocr-pages\", type=int, default=6, help=\"Max pages to OCR for fallback (default 6)\")\n",
    "\n",
    "    # Presentation options\n",
    "    p.add_argument(\"--include-presentation\", type=str, default=\"single_row\", choices=[\"none\", \"single_row\", \"per_speaker\"],\n",
    "                   help=\"Attach opening statement before Q&A (default: single_row)\")\n",
    "\n",
    "    # Pleasantry options\n",
    "    p.add_argument(\"--pleasantry-mode\", type=str, default=\"label_and_resequence\",\n",
    "                   choices=[\"keep_raw\", \"label_only\", \"label_and_resequence\"],\n",
    "                   help=\"Pleasantry handling (default: label_and_resequence)\")\n",
    "\n",
    "    # Batch options\n",
    "    p.add_argument(\"--batch\", action=\"store_true\", help=\"Treat input as a folder and process multiple PDFs\")\n",
    "    p.add_argument(\"--recursive\", action=\"store_true\", help=\"Recurse into subfolders when batch processing\")\n",
    "    p.add_argument(\"--outdir\", type=str, default=None, help=\"Directory to write per-file CSVs in batch mode\")\n",
    "    p.add_argument(\"--pattern\", type=str, default=None, help=\"Filename pattern filter (e.g., 'jpm*earnings*pdf')\")\n",
    "    p.add_argument(\"--combined-out\", type=str, default=None, help=\"Path to write a single combined CSV across PDFs\")\n",
    "\n",
    "    args = p.parse_args()\n",
    "    in_path = Path(args.input)\n",
    "\n",
    "    if not args.batch and in_path.is_file():\n",
    "        df = parse_pdf_to_df(\n",
    "            in_path,\n",
    "            year=args.year,\n",
    "            quarter=args.quarter,\n",
    "            prefer=args.prefer,\n",
    "            ocr=args.ocr,\n",
    "            ocr_lang=args.ocr_lang,\n",
    "            max_ocr_pages=args.max_ocr_pages,\n",
    "            include_presentation=args.include_presentation,\n",
    "            pleasantry_mode=args.pleasantry_mode,\n",
    "            verbose=True,\n",
    "        )\n",
    "        out_path = Path(args.out) if args.out else in_path.with_suffix(\"\").with_name(in_path.stem + \"_qa.csv\")\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"Wrote {len(df)} rows -> {out_path}\")\n",
    "    else:\n",
    "        outdir = Path(args.outdir) if args.outdir else None\n",
    "        combined_out = Path(args.combined_out) if args.combined_out else None\n",
    "        batch_process(\n",
    "            in_path,\n",
    "            outdir=outdir,\n",
    "            recursive=args.recursive,\n",
    "            pattern=args.pattern,\n",
    "            combined_out=combined_out,\n",
    "            prefer=args.prefer,\n",
    "            ocr=args.ocr,\n",
    "            ocr_lang=args.ocr_lang,\n",
    "            max_ocr_pages=args.max_ocr_pages,\n",
    "            include_presentation=args.include_presentation,\n",
    "            pleasantry_mode=args.pleasantry_mode,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b2691-e40c-481c-a86f-09515f66e5f5",
   "metadata": {},
   "source": [
    "## HSBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b1ab4-018e-48da-9308-a8b319120570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import PyPDF2\n",
    "from datetime import datetime\n",
    "\n",
    "class HSBCEarningsParser:\n",
    "    \"\"\"Final HSBC earnings parser with corrected role/company logic and answer numbering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Known HSBC executives with their roles - used to identify internal speakers\n",
    "        self.hsbc_executives = {\n",
    "            'noel quinn': 'Group Chief Executive',\n",
    "            'georges elhedery': 'Group Chief Executive',  \n",
    "            'pam kaur': 'Group Chief Financial Officer',\n",
    "            'ewen stevenson': 'Group Chief Financial Officer', \n",
    "            'richard holmes': 'Group Head of Investor Relations',\n",
    "            'richard o\\'connor': 'Global Head of Investor Relations',\n",
    "            'stuart tait': 'Group Chief Risk and Compliance Officer',\n",
    "            'barry o\\'byrne': 'Chief Executive, Global Commercial Banking',\n",
    "            'surendra rosha': 'Chief Executive, Wealth and Personal Banking',\n",
    "            'greg guyett': 'Chief Executive, Global Banking and Markets',\n",
    "            'jose carvalho': 'Chief Executive, Global Banking',\n",
    "            'colin bell': 'Chief Executive, HSBC UK',\n",
    "            'david liao': 'Chief Executive, HSBC Asia Pacific',\n",
    "            'jon bingham': 'Interim Group Chief Financial Officer',\n",
    "            'mark tucker': 'Group Chairman'\n",
    "        }\n",
    "        \n",
    "        # Common analyst firm names for identification\n",
    "        self.analyst_firms = [\n",
    "            'Morgan Stanley', 'JP Morgan', 'JPMorgan', 'Goldman Sachs', 'Barclays', \n",
    "            'UBS', 'Credit Suisse', 'Deutsche Bank', 'Bank of America', 'Citigroup', \n",
    "            'Citi', 'BNP Paribas', 'Societe Generale', 'RBC', 'BMO', 'Nomura',\n",
    "            'Jefferies', 'Berenberg', 'Autonomous', 'KBW', 'Numis', 'Mediobanca',\n",
    "            'CICC', 'Redburn Atlantic', 'China Securities', 'RBC Capital Markets',\n",
    "            'Autonomous Research', 'Deutsche Numis', 'BNP Paribas Exane'\n",
    "        ]\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text content from PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def parse_filename_metadata(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract year, quarter information from filename\"\"\"\n",
    "        # Handle HSBC naming conventions\n",
    "        filename_lower = filename.lower()\n",
    "        \n",
    "        # After: Simple string matching\n",
    "        if '2023' in filename_lower:\n",
    "            year = 2023\n",
    "        elif '2024' in filename_lower:\n",
    "            year = 2024\n",
    "        elif '2025' in filename_lower:\n",
    "            year = 2025\n",
    "        else:\n",
    "            year = None\n",
    "        \n",
    "        # Map HSBC quarter conventions\n",
    "        if any(q in filename_lower for q in ['1q', 'q1', 'quarter1', 'first']):\n",
    "            quarter = 'Q1'\n",
    "        elif any(q in filename_lower for q in ['2q', 'q2', 'interim', 'half', 'h1']):\n",
    "            quarter = 'Q2'\n",
    "        elif any(q in filename_lower for q in ['3q', 'q3', 'quarter3', 'third']):\n",
    "            quarter = 'Q3'  \n",
    "        elif any(q in filename_lower for q in ['4q', 'q4', 'annual', 'full', 'year']):\n",
    "            quarter = 'Q4'\n",
    "        else:\n",
    "            quarter = 'Unknown'\n",
    "            \n",
    "        return {'year': year, 'quarter': quarter}\n",
    "\n",
    "    def identify_section_boundaries(self, text: str) -> Tuple[int, Optional[int]]:\n",
    "        \"\"\"Find where presentation ends and Q&A begins\"\"\"\n",
    "        presentation_start = 0\n",
    "        qa_start = None\n",
    "        \n",
    "        # Look for Q&A section indicators\n",
    "        qa_patterns = [\n",
    "            r'(?:Questions?\\s*(?:and|&)\\s*Answers?|Q\\s*&\\s*A)',\n",
    "            r'(?:Operator|Host).*(?:question|Q&A)',\n",
    "            r'(?:We\\s*will\\s*now\\s*(?:begin|start|take).*questions?)',\n",
    "            r'(?:(?:Thank\\s*you|Thanks).*(?:question|Q&A))'\n",
    "        ]\n",
    "        \n",
    "        for pattern in qa_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                qa_start = match.start()\n",
    "                break\n",
    "                \n",
    "        # Alternative: look for first analyst question\n",
    "        if qa_start is None:\n",
    "            analyst_pattern = r'(?:Thank you|Thanks|Good morning|Hi)'\n",
    "            match = re.search(analyst_pattern, text)\n",
    "            if match:\n",
    "                qa_start = match.start()\n",
    "        \n",
    "        return 0, qa_start\n",
    "\n",
    "    def extract_speaker_info(self, speaker_line: str) -> Dict[str, str]:\n",
    "        \"\"\"Enhanced speaker extraction with corrected role/company logic\"\"\"\n",
    "        # Clean the speaker line\n",
    "        speaker_line = re.sub(r'^\\s*[\\-\\*‚Ä¢]+\\s*', '', speaker_line.strip())\n",
    "        \n",
    "        # Try the standard pattern: \"NAME, TITLE/COMPANY:\" \n",
    "        match = re.match(r'^([A-Z][A-Z\\s\\'\\-\\.]+?),\\s*([^:]+?):\\s*(.+)', speaker_line, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            name = match.group(1).strip().title()\n",
    "            title_company_part = match.group(2).strip()\n",
    "            content_preview = match.group(3).strip()\n",
    "            \n",
    "            # Check if this is an HSBC executive by name\n",
    "            name_key = name.lower()\n",
    "            if name_key in self.hsbc_executives:\n",
    "                return {\n",
    "                    'speaker_name': name,\n",
    "                    'role': self.hsbc_executives[name_key],  # Use specific job title as role\n",
    "                    'company': 'HSBC',  # Company should always be HSBC for internal\n",
    "                    'content_start': content_preview\n",
    "                }\n",
    "            \n",
    "            # Check if it's an HSBC executive by title keywords in the title part\n",
    "            if any(keyword in title_company_part.lower() for keyword in \n",
    "                   ['group', 'chief', 'ceo', 'cfo', 'head', 'interim', 'chairman']):\n",
    "                return {\n",
    "                    'speaker_name': name,\n",
    "                    'role': title_company_part,  # Use the title as role\n",
    "                    'company': 'HSBC',  # Company is HSBC for internal speakers\n",
    "                    'content_start': content_preview\n",
    "                }\n",
    "            \n",
    "            # Otherwise, it's an external analyst\n",
    "            company = self.extract_company_name(title_company_part)\n",
    "            return {\n",
    "                'speaker_name': name,\n",
    "                'role': 'Analyst',\n",
    "                'company': company,\n",
    "                'content_start': content_preview\n",
    "            }\n",
    "        \n",
    "        # Try simple pattern for cases without comma: \"NAME:\"\n",
    "        simple_match = re.match(r'^([A-Z][A-Za-z\\s\\'\\-\\.]+?):\\s*(.+)', speaker_line, re.IGNORECASE)\n",
    "        if simple_match:\n",
    "            name = simple_match.group(1).strip().title()\n",
    "            content_preview = simple_match.group(2).strip()\n",
    "            \n",
    "            # Check if known HSBC executive\n",
    "            name_key = name.lower()\n",
    "            if name_key in self.hsbc_executives:\n",
    "                return {\n",
    "                    'speaker_name': name,\n",
    "                    'role': self.hsbc_executives[name_key],\n",
    "                    'company': 'HSBC',\n",
    "                    'content_start': content_preview\n",
    "                }\n",
    "        \n",
    "        # Fallback - couldn't parse properly\n",
    "        return {\n",
    "            'speaker_name': '',\n",
    "            'role': '',\n",
    "            'company': '',\n",
    "            'content_start': speaker_line\n",
    "        }\n",
    "\n",
    "    def extract_company_name(self, company_part: str) -> str:\n",
    "        \"\"\"Extract clean company name from company part of speaker line\"\"\"\n",
    "        # Look for known analyst firms in the company part\n",
    "        for firm in self.analyst_firms:\n",
    "            if firm.lower() in company_part.lower():\n",
    "                return firm\n",
    "        \n",
    "        # If no known firm found, clean up the company part\n",
    "        # Remove common title words and return what's left\n",
    "        company_clean = re.sub(r'\\b(?:analyst|research|equity|managing|director|senior|vice|president)\\b', \n",
    "                              '', company_part, flags=re.IGNORECASE)\n",
    "        company_clean = re.sub(r'\\s+', ' ', company_clean.strip())\n",
    "        \n",
    "        return company_clean if company_clean else company_part\n",
    "\n",
    "    def simple_parse_transcript_content(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parse transcript content with corrected logic\"\"\"\n",
    "        records = []\n",
    "        \n",
    "        # Identify section boundaries  \n",
    "        presentation_start, qa_start = self.identify_section_boundaries(text)\n",
    "        \n",
    "        if qa_start is None:\n",
    "            sections = [('presentation', text)]\n",
    "        else:\n",
    "            sections = [\n",
    "                ('presentation', text[presentation_start:qa_start]),\n",
    "                ('qa', text[qa_start:])\n",
    "            ]\n",
    "        \n",
    "        for section_type, section_text in sections:\n",
    "            lines = section_text.split('\\n')\n",
    "            \n",
    "            current_speaker_info = None\n",
    "            current_content = []\n",
    "            question_number = 0\n",
    "            answer_number_for_current_question = 0\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Check if this line starts a new speaker\n",
    "                if re.match(r'^[A-Z][A-Za-z\\s\\',\\-\\.&]+:', line):\n",
    "                    # Save previous speaker's content if any\n",
    "                    if current_speaker_info and current_content:\n",
    "                        self._save_speaker_record(\n",
    "                            records, current_speaker_info, current_content, \n",
    "                            section_type, question_number, answer_number_for_current_question, metadata\n",
    "                        )\n",
    "                    \n",
    "                    # Start new speaker\n",
    "                    current_speaker_info = self.extract_speaker_info(line)\n",
    "                    current_content = []\n",
    "                    \n",
    "                    # Update counters based on speaker type\n",
    "                    if section_type == 'qa':\n",
    "                        if current_speaker_info['role'] == 'Analyst':\n",
    "                            # New question - increment question number and reset answer number\n",
    "                            question_number += 1\n",
    "                            answer_number_for_current_question = 0\n",
    "                        elif current_speaker_info['company'] == 'HSBC':\n",
    "                            # New answer to the current question\n",
    "                            answer_number_for_current_question += 1\n",
    "                    \n",
    "                    # Add any content that was on the same line as speaker name\n",
    "                    if current_speaker_info.get('content_start'):\n",
    "                        current_content.append(current_speaker_info['content_start'])\n",
    "                        \n",
    "                else:\n",
    "                    # Continue accumulating content for current speaker\n",
    "                    if current_speaker_info:\n",
    "                        current_content.append(line)\n",
    "            \n",
    "            # Don't forget the last speaker\n",
    "            if current_speaker_info and current_content:\n",
    "                self._save_speaker_record(\n",
    "                    records, current_speaker_info, current_content,\n",
    "                    section_type, question_number, answer_number_for_current_question, metadata\n",
    "                )\n",
    "        \n",
    "        return records\n",
    "\n",
    "    def _save_speaker_record(self, records: List, speaker_info: Dict, content: List, \n",
    "                           section_type: str, question_num: int, answer_num: int, metadata: Dict):\n",
    "        \"\"\"Helper to save a speaker's record with corrected numbering and role logic\"\"\"\n",
    "        content_text = ' '.join(content).strip()\n",
    "        \n",
    "        # Skip very short content\n",
    "        if len(content_text) < 10:\n",
    "            return\n",
    "        \n",
    "        # Determine question/answer numbers based on section and speaker type\n",
    "        if section_type == 'presentation':\n",
    "            # No question/answer numbering in presentation\n",
    "            final_question_num = None\n",
    "            final_answer_num = None\n",
    "        elif section_type == 'qa':\n",
    "            if speaker_info['role'] == 'Analyst':\n",
    "                # Analyst question\n",
    "                final_question_num = question_num\n",
    "                final_answer_num = None\n",
    "            elif speaker_info['company'] == 'HSBC':\n",
    "                # HSBC executive answer\n",
    "                final_question_num = None\n",
    "                final_answer_num = answer_num if answer_num > 0 else None\n",
    "            else:\n",
    "                final_question_num = None\n",
    "                final_answer_num = None\n",
    "        else:\n",
    "            final_question_num = None\n",
    "            final_answer_num = None\n",
    "        \n",
    "        # Fix role assignment based on section\n",
    "        final_role = speaker_info['role']\n",
    "        if speaker_info['company'] == 'HSBC':\n",
    "            if section_type == 'presentation':\n",
    "                # Keep specific job title for presentation section\n",
    "                final_role = speaker_info['role']\n",
    "            elif section_type == 'qa':\n",
    "                # Use generic 'management' for Q&A section\n",
    "                final_role = 'management'\n",
    "            \n",
    "        records.append({\n",
    "            'section': section_type,\n",
    "            'question_number': final_question_num,\n",
    "            'answer_number': final_answer_num,\n",
    "            'speaker_name': speaker_info['speaker_name'],\n",
    "            'role': final_role,\n",
    "            'company': speaker_info['company'],\n",
    "            'content': content_text,\n",
    "            'year': metadata['year'],\n",
    "            'quarter': metadata['quarter'],\n",
    "            'is_pleasantry': self.is_pleasantry(content_text),\n",
    "            'source_pdf': metadata['filename']\n",
    "        })\n",
    "\n",
    "    def is_pleasantry(self, content: str) -> bool:\n",
    "        \"\"\"Determine if content is a pleasantry/greeting\"\"\"\n",
    "        content_lower = content.lower().strip()\n",
    "        \n",
    "        pleasantry_patterns = [\n",
    "            r'^(?:good (?:morning|afternoon|evening)|hi|hello|thanks?|thank you)(?:\\s|$|\\.)',\n",
    "            r'^(?:thanks?|thank you)(?:\\s|\\.)',\n",
    "            r'^(?:hi|hello)\\s*(?:everyone|all)?(?:\\s|$|\\.)',\n",
    "            r'^(?:good\\s+(?:morning|afternoon)|thanks?\\s*(?:very\\s+much)?)\\s*[,.]?\\s*$',\n",
    "            r'^(?:thank you for (?:taking|the))',\n",
    "        ]\n",
    "        \n",
    "        for pattern in pleasantry_patterns:\n",
    "            if re.match(pattern, content_lower):\n",
    "                return True\n",
    "                \n",
    "        return len(content.split()) <= 8 and any(word in content_lower for word in ['thank', 'good', 'hi', 'hello'])\n",
    "\n",
    "    def process_single_file(self, pdf_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Process a single PDF file\"\"\"\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        logging.info(f\"Processing: {filename}\")\n",
    "        \n",
    "        # Extract text\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not text.strip():\n",
    "            logging.warning(f\"No text extracted from {filename}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Parse metadata\n",
    "        metadata = self.parse_filename_metadata(filename)\n",
    "        metadata['filename'] = filename\n",
    "        \n",
    "        # Parse content\n",
    "        records = self.simple_parse_transcript_content(text, metadata)\n",
    "        \n",
    "        if not records:\n",
    "            logging.warning(f\"No records parsed from {filename}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "    def process_directory(self, input_dir: str, output_dir: str) -> pd.DataFrame:\n",
    "        \"\"\"Process all PDF files in directory\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        all_records = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        for filename in sorted(os.listdir(input_dir)):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(input_dir, filename)\n",
    "                df = self.process_single_file(pdf_path)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    all_records.append(df)\n",
    "                    \n",
    "                    # Save individual quarter file\n",
    "                    quarter_filename = f\"hsbc_{filename.replace('.pdf', '')}_parsed_final.csv\"\n",
    "                    quarter_path = os.path.join(output_dir, quarter_filename)\n",
    "                    df.to_csv(quarter_path, index=False)\n",
    "                    logging.info(f\"Saved {len(df)} records to {quarter_filename}\")\n",
    "        \n",
    "        # Combine all data\n",
    "        if all_records:\n",
    "            combined_df = pd.concat(all_records, ignore_index=True)\n",
    "            \n",
    "            # Save combined file\n",
    "            combined_path = os.path.join(output_dir, 'all_hsbc_earnings_data.csv')\n",
    "            combined_df.to_csv(combined_path, index=False)\n",
    "            logging.info(f\"Saved combined dataset: {len(combined_df)} records\")\n",
    "            \n",
    "            return combined_df\n",
    "        else:\n",
    "            logging.warning(\"No data processed successfully\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Example usage/test function\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    input_directory = \"~/data/raw/hsbc\"  # Update path as needed\n",
    "    output_directory = \"~/data/processed/hsbc\"  # Update path as needed\n",
    "    \n",
    "    # Expand user paths\n",
    "    input_directory = os.path.expanduser(input_directory)\n",
    "    output_directory = os.path.expanduser(output_directory)\n",
    "    \n",
    "    if not os.path.exists(input_directory):\n",
    "        logging.error(f\"Input directory not found: {input_directory}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        parser = HSBCEarningsParser()\n",
    "        df = parser.process_directory(input_directory, output_directory)\n",
    "        \n",
    "        if not df.empty:\n",
    "            logging.info(f\"Successfully processed {len(df)} total records\")\n",
    "            logging.info(f\"Data shape: {df.shape}\")\n",
    "            logging.info(f\"Years covered: {sorted(df['year'].unique())}\")\n",
    "            logging.info(f\"Quarters covered: {sorted(df['quarter'].unique())}\")\n",
    "            \n",
    "            # Show sample of data\n",
    "            print(\"\\nSample data:\")\n",
    "            print(df[['speaker_name', 'role', 'company', 'section']].head(10))\n",
    "            \n",
    "            # Show role distribution\n",
    "            print(\"\\nRole distribution:\")\n",
    "            print(df['role'].value_counts())\n",
    "            \n",
    "        else:\n",
    "            logging.warning(\"No data was processed successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
