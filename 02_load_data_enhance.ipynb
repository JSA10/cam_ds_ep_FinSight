{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ6edLHd7tg4",
        "outputId": "04f20158-2298-4197-bb66-35be875cb8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Enhanced data loading for banks: JPM, HSBC\n",
            "Quarters: q1_2025, q2_2025\n"
          ]
        }
      ],
      "source": [
        "# 02_load_data_enhanced.ipynb\n",
        "# Purpose: Load earnings call transcript data for both JP Morgan and HSBC\n",
        "# Banks: JP Morgan (JPM) and HSBC\n",
        "# Quarters: Q1 2025, Q2 2025\n",
        "# Input: Google Drive CSV files for both banks\n",
        "# Output: Enhanced raw datasets for both banks and quarters\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import io\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load enhanced configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/configs/enhanced_config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    enhanced_config = json.load(f)\n",
        "\n",
        "SEED = enhanced_config[\"SEED\"]\n",
        "BANKS = enhanced_config[\"BANKS\"]\n",
        "QUARTERS = enhanced_config[\"QUARTERS\"]\n",
        "MODELS = enhanced_config[\"MODELS\"]\n",
        "drive_base = Path(enhanced_config[\"drive_base\"])\n",
        "colab_base = Path(enhanced_config[\"colab_base\"])\n",
        "data_urls = enhanced_config[\"data_urls\"]\n",
        "\n",
        "print(f\"Enhanced data loading for banks: {', '.join([bank.upper() for bank in BANKS])}\")\n",
        "print(f\"Quarters: {', '.join(QUARTERS)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Helper Functions\n",
        "\n",
        "def extract_file_id_from_drive_url(url: str) -> str:\n",
        "    \"\"\"Extract file ID from Google Drive sharing URL.\"\"\"\n",
        "    if \"drive.google.com\" in url and \"/file/d/\" in url:\n",
        "        return url.split(\"/file/d/\")[1].split(\"/\")[0]\n",
        "    return None\n",
        "\n",
        "def download_from_drive_enhanced(file_id: str, filename: str, bank_code: str) -> Tuple[Path, Path]:\n",
        "    \"\"\"Enhanced download function with bank-specific paths.\"\"\"\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    # Bank-specific paths\n",
        "    drive_path = drive_base / f\"data/raw/{bank_code}\" / filename\n",
        "    colab_path = colab_base / f\"data/raw/{bank_code}\" / filename\n",
        "\n",
        "    # Ensure directories exist\n",
        "    drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    colab_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(download_url, timeout=300)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save to both locations\n",
        "        with open(drive_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        with open(colab_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"✅ Downloaded {filename} for {bank_code.upper()}\")\n",
        "        print(f\"  Drive: {drive_path}\")\n",
        "        print(f\"  Colab: {colab_path}\")\n",
        "\n",
        "        return drive_path, colab_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {filename} for {bank_code.upper()}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def read_csv_safe_enhanced(path: Path, encoding='utf-8') -> pd.DataFrame:\n",
        "    \"\"\"Enhanced CSV reading with multiple encoding attempts.\"\"\"\n",
        "    encodings_to_try = [encoding, 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "\n",
        "    for enc in encodings_to_try:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc)\n",
        "            print(f\"✅ Loaded {path.name}: {len(df):,} rows × {len(df.columns)} cols (encoding: {enc})\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading {path.name}: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "xr4QCCxo7_Na"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Download Enhanced Datasets\n",
        "\n",
        "def download_bank_datasets(bank_code: str) -> Dict[str, Tuple[Path, Path]]:\n",
        "    \"\"\"Download all datasets for a specific bank.\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"DOWNLOADING {bank_code.upper()} DATASETS\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    bank_urls = data_urls[bank_code]\n",
        "    downloaded_files = {}\n",
        "\n",
        "    # Download quarterly data\n",
        "    for quarter in QUARTERS:\n",
        "        if quarter in bank_urls:\n",
        "            print(f\"\\nDownloading {bank_code.upper()} {quarter.upper()} data...\")\n",
        "\n",
        "            file_id = extract_file_id_from_drive_url(bank_urls[quarter])\n",
        "            filename = f\"{bank_code}-{quarter.replace('_', '')}-earnings-call-transcript.csv\"\n",
        "\n",
        "            if file_id:\n",
        "                drive_path, colab_path = download_from_drive_enhanced(file_id, filename, bank_code)\n",
        "                if drive_path and colab_path:\n",
        "                    downloaded_files[quarter] = (drive_path, colab_path)\n",
        "            else:\n",
        "                print(f\"❌ Could not extract file ID from {quarter} URL\")\n",
        "\n",
        "    # Download manual labels if available\n",
        "    if \"manual_labels\" in bank_urls:\n",
        "        print(f\"\\nDownloading {bank_code.upper()} manual labels...\")\n",
        "\n",
        "        file_id = extract_file_id_from_drive_url(bank_urls[\"manual_labels\"])\n",
        "        filename = f\"{bank_code}_manual_labels.csv\"\n",
        "\n",
        "        if file_id:\n",
        "            drive_path, colab_path = download_from_drive_enhanced(file_id, filename, bank_code)\n",
        "            if drive_path and colab_path:\n",
        "                downloaded_files[\"manual_labels\"] = (drive_path, colab_path)\n",
        "        else:\n",
        "            print(f\"❌ Could not extract file ID from manual labels URL\")\n",
        "\n",
        "    return downloaded_files\n",
        "\n",
        "# Download datasets for both banks\n",
        "all_downloaded_files = {}\n",
        "for bank in BANKS:\n",
        "    all_downloaded_files[bank] = download_bank_datasets(bank)\n"
      ],
      "metadata": {
        "id": "K-3IaTco8DY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9630d59e-424a-427f-8858-5c7727a4ee94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "DOWNLOADING JPM DATASETS\n",
            "==================================================\n",
            "\n",
            "Downloading JPM Q1_2025 data...\n",
            "✅ Downloaded jpm-q12025-earnings-call-transcript.csv for JPM\n",
            "  Drive: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/raw/jpm/jpm-q12025-earnings-call-transcript.csv\n",
            "  Colab: /content/cam_ds_ai_enhanced/data/raw/jpm/jpm-q12025-earnings-call-transcript.csv\n",
            "\n",
            "Downloading JPM Q2_2025 data...\n",
            "✅ Downloaded jpm-q22025-earnings-call-transcript.csv for JPM\n",
            "  Drive: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/raw/jpm/jpm-q22025-earnings-call-transcript.csv\n",
            "  Colab: /content/cam_ds_ai_enhanced/data/raw/jpm/jpm-q22025-earnings-call-transcript.csv\n",
            "\n",
            "Downloading JPM manual labels...\n",
            "✅ Downloaded jpm_manual_labels.csv for JPM\n",
            "  Drive: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/raw/jpm/jpm_manual_labels.csv\n",
            "  Colab: /content/cam_ds_ai_enhanced/data/raw/jpm/jpm_manual_labels.csv\n",
            "\n",
            "==================================================\n",
            "DOWNLOADING HSBC DATASETS\n",
            "==================================================\n",
            "\n",
            "Downloading HSBC Q1_2025 data...\n",
            "✅ Downloaded hsbc-q12025-earnings-call-transcript.csv for HSBC\n",
            "  Drive: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/raw/hsbc/hsbc-q12025-earnings-call-transcript.csv\n",
            "  Colab: /content/cam_ds_ai_enhanced/data/raw/hsbc/hsbc-q12025-earnings-call-transcript.csv\n",
            "\n",
            "Downloading HSBC Q2_2025 data...\n",
            "✅ Downloaded hsbc-q22025-earnings-call-transcript.csv for HSBC\n",
            "  Drive: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/raw/hsbc/hsbc-q22025-earnings-call-transcript.csv\n",
            "  Colab: /content/cam_ds_ai_enhanced/data/raw/hsbc/hsbc-q22025-earnings-call-transcript.csv\n",
            "\n",
            "Downloading HSBC manual labels...\n",
            "✅ Downloaded hsbc_manual_labels.csv for HSBC\n",
            "  Drive: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/raw/hsbc/hsbc_manual_labels.csv\n",
            "  Colab: /content/cam_ds_ai_enhanced/data/raw/hsbc/hsbc_manual_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Data Loading and Validation\n",
        "\n",
        "def load_and_validate_datasets():\n",
        "    \"\"\"Load and validate all downloaded datasets.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"LOADING AND VALIDATING DATASETS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    loaded_datasets = {}\n",
        "\n",
        "    for bank in BANKS:\n",
        "        print(f\"\\n📊 Loading {bank.upper()} datasets...\")\n",
        "        loaded_datasets[bank] = {}\n",
        "\n",
        "        bank_files = all_downloaded_files[bank]\n",
        "\n",
        "        # Load quarterly data\n",
        "        for quarter in QUARTERS:\n",
        "            if quarter in bank_files:\n",
        "                drive_path, colab_path = bank_files[quarter]\n",
        "\n",
        "                if drive_path and drive_path.exists():\n",
        "                    df = read_csv_safe_enhanced(drive_path)\n",
        "                    if df is not None:\n",
        "                        loaded_datasets[bank][quarter] = df\n",
        "\n",
        "                        # Basic validation\n",
        "                        print(f\"  {quarter.upper()}: {df.shape}\")\n",
        "                        if 'text' in df.columns or 'content' in df.columns:\n",
        "                            text_col = 'text' if 'text' in df.columns else 'content'\n",
        "                            avg_text_length = df[text_col].astype(str).str.len().mean()\n",
        "                            print(f\"    Average text length: {avg_text_length:.0f} characters\")\n",
        "\n",
        "                        if 'speaker' in df.columns or 'speaker_name' in df.columns:\n",
        "                            speaker_col = 'speaker' if 'speaker' in df.columns else 'speaker_name'\n",
        "                            unique_speakers = df[speaker_col].nunique()\n",
        "                            print(f\"    Unique speakers: {unique_speakers}\")\n",
        "                    else:\n",
        "                        print(f\"  ❌ Failed to load {quarter} data\")\n",
        "                else:\n",
        "                    print(f\"  ❌ {quarter} file not found\")\n",
        "\n",
        "        # Load manual labels if available\n",
        "        if \"manual_labels\" in bank_files:\n",
        "            drive_path, colab_path = bank_files[\"manual_labels\"]\n",
        "\n",
        "            if drive_path and drive_path.exists():\n",
        "                df = read_csv_safe_enhanced(drive_path)\n",
        "                if df is not None:\n",
        "                    loaded_datasets[bank][\"manual_labels\"] = df\n",
        "                    print(f\"  Manual Labels: {df.shape}\")\n",
        "\n",
        "                    # Validate manual labels structure\n",
        "                    label_cols = [col for col in df.columns if 'label' in col.lower()]\n",
        "                    if label_cols:\n",
        "                        print(f\"    Label columns: {label_cols}\")\n",
        "\n",
        "                        # Check label distribution\n",
        "                        for label_col in label_cols:\n",
        "                            if df[label_col].notna().sum() > 0:\n",
        "                                dist = df[label_col].value_counts()\n",
        "                                print(f\"    {label_col} distribution: {dist.to_dict()}\")\n",
        "                else:\n",
        "                    print(f\"  ❌ Failed to load manual labels\")\n",
        "\n",
        "    return loaded_datasets\n",
        "\n",
        "# Load all datasets\n",
        "loaded_datasets = load_and_validate_datasets()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW4q4UVF8JEc",
        "outputId": "37a601d2-2dcd-48ea-8e27-b72b2c41bc97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LOADING AND VALIDATING DATASETS\n",
            "============================================================\n",
            "\n",
            "📊 Loading JPM datasets...\n",
            "✅ Loaded jpm-q12025-earnings-call-transcript.csv: 112 rows × 10 cols (encoding: utf-8)\n",
            "  Q1_2025: (112, 10)\n",
            "    Average text length: 518 characters\n",
            "    Unique speakers: 14\n",
            "✅ Loaded jpm-q22025-earnings-call-transcript.csv: 149 rows × 10 cols (encoding: utf-8)\n",
            "  Q2_2025: (149, 10)\n",
            "    Average text length: 372 characters\n",
            "    Unique speakers: 17\n",
            "✅ Loaded jpm_manual_labels.csv: 1,121 rows × 27 cols (encoding: utf-8)\n",
            "  Manual Labels: (1121, 27)\n",
            "    Label columns: ['human_label']\n",
            "    human_label distribution: {'neutral': 19, 'positive': 19, 'negative': 11}\n",
            "\n",
            "📊 Loading HSBC datasets...\n",
            "✅ Loaded hsbc-q12025-earnings-call-transcript.csv: 49 rows × 11 cols (encoding: utf-8)\n",
            "  Q1_2025: (49, 11)\n",
            "    Average text length: 1121 characters\n",
            "    Unique speakers: 14\n",
            "✅ Loaded hsbc-q22025-earnings-call-transcript.csv: 30 rows × 11 cols (encoding: utf-8)\n",
            "  Q2_2025: (30, 11)\n",
            "    Average text length: 1714 characters\n",
            "    Unique speakers: 10\n",
            "✅ Loaded hsbc_manual_labels.csv: 858 rows × 27 cols (encoding: utf-8)\n",
            "  Manual Labels: (858, 27)\n",
            "    Label columns: ['human_label']\n",
            "    human_label distribution: {'positive': 39, 'neutral': 20, 'negative': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Data Exploration\n",
        "\n",
        "def explore_dataset_enhanced(df: pd.DataFrame, dataset_name: str, bank_code: str):\n",
        "    \"\"\"Enhanced dataset exploration with bank context.\"\"\"\n",
        "    if df is None:\n",
        "        print(f\"❌ {bank_code.upper()} {dataset_name} dataset is None\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n📋 [{bank_code.upper()}] {dataset_name.upper()} DATASET EXPLORATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Basic info\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Column types analysis\n",
        "    print(f\"\\nColumn Types:\")\n",
        "    for col, dtype in df.dtypes.items():\n",
        "        null_count = df[col].isnull().sum()\n",
        "        null_pct = (null_count / len(df)) * 100\n",
        "        print(f\"  {col}: {dtype} ({null_count} nulls, {null_pct:.1f}%)\")\n",
        "\n",
        "    # Memory usage\n",
        "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    print(f\"\\nMemory usage: {memory_mb:.2f} MB\")\n",
        "\n",
        "    # Text analysis for relevant columns\n",
        "    text_columns = [col for col in df.columns\n",
        "                   if col.lower() in ['text', 'content', 'question', 'answer', 'transcript']]\n",
        "\n",
        "    for text_col in text_columns:\n",
        "        if text_col in df.columns and df[text_col].dtype == 'object':\n",
        "            text_data = df[text_col].dropna().astype(str)\n",
        "            if len(text_data) > 0:\n",
        "                print(f\"\\n{text_col} Analysis:\")\n",
        "                print(f\"  Valid entries: {len(text_data)}\")\n",
        "                print(f\"  Avg length: {text_data.str.len().mean():.0f} chars\")\n",
        "                print(f\"  Avg words: {text_data.str.split().str.len().mean():.0f} words\")\n",
        "                print(f\"  Length range: {text_data.str.len().min()}-{text_data.str.len().max()} chars\")\n",
        "\n",
        "                # Sample text\n",
        "                sample_text = text_data.iloc[0] if len(text_data) > 0 else \"\"\n",
        "                print(f\"  Sample: '{sample_text[:100]}...'\")\n",
        "\n",
        "    # Speaker analysis\n",
        "    speaker_columns = [col for col in df.columns\n",
        "                      if 'speaker' in col.lower() or 'role' in col.lower()]\n",
        "\n",
        "    for speaker_col in speaker_columns:\n",
        "        if speaker_col in df.columns:\n",
        "            speaker_dist = df[speaker_col].value_counts()\n",
        "            print(f\"\\n{speaker_col} Distribution:\")\n",
        "            for speaker, count in speaker_dist.head(10).items():\n",
        "                print(f\"  {speaker}: {count}\")\n",
        "\n",
        "    # Sample data preview\n",
        "    print(f\"\\nFirst 3 rows:\")\n",
        "    display_cols = df.columns[:5] if len(df.columns) > 5 else df.columns\n",
        "    print(df[display_cols].head(3).to_string())\n",
        "\n",
        "# Explore all loaded datasets\n",
        "for bank in BANKS:\n",
        "    for dataset_name, df in loaded_datasets[bank].items():\n",
        "        if df is not None:\n",
        "            explore_dataset_enhanced(df, dataset_name, bank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCU1QZ6v8M-n",
        "outputId": "950e4c99-84eb-475b-a063-be1906713dee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 [JPM] Q1_2025 DATASET EXPLORATION\n",
            "--------------------------------------------------\n",
            "Shape: (112, 10)\n",
            "Columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry']\n",
            "\n",
            "Column Types:\n",
            "  section: object (0 nulls, 0.0%)\n",
            "  question_number: float64 (1 nulls, 0.9%)\n",
            "  answer_number: float64 (23 nulls, 20.5%)\n",
            "  speaker_name: object (0 nulls, 0.0%)\n",
            "  role: object (0 nulls, 0.0%)\n",
            "  company: object (0 nulls, 0.0%)\n",
            "  content: object (0 nulls, 0.0%)\n",
            "  year: int64 (0 nulls, 0.0%)\n",
            "  quarter: object (0 nulls, 0.0%)\n",
            "  is_pleasantry: bool (0 nulls, 0.0%)\n",
            "\n",
            "Memory usage: 0.13 MB\n",
            "\n",
            "content Analysis:\n",
            "  Valid entries: 112\n",
            "  Avg length: 518 chars\n",
            "  Avg words: 91 words\n",
            "  Length range: 5-10011 chars\n",
            "  Sample: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 bill...'\n",
            "\n",
            "speaker_name Distribution:\n",
            "  Jeremy Barnum: 37\n",
            "  Jamie Dimon: 31\n",
            "  Betsy L. Graseck: 7\n",
            "  John McDonald: 5\n",
            "  Jim Mitchell: 5\n",
            "  Mike Mayo: 5\n",
            "  Ebrahim Poonawala: 4\n",
            "  Steven Chubak: 3\n",
            "  Erika Najarian: 3\n",
            "  Matt O’Connor: 3\n",
            "\n",
            "role Distribution:\n",
            "  analyst: 44\n",
            "  Chief Financial Officer: 37\n",
            "  Chairman & Chief Executive Officer: 31\n",
            "\n",
            "First 3 rows:\n",
            "        section  question_number  answer_number   speaker_name                     role\n",
            "0  presentation              NaN            NaN  Jeremy Barnum  Chief Financial Officer\n",
            "1            qa              1.0            NaN      Ken Usdin                  analyst\n",
            "2            qa              1.0            1.0  Jeremy Barnum  Chief Financial Officer\n",
            "\n",
            "📋 [JPM] Q2_2025 DATASET EXPLORATION\n",
            "--------------------------------------------------\n",
            "Shape: (149, 10)\n",
            "Columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry']\n",
            "\n",
            "Column Types:\n",
            "  section: object (0 nulls, 0.0%)\n",
            "  question_number: float64 (5 nulls, 3.4%)\n",
            "  answer_number: float64 (39 nulls, 26.2%)\n",
            "  speaker_name: object (0 nulls, 0.0%)\n",
            "  role: object (0 nulls, 0.0%)\n",
            "  company: object (0 nulls, 0.0%)\n",
            "  content: object (0 nulls, 0.0%)\n",
            "  year: int64 (0 nulls, 0.0%)\n",
            "  quarter: object (0 nulls, 0.0%)\n",
            "  is_pleasantry: bool (0 nulls, 0.0%)\n",
            "\n",
            "Memory usage: 0.13 MB\n",
            "\n",
            "content Analysis:\n",
            "  Valid entries: 149\n",
            "  Avg length: 372 chars\n",
            "  Avg words: 67 words\n",
            "  Length range: 5-2490 chars\n",
            "  Sample: 'names, partially offset by the scenario probability adjustment I mentioned upfront. Turning to Asset...'\n",
            "\n",
            "speaker_name Distribution:\n",
            "  Jeremy Barnum: 55\n",
            "  Jamie Dimon: 35\n",
            "  Ebrahim Poonawala: 6\n",
            "  Mike Mayo: 6\n",
            "  Gerard Cassidy: 6\n",
            "  Betsy L. Graseck: 5\n",
            "  Christopher McGratty: 5\n",
            "  Jim Mitchell: 4\n",
            "  Steven Alexopolous: 4\n",
            "  Glenn Schorr: 4\n",
            "\n",
            "role Distribution:\n",
            "  analyst: 57\n",
            "  Chief Financial Officer: 55\n",
            "  Chairman & Chief Executive Officer: 35\n",
            "  And then some. Theres a lot of value added.: 1\n",
            "  Okay: 1\n",
            "\n",
            "First 3 rows:\n",
            "        section  question_number  answer_number          speaker_name                     role\n",
            "0  presentation              NaN            NaN         Jeremy Barnum  Chief Financial Officer\n",
            "1            qa              NaN            NaN  Christopher McGratty                  analyst\n",
            "2            qa              NaN            NaN         Jeremy Barnum  Chief Financial Officer\n",
            "\n",
            "📋 [JPM] MANUAL_LABELS DATASET EXPLORATION\n",
            "--------------------------------------------------\n",
            "Shape: (1121, 27)\n",
            "Columns: ['original_qa_id', 'sentence_id', 'text', 'speaker', 'speaker_role', 'quarter', 'bank_code', 'dataset_id', 'sentence_length', 'sentence_word_count', 'sentence_index_in_qa', 'text_quality_score', 'financial_relevance_score', 'has_revenue_mentions', 'has_risk_mentions', 'has_growth_mentions', 'has_performance_mentions', 'ready_for_finbert', 'ready_for_distilroberta', 'ready_for_cardiffnlp', 'has_financial_numbers', 'has_financial_entities', 'sentence_complexity', 'human_label', 'human_confidence', 'annotation_notes', 'annotator_id']\n",
            "\n",
            "Column Types:\n",
            "  original_qa_id: int64 (0 nulls, 0.0%)\n",
            "  sentence_id: object (0 nulls, 0.0%)\n",
            "  text: object (0 nulls, 0.0%)\n",
            "  speaker: object (0 nulls, 0.0%)\n",
            "  speaker_role: object (0 nulls, 0.0%)\n",
            "  quarter: object (0 nulls, 0.0%)\n",
            "  bank_code: object (0 nulls, 0.0%)\n",
            "  dataset_id: object (0 nulls, 0.0%)\n",
            "  sentence_length: int64 (0 nulls, 0.0%)\n",
            "  sentence_word_count: int64 (0 nulls, 0.0%)\n",
            "  sentence_index_in_qa: int64 (0 nulls, 0.0%)\n",
            "  text_quality_score: float64 (0 nulls, 0.0%)\n",
            "  financial_relevance_score: float64 (0 nulls, 0.0%)\n",
            "  has_revenue_mentions: bool (0 nulls, 0.0%)\n",
            "  has_risk_mentions: bool (0 nulls, 0.0%)\n",
            "  has_growth_mentions: bool (0 nulls, 0.0%)\n",
            "  has_performance_mentions: bool (0 nulls, 0.0%)\n",
            "  ready_for_finbert: bool (0 nulls, 0.0%)\n",
            "  ready_for_distilroberta: bool (0 nulls, 0.0%)\n",
            "  ready_for_cardiffnlp: bool (0 nulls, 0.0%)\n",
            "  has_financial_numbers: bool (0 nulls, 0.0%)\n",
            "  has_financial_entities: bool (0 nulls, 0.0%)\n",
            "  sentence_complexity: int64 (0 nulls, 0.0%)\n",
            "  human_label: object (1072 nulls, 95.6%)\n",
            "  human_confidence: float64 (1072 nulls, 95.6%)\n",
            "  annotation_notes: object (1120 nulls, 99.9%)\n",
            "  annotator_id: object (1072 nulls, 95.6%)\n",
            "\n",
            "Memory usage: 0.73 MB\n",
            "\n",
            "text Analysis:\n",
            "  Valid entries: 1121\n",
            "  Avg length: 98 chars\n",
            "  Avg words: 18 words\n",
            "  Length range: 20-517 chars\n",
            "  Sample: 'Thank you and good morning, everyone....'\n",
            "\n",
            "speaker Distribution:\n",
            "  jeremy barnum: 466\n",
            "  jamie dimon: 383\n",
            "  betsy l. graseck: 31\n",
            "  mike mayo: 30\n",
            "  erika najarian: 28\n",
            "  glenn schorr: 21\n",
            "  ebrahim poonawala: 20\n",
            "  gerard cassidy: 19\n",
            "  jim mitchell: 17\n",
            "  saul martinez: 15\n",
            "\n",
            "speaker_role Distribution:\n",
            "  executive: 860\n",
            "  analyst: 261\n",
            "\n",
            "First 3 rows:\n",
            "   original_qa_id    sentence_id                                                                                                                                     text        speaker speaker_role\n",
            "0               0  jpm_multi_0_0                                                                                                    Thank you and good morning, everyone.  jeremy barnum    executive\n",
            "1               0  jpm_multi_0_1  Starting on page 1, the Firm reported net income of $14.6 billion, EPS of $5.07 on revenue of $46 billion, with an ROTCE of 21 percent.  jeremy barnum    executive\n",
            "2               0  jpm_multi_0_2                        These results included a First Republic-related gain of $588 million, which was previously disclosed in the 10-K.  jeremy barnum    executive\n",
            "\n",
            "📋 [HSBC] Q1_2025 DATASET EXPLORATION\n",
            "--------------------------------------------------\n",
            "Shape: (49, 11)\n",
            "Columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry', 'source_pdf']\n",
            "\n",
            "Column Types:\n",
            "  section: object (0 nulls, 0.0%)\n",
            "  question_number: float64 (38 nulls, 77.6%)\n",
            "  answer_number: float64 (14 nulls, 28.6%)\n",
            "  speaker_name: object (1 nulls, 2.0%)\n",
            "  role: object (1 nulls, 2.0%)\n",
            "  company: object (1 nulls, 2.0%)\n",
            "  content: object (0 nulls, 0.0%)\n",
            "  year: int64 (0 nulls, 0.0%)\n",
            "  quarter: object (0 nulls, 0.0%)\n",
            "  is_pleasantry: bool (0 nulls, 0.0%)\n",
            "  source_pdf: object (0 nulls, 0.0%)\n",
            "\n",
            "Memory usage: 0.13 MB\n",
            "\n",
            "content Analysis:\n",
            "  Valid entries: 49\n",
            "  Avg length: 1121 chars\n",
            "  Avg words: 200 words\n",
            "  Length range: 22-5960 chars\n",
            "  Sample: 'Welcome, all, to today ’s call .  I’m joined here in London by Pam .  Before Pam takes you through t...'\n",
            "\n",
            "speaker_name Distribution:\n",
            "  Georges Elhedery: 22\n",
            "  Pam Kaur: 14\n",
            "  Benjamin Toms: 1\n",
            "  Joseph Dickerson: 1\n",
            "  Kunpeng Ma: 1\n",
            "  Aman Rakkar: 1\n",
            "  Jason Napier: 1\n",
            "  Kian Abouhossein: 1\n",
            "  Amit Goel: 1\n",
            "  Gurpreet Singh Sahi: 1\n",
            "\n",
            "role Distribution:\n",
            "  management: 35\n",
            "  Analyst: 11\n",
            "  Group Chief Financial Officer: 1\n",
            "  Group Chief Executive: 1\n",
            "\n",
            "First 3 rows:\n",
            "        section  question_number  answer_number      speaker_name                           role\n",
            "0  presentation              NaN            NaN  Georges Elhedery          Group Chief Executive\n",
            "1  presentation              NaN            NaN          Pam Kaur  Group Chief Financial Officer\n",
            "2            qa              1.0            NaN     Benjamin Toms                        Analyst\n",
            "\n",
            "📋 [HSBC] Q2_2025 DATASET EXPLORATION\n",
            "--------------------------------------------------\n",
            "Shape: (30, 11)\n",
            "Columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry', 'source_pdf']\n",
            "\n",
            "Column Types:\n",
            "  section: object (0 nulls, 0.0%)\n",
            "  question_number: float64 (22 nulls, 73.3%)\n",
            "  answer_number: float64 (14 nulls, 46.7%)\n",
            "  speaker_name: object (4 nulls, 13.3%)\n",
            "  role: object (4 nulls, 13.3%)\n",
            "  company: object (4 nulls, 13.3%)\n",
            "  content: object (0 nulls, 0.0%)\n",
            "  year: int64 (0 nulls, 0.0%)\n",
            "  quarter: object (0 nulls, 0.0%)\n",
            "  is_pleasantry: bool (0 nulls, 0.0%)\n",
            "  source_pdf: object (0 nulls, 0.0%)\n",
            "\n",
            "Memory usage: 0.11 MB\n",
            "\n",
            "content Analysis:\n",
            "  Valid entries: 30\n",
            "  Avg length: 1714 chars\n",
            "  Avg words: 296 words\n",
            "  Length range: 84-8667 chars\n",
            "  Sample: 'Welcome, all, to today’s call. I’m joined by Pam. Before Pam takes you through the second quarter nu...'\n",
            "\n",
            "speaker_name Distribution:\n",
            "  Georges Elhedery: 9\n",
            "  Pam Kaur: 9\n",
            "  Benjamin Toms: 1\n",
            "  Kian Abouhossein: 1\n",
            "  Kunpeng Ma: 1\n",
            "  Aman Rakkar: 1\n",
            "  Kendra Yan: 1\n",
            "  Joseph Dickerson: 1\n",
            "  Gurpreet Singh Sahi: 1\n",
            "  Katherine Lei: 1\n",
            "\n",
            "role Distribution:\n",
            "  management: 16\n",
            "  Analyst: 8\n",
            "  Group Chief Financial Officer: 1\n",
            "  Group Chief Executive: 1\n",
            "\n",
            "First 3 rows:\n",
            "        section  question_number  answer_number      speaker_name                           role\n",
            "0  presentation              NaN            NaN  Georges Elhedery          Group Chief Executive\n",
            "1  presentation              NaN            NaN          Pam Kaur  Group Chief Financial Officer\n",
            "2  presentation              NaN            NaN               NaN                            NaN\n",
            "\n",
            "📋 [HSBC] MANUAL_LABELS DATASET EXPLORATION\n",
            "--------------------------------------------------\n",
            "Shape: (858, 27)\n",
            "Columns: ['original_qa_id', 'sentence_id', 'text', 'speaker', 'speaker_role', 'quarter', 'bank_code', 'dataset_id', 'sentence_length', 'sentence_word_count', 'sentence_index_in_qa', 'text_quality_score', 'financial_relevance_score', 'has_revenue_mentions', 'has_risk_mentions', 'has_growth_mentions', 'has_performance_mentions', 'ready_for_finbert', 'ready_for_distilroberta', 'ready_for_cardiffnlp', 'has_financial_numbers', 'has_financial_entities', 'sentence_complexity', 'human_label', 'human_confidence', 'annotation_notes', 'annotator_id']\n",
            "\n",
            "Column Types:\n",
            "  original_qa_id: int64 (0 nulls, 0.0%)\n",
            "  sentence_id: object (0 nulls, 0.0%)\n",
            "  text: object (0 nulls, 0.0%)\n",
            "  speaker: object (56 nulls, 6.5%)\n",
            "  speaker_role: object (0 nulls, 0.0%)\n",
            "  quarter: object (0 nulls, 0.0%)\n",
            "  bank_code: object (0 nulls, 0.0%)\n",
            "  dataset_id: object (0 nulls, 0.0%)\n",
            "  sentence_length: int64 (0 nulls, 0.0%)\n",
            "  sentence_word_count: int64 (0 nulls, 0.0%)\n",
            "  sentence_index_in_qa: int64 (0 nulls, 0.0%)\n",
            "  text_quality_score: float64 (0 nulls, 0.0%)\n",
            "  financial_relevance_score: float64 (0 nulls, 0.0%)\n",
            "  has_revenue_mentions: bool (0 nulls, 0.0%)\n",
            "  has_risk_mentions: bool (0 nulls, 0.0%)\n",
            "  has_growth_mentions: bool (0 nulls, 0.0%)\n",
            "  has_performance_mentions: bool (0 nulls, 0.0%)\n",
            "  ready_for_finbert: bool (0 nulls, 0.0%)\n",
            "  ready_for_distilroberta: bool (0 nulls, 0.0%)\n",
            "  ready_for_cardiffnlp: bool (0 nulls, 0.0%)\n",
            "  has_financial_numbers: bool (0 nulls, 0.0%)\n",
            "  has_financial_entities: bool (0 nulls, 0.0%)\n",
            "  sentence_complexity: int64 (0 nulls, 0.0%)\n",
            "  human_label: object (789 nulls, 92.0%)\n",
            "  human_confidence: float64 (789 nulls, 92.0%)\n",
            "  annotation_notes: object (855 nulls, 99.7%)\n",
            "  annotator_id: object (789 nulls, 92.0%)\n",
            "\n",
            "Memory usage: 0.61 MB\n",
            "\n",
            "text Analysis:\n",
            "  Valid entries: 858\n",
            "  Avg length: 122 chars\n",
            "  Avg words: 22 words\n",
            "  Length range: 20-467 chars\n",
            "  Sample: 'Welcome, all, to today ’s call ....'\n",
            "\n",
            "speaker Distribution:\n",
            "  georges elhedery: 327\n",
            "  pam kaur: 305\n",
            "  aman rakkar: 33\n",
            "  kunpeng ma: 18\n",
            "  katherine lei: 17\n",
            "  ed firth: 17\n",
            "  gurpreet singh sahi: 14\n",
            "  benjamin toms: 13\n",
            "  joseph dickerson: 13\n",
            "  kian abouhossein: 12\n",
            "\n",
            "speaker_role Distribution:\n",
            "  executive: 639\n",
            "  analyst: 163\n",
            "  other: 56\n",
            "\n",
            "First 3 rows:\n",
            "   original_qa_id     sentence_id                                                                                         text           speaker speaker_role\n",
            "0               0  hsbc_multi_0_0                                                             Welcome, all, to today ’s call .  georges elhedery    executive\n",
            "1               0  hsbc_multi_0_1                                                           I’m joined here in London by Pam .  georges elhedery    executive\n",
            "2               0  hsbc_multi_0_2  Before Pam takes you through the numbers, I would like to begin with some opening remarks .  georges elhedery    executive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Enhanced Combined Datasets\n",
        "\n",
        "def create_combined_datasets():\n",
        "    \"\"\"Create combined datasets for multi-quarter analysis.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"CREATING ENHANCED COMBINED DATASETS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    combined_datasets = {}\n",
        "\n",
        "    for bank in BANKS:\n",
        "        print(f\"\\n🔄 Creating combined dataset for {bank.upper()}...\")\n",
        "\n",
        "        bank_data = loaded_datasets[bank]\n",
        "        quarterly_dfs = []\n",
        "\n",
        "        # Combine quarterly data\n",
        "        for quarter in QUARTERS:\n",
        "            if quarter in bank_data and bank_data[quarter] is not None:\n",
        "                df = bank_data[quarter].copy()\n",
        "                df['quarter'] = quarter\n",
        "                df['bank_code'] = bank\n",
        "                quarterly_dfs.append(df)\n",
        "                print(f\"  Added {quarter}: {df.shape}\")\n",
        "\n",
        "        if quarterly_dfs:\n",
        "            # Check column alignment\n",
        "            all_columns = [set(df.columns) for df in quarterly_dfs]\n",
        "            common_columns = set.intersection(*all_columns)\n",
        "            all_unique_columns = set.union(*all_columns)\n",
        "\n",
        "            print(f\"  Common columns: {len(common_columns)}\")\n",
        "            print(f\"  All unique columns: {len(all_unique_columns)}\")\n",
        "\n",
        "            # Identify column differences\n",
        "            column_differences = all_unique_columns - common_columns\n",
        "            if column_differences:\n",
        "                print(f\"  Column differences: {column_differences}\")\n",
        "\n",
        "                # Align columns by adding missing ones\n",
        "                for df in quarterly_dfs:\n",
        "                    for col in column_differences:\n",
        "                        if col not in df.columns:\n",
        "                            df[col] = None\n",
        "\n",
        "            # Combine datasets\n",
        "            combined_df = pd.concat(quarterly_dfs, ignore_index=True, sort=False)\n",
        "            combined_datasets[bank] = combined_df\n",
        "\n",
        "            print(f\"  ✅ Combined dataset shape: {combined_df.shape}\")\n",
        "\n",
        "            # Verify quarter distribution\n",
        "            quarter_dist = combined_df['quarter'].value_counts()\n",
        "            print(f\"  Quarter distribution: {quarter_dist.to_dict()}\")\n",
        "        else:\n",
        "            print(f\"  ❌ No quarterly data found for {bank.upper()}\")\n",
        "\n",
        "    return combined_datasets\n",
        "\n",
        "# Create combined datasets\n",
        "combined_datasets = create_combined_datasets()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11417iuY8S0B",
        "outputId": "7f351c95-8a31-4a71-9c94-ee1eae0a04a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CREATING ENHANCED COMBINED DATASETS\n",
            "============================================================\n",
            "\n",
            "🔄 Creating combined dataset for JPM...\n",
            "  Added q1_2025: (112, 11)\n",
            "  Added q2_2025: (149, 11)\n",
            "  Common columns: 11\n",
            "  All unique columns: 11\n",
            "  ✅ Combined dataset shape: (261, 11)\n",
            "  Quarter distribution: {'q2_2025': 149, 'q1_2025': 112}\n",
            "\n",
            "🔄 Creating combined dataset for HSBC...\n",
            "  Added q1_2025: (49, 12)\n",
            "  Added q2_2025: (30, 12)\n",
            "  Common columns: 12\n",
            "  All unique columns: 12\n",
            "  ✅ Combined dataset shape: (79, 12)\n",
            "  Quarter distribution: {'q1_2025': 49, 'q2_2025': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Data Quality Assessment\n",
        "\n",
        "def assess_data_quality_enhanced():\n",
        "    \"\"\"Enhanced data quality assessment across all datasets.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ENHANCED DATA QUALITY ASSESSMENT\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    quality_report = {\n",
        "        \"banks_loaded\": len(loaded_datasets),\n",
        "        \"bank_details\": {},\n",
        "        \"overall_statistics\": {},\n",
        "        \"data_quality_flags\": []\n",
        "    }\n",
        "\n",
        "    total_records = 0\n",
        "    total_memory = 0\n",
        "\n",
        "    for bank in BANKS:\n",
        "        bank_stats = {\n",
        "            \"quarters_loaded\": 0,\n",
        "            \"total_records\": 0,\n",
        "            \"has_manual_labels\": False,\n",
        "            \"data_issues\": []\n",
        "        }\n",
        "\n",
        "        print(f\"\\n📊 [{bank.upper()}] Data Quality Assessment\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        if bank in loaded_datasets:\n",
        "            bank_data = loaded_datasets[bank]\n",
        "\n",
        "            # Check quarterly data\n",
        "            for quarter in QUARTERS:\n",
        "                if quarter in bank_data and bank_data[quarter] is not None:\n",
        "                    df = bank_data[quarter]\n",
        "                    bank_stats[\"quarters_loaded\"] += 1\n",
        "                    bank_stats[\"total_records\"] += len(df)\n",
        "                    total_records += len(df)\n",
        "                    total_memory += df.memory_usage(deep=True).sum()\n",
        "\n",
        "                    # Data quality checks\n",
        "                    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
        "                    if missing_pct > 20:\n",
        "                        bank_stats[\"data_issues\"].append(f\"{quarter}: High missing data ({missing_pct:.1f}%)\")\n",
        "\n",
        "                    # Check for essential columns\n",
        "                    essential_cols = ['text', 'content', 'speaker', 'speaker_name']\n",
        "                    has_text_col = any(col in df.columns for col in essential_cols[:2])\n",
        "                    has_speaker_col = any(col in df.columns for col in essential_cols[2:])\n",
        "\n",
        "                    if not has_text_col:\n",
        "                        bank_stats[\"data_issues\"].append(f\"{quarter}: Missing text column\")\n",
        "                    if not has_speaker_col:\n",
        "                        bank_stats[\"data_issues\"].append(f\"{quarter}: Missing speaker column\")\n",
        "\n",
        "                    print(f\"  {quarter}: {len(df)} records, {missing_pct:.1f}% missing data\")\n",
        "\n",
        "            # Check manual labels\n",
        "            if \"manual_labels\" in bank_data and bank_data[\"manual_labels\"] is not None:\n",
        "                bank_stats[\"has_manual_labels\"] = True\n",
        "                manual_df = bank_data[\"manual_labels\"]\n",
        "\n",
        "                # Count labeled records\n",
        "                label_cols = [col for col in manual_df.columns if 'label' in col.lower()]\n",
        "                if label_cols:\n",
        "                    labeled_count = manual_df[label_cols[0]].notna().sum()\n",
        "                    print(f\"  Manual labels: {labeled_count} labeled records\")\n",
        "                    bank_stats[\"labeled_records\"] = labeled_count\n",
        "\n",
        "        quality_report[\"bank_details\"][bank] = bank_stats\n",
        "\n",
        "        # Print bank summary\n",
        "        print(f\"  Quarters loaded: {bank_stats['quarters_loaded']}/{len(QUARTERS)}\")\n",
        "        print(f\"  Total records: {bank_stats['total_records']:,}\")\n",
        "        print(f\"  Has manual labels: {bank_stats['has_manual_labels']}\")\n",
        "\n",
        "        if bank_stats[\"data_issues\"]:\n",
        "            print(f\"  Issues found: {len(bank_stats['data_issues'])}\")\n",
        "            for issue in bank_stats[\"data_issues\"]:\n",
        "                print(f\"    - {issue}\")\n",
        "        else:\n",
        "            print(f\"  ✅ No major data quality issues\")\n",
        "\n",
        "    # Overall statistics\n",
        "    quality_report[\"overall_statistics\"] = {\n",
        "        \"total_records\": total_records,\n",
        "        \"total_memory_mb\": total_memory / 1024**2,\n",
        "        \"banks_with_all_quarters\": sum(1 for bank_stats in quality_report[\"bank_details\"].values()\n",
        "                                      if bank_stats[\"quarters_loaded\"] == len(QUARTERS)),\n",
        "        \"banks_with_manual_labels\": sum(1 for bank_stats in quality_report[\"bank_details\"].values()\n",
        "                                       if bank_stats[\"has_manual_labels\"])\n",
        "    }\n",
        "\n",
        "    print(f\"\\n📈 Overall Statistics:\")\n",
        "    print(f\"  Total records: {total_records:,}\")\n",
        "    print(f\"  Total memory: {total_memory / 1024**2:.2f} MB\")\n",
        "    print(f\"  Banks with all quarters: {quality_report['overall_statistics']['banks_with_all_quarters']}/{len(BANKS)}\")\n",
        "    print(f\"  Banks with manual labels: {quality_report['overall_statistics']['banks_with_manual_labels']}/{len(BANKS)}\")\n",
        "\n",
        "    return quality_report\n",
        "\n",
        "# Assess data quality\n",
        "quality_report = assess_data_quality_enhanced()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wir1Mn788Xb2",
        "outputId": "0236fd55-1610-4ffa-ad87-d2741595a119"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED DATA QUALITY ASSESSMENT\n",
            "============================================================\n",
            "\n",
            "📊 [JPM] Data Quality Assessment\n",
            "----------------------------------------\n",
            "  q1_2025: 112 records, 2.1% missing data\n",
            "  q2_2025: 149 records, 3.0% missing data\n",
            "  Manual labels: 49 labeled records\n",
            "  Quarters loaded: 2/2\n",
            "  Total records: 261\n",
            "  Has manual labels: True\n",
            "  ✅ No major data quality issues\n",
            "\n",
            "📊 [HSBC] Data Quality Assessment\n",
            "----------------------------------------\n",
            "  q1_2025: 49 records, 10.2% missing data\n",
            "  q2_2025: 30 records, 14.5% missing data\n",
            "  Manual labels: 69 labeled records\n",
            "  Quarters loaded: 2/2\n",
            "  Total records: 79\n",
            "  Has manual labels: True\n",
            "  ✅ No major data quality issues\n",
            "\n",
            "📈 Overall Statistics:\n",
            "  Total records: 340\n",
            "  Total memory: 0.50 MB\n",
            "  Banks with all quarters: 2/2\n",
            "  Banks with manual labels: 2/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Enhanced Raw Datasets\n",
        "\n",
        "def save_enhanced_datasets():\n",
        "    \"\"\"Save all loaded datasets with enhanced organization.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SAVING ENHANCED DATASETS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    saved_files = {}\n",
        "\n",
        "    # Save individual quarterly datasets\n",
        "    for bank in BANKS:\n",
        "        print(f\"\\n💾 Saving {bank.upper()} datasets...\")\n",
        "        saved_files[bank] = {}\n",
        "\n",
        "        if bank in loaded_datasets:\n",
        "            bank_data = loaded_datasets[bank]\n",
        "\n",
        "            # Save quarterly datasets\n",
        "            for quarter in QUARTERS:\n",
        "                if quarter in bank_data and bank_data[quarter] is not None:\n",
        "                    df = bank_data[quarter]\n",
        "\n",
        "                    # Enhanced filename\n",
        "                    filename = f\"raw_{bank}_{quarter}_earnings_call.csv\"\n",
        "\n",
        "                    # Save to both locations\n",
        "                    drive_path = drive_base / f\"data/raw/{bank}\" / filename\n",
        "                    colab_path = colab_base / f\"data/raw/{bank}\" / filename\n",
        "\n",
        "                    df.to_csv(drive_path, index=False)\n",
        "                    df.to_csv(colab_path, index=False)\n",
        "\n",
        "                    saved_files[bank][quarter] = {\n",
        "                        \"drive_path\": str(drive_path),\n",
        "                        \"colab_path\": str(colab_path),\n",
        "                        \"shape\": df.shape\n",
        "                    }\n",
        "\n",
        "                    print(f\"  ✅ {quarter}: {filename} ({df.shape})\")\n",
        "\n",
        "            # Save manual labels\n",
        "            if \"manual_labels\" in bank_data and bank_data[\"manual_labels\"] is not None:\n",
        "                df = bank_data[\"manual_labels\"]\n",
        "                filename = f\"raw_{bank}_manual_labels.csv\"\n",
        "\n",
        "                drive_path = drive_base / f\"data/raw/{bank}\" / filename\n",
        "                colab_path = colab_base / f\"data/raw/{bank}\" / filename\n",
        "\n",
        "                df.to_csv(drive_path, index=False)\n",
        "                df.to_csv(colab_path, index=False)\n",
        "\n",
        "                saved_files[bank][\"manual_labels\"] = {\n",
        "                    \"drive_path\": str(drive_path),\n",
        "                    \"colab_path\": str(colab_path),\n",
        "                    \"shape\": df.shape\n",
        "                }\n",
        "\n",
        "                print(f\"  ✅ Manual labels: {filename} ({df.shape})\")\n",
        "\n",
        "    # Save combined datasets\n",
        "    print(f\"\\n💾 Saving combined datasets...\")\n",
        "    for bank in BANKS:\n",
        "        if bank in combined_datasets:\n",
        "            df = combined_datasets[bank]\n",
        "            filename = f\"raw_{bank}_multi_2025_earnings_call.csv\"\n",
        "\n",
        "            drive_path = drive_base / f\"data/raw/{bank}\" / filename\n",
        "            colab_path = colab_base / f\"data/raw/{bank}\" / filename\n",
        "\n",
        "            df.to_csv(drive_path, index=False)\n",
        "            df.to_csv(colab_path, index=False)\n",
        "\n",
        "            if \"combined\" not in saved_files[bank]:\n",
        "                saved_files[bank][\"combined\"] = {}\n",
        "\n",
        "            saved_files[bank][\"combined\"] = {\n",
        "                \"drive_path\": str(drive_path),\n",
        "                \"colab_path\": str(colab_path),\n",
        "                \"shape\": df.shape\n",
        "            }\n",
        "\n",
        "            print(f\"  ✅ {bank.upper()} combined: {filename} ({df.shape})\")\n",
        "\n",
        "    return saved_files\n",
        "\n",
        "# Save all datasets\n",
        "saved_files = save_enhanced_datasets()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abNA5iHN8doq",
        "outputId": "da8e3e07-f529-4433-ea6d-9d54056dd965"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING ENHANCED DATASETS\n",
            "============================================================\n",
            "\n",
            "💾 Saving JPM datasets...\n",
            "  ✅ q1_2025: raw_jpm_q1_2025_earnings_call.csv ((112, 10))\n",
            "  ✅ q2_2025: raw_jpm_q2_2025_earnings_call.csv ((149, 10))\n",
            "  ✅ Manual labels: raw_jpm_manual_labels.csv ((1121, 27))\n",
            "\n",
            "💾 Saving HSBC datasets...\n",
            "  ✅ q1_2025: raw_hsbc_q1_2025_earnings_call.csv ((49, 11))\n",
            "  ✅ q2_2025: raw_hsbc_q2_2025_earnings_call.csv ((30, 11))\n",
            "  ✅ Manual labels: raw_hsbc_manual_labels.csv ((858, 27))\n",
            "\n",
            "💾 Saving combined datasets...\n",
            "  ✅ JPM combined: raw_jpm_multi_2025_earnings_call.csv ((261, 11))\n",
            "  ✅ HSBC combined: raw_hsbc_multi_2025_earnings_call.csv ((79, 12))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Enhanced Data Registry\n",
        "\n",
        "enhanced_data_registry = {\n",
        "    \"creation_timestamp\": pd.Timestamp.now().isoformat(),\n",
        "    \"banks\": BANKS,\n",
        "    \"quarters\": QUARTERS,\n",
        "    \"models_configured\": list(MODELS.keys()),\n",
        "    \"saved_files\": saved_files,\n",
        "    \"quality_report\": quality_report,\n",
        "    \"data_urls\": data_urls,\n",
        "    \"loading_summary\": {\n",
        "        \"total_files_saved\": sum(len(bank_files) for bank_files in saved_files.values()),\n",
        "        \"banks_successfully_loaded\": len([bank for bank in BANKS if bank in saved_files and saved_files[bank]]),\n",
        "        \"quarters_loaded_per_bank\": {bank: len([q for q in QUARTERS if q in saved_files.get(bank, {})])\n",
        "                                    for bank in BANKS}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save enhanced registry\n",
        "registry_path = drive_base / \"configs\" / \"enhanced_data_registry.json\"\n",
        "with open(registry_path, \"w\") as f:\n",
        "    json.dump(enhanced_data_registry, f, indent=2, default=str)\n",
        "\n",
        "print(f\"✅ Enhanced data registry saved: {registry_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKhNA78y8iOc",
        "outputId": "ec7b0cd1-05ab-4824-a0ed-de6dc9dd0c4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enhanced data registry saved: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/configs/enhanced_data_registry.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Final Summary\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ENHANCED DATA LOADING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Summary statistics\n",
        "total_files = sum(len(bank_files) for bank_files in saved_files.values())\n",
        "successful_banks = len([bank for bank in BANKS if bank in saved_files and saved_files[bank]])\n",
        "total_records = quality_report[\"overall_statistics\"][\"total_records\"]\n",
        "total_memory = quality_report[\"overall_statistics\"][\"total_memory_mb\"]\n",
        "\n",
        "print(f\"📊 Loading Summary:\")\n",
        "print(f\"  Banks processed: {successful_banks}/{len(BANKS)}\")\n",
        "print(f\"  Files saved: {total_files}\")\n",
        "print(f\"  Total records: {total_records:,}\")\n",
        "print(f\"  Total memory: {total_memory:.2f} MB\")\n",
        "\n",
        "print(f\"\\n🏦 Bank-specific Summary:\")\n",
        "for bank in BANKS:\n",
        "    if bank in saved_files and saved_files[bank]:\n",
        "        bank_quarters = len([q for q in QUARTERS if q in saved_files[bank]])\n",
        "        has_manual = \"manual_labels\" in saved_files[bank]\n",
        "        has_combined = \"combined\" in saved_files[bank]\n",
        "\n",
        "        print(f\"  {bank.upper()}:\")\n",
        "        print(f\"    Quarters: {bank_quarters}/{len(QUARTERS)}\")\n",
        "        print(f\"    Manual labels: {'✅' if has_manual else '❌'}\")\n",
        "        print(f\"    Combined dataset: {'✅' if has_combined else '❌'}\")\n",
        "\n",
        "print(f\"\\n🚀 Ready for next step: 03_clean_preprocess_enhanced.ipynb\")\n",
        "print(f\"   Enhanced datasets loaded for {len(BANKS)} banks\")\n",
        "print(f\"   {len(MODELS)} models configured for analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iIR17r08nzJ",
        "outputId": "db7b1141-2162-4f01-a159-2bef0e64dae7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED DATA LOADING COMPLETE\n",
            "============================================================\n",
            "📊 Loading Summary:\n",
            "  Banks processed: 2/2\n",
            "  Files saved: 8\n",
            "  Total records: 340\n",
            "  Total memory: 0.50 MB\n",
            "\n",
            "🏦 Bank-specific Summary:\n",
            "  JPM:\n",
            "    Quarters: 2/2\n",
            "    Manual labels: ✅\n",
            "    Combined dataset: ✅\n",
            "  HSBC:\n",
            "    Quarters: 2/2\n",
            "    Manual labels: ✅\n",
            "    Combined dataset: ✅\n",
            "\n",
            "🚀 Ready for next step: 03_clean_preprocess_enhanced.ipynb\n",
            "   Enhanced datasets loaded for 2 banks\n",
            "   4 models configured for analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nxo6JihL8r1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}