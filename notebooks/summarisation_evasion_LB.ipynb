{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b820ac6",
   "metadata": {},
   "source": [
    "# **Summarisation & Evasion Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e2962",
   "metadata": {},
   "source": [
    "# **Handover Notes:** [delete after]\n",
    "- Library imports and versions are saved in environments/summarisation_evasion_env.txt\n",
    "- This notebook was originally built for a macbook pro M3 chip so some settings may need to be altered depending on your machine\n",
    "- All files related/ generated by this notebook can be found in notebooks/summarisation_evasion_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b76a8",
   "metadata": {},
   "source": [
    "### **Work progress**\n",
    "1. **Complete**\n",
    "- Summarise banker answers using baseline model.\n",
    "- Use Local RAG pipeline to bring in relevant external documents (PRA risk definitions) to create PRA aligned summaries.\n",
    "- Developed a evasion detection prototype that generates evasion scores based on bankers answers (uses baseline model, LLM- natural language inference using RoBERTa and a blended score)\n",
    "- Used jpm_2025 transcripts to get the pipeline working. Validated the evasion pipeline using jpm-23-1q data (involved human labelling the answer as Direct or Evasive- file saved in notebooks/summarisation_evasion_files).\n",
    "\n",
    "2. **Not complete**\n",
    "- Visualisations e.g. how many evasive answers were there? etc - apply evasion pipeline to dataset and generate statistics on evasiveness \n",
    "- Need to test pipleine on larger data set (e.g. jpm 2023-2025) and check against HSBC to make conclusions & comment on generalisability (answering research question: How does one bank’s tone and thematic profile compare to peers? Are divergences systemic or firm specific?)\n",
    "- Summarisation pipeline could be improved using a two-stage pipeline: by first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned.\n",
    "- Post-processing on the output file for the PRA aligned summaries by Mistral model so they are clearer- can this output be fed into another model to extract more insights/ detect evasion or risk?\n",
    "- Increase the size of the validation set for the evasion pipeline prototype (e.g. more human labelling)\n",
    "- Need to fine tune the evasion pipeline to increase accuracy\n",
    "- Optional extensions e.g. using Agents, more complex RAG pipeline (including more useful context for the model), validation of instances of evasion using external news sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e874e23",
   "metadata": {},
   "source": [
    "# 1. **Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670282",
   "metadata": {},
   "source": [
    "# **2. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5f5884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "import math\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set SEED.\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91bb8c",
   "metadata": {},
   "source": [
    "# **3. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "743f6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "jpm_2025_df = pd.read_csv('../data/processed/jpm/all_jpm_2025.csv')\n",
    "\n",
    "# View the data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75628c",
   "metadata": {},
   "source": [
    "# **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b70dc",
   "metadata": {},
   "source": [
    "- Used all_jpm_2025.csv dataset\n",
    "- Preliminary preprocessing to label roles as analyst vs banker (invalid roles were corrected) to make downstream analysis easier. Created a new column 'role_normalised'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc49b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec70c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_number  answer_number                            speaker_name  \\\n",
       "201               35            5.0  Chief Financial Officer, JPMorganChase   \n",
       "205               36            3.0  Chief Financial Officer, JPMorganChase   \n",
       "\n",
       "                                            role             company  \\\n",
       "201  And then some. Theres a lot of value added.       JPMorganChase   \n",
       "205                                         Okay  there you have it.   \n",
       "\n",
       "                                               content  year quarter  \\\n",
       "201  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "205  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "\n",
       "                                            source_pdf  \n",
       "201  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "205  data/raw/jpm/jpm-2q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles.\n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = jpm_2025_df[~jpm_2025_df['role'].isin(valid_roles)]\n",
    "\n",
    "# Number of rows with invalid roles.\n",
    "print('Number of rows:', invalid_roles_df.shape[0])\n",
    "\n",
    "# View the rows.\n",
    "invalid_roles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e31cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the correct role information.\n",
    "jpm_2025_df.at[205, 'role'] = 'Chief Financial Officer'\n",
    "jpm_2025_df.at[209, 'role'] = 'Chief Financial Officer'\n",
    "\n",
    "# Verify the roles have been updated.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e44043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_2025_df['role_normalised'] = jpm_2025_df['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "755f26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf565",
   "metadata": {},
   "source": [
    "# **5. Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401764b",
   "metadata": {},
   "source": [
    "## **5.1 Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee15b",
   "metadata": {},
   "source": [
    "- Initial model exploration using BART and mistral-7B-instruct to summarise banker's answers (no additional context given to model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92395bc",
   "metadata": {},
   "source": [
    "### **5.1.1 BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "671f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the w\n"
     ]
    }
   ],
   "source": [
    "# Filter data to banker answers only.\n",
    "banker_answers = jpm_2025_df[jpm_2025_df['role_normalised'] == 'banker']['content'].tolist()\n",
    "print(banker_answers[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54559054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The main thing that we see there, what would appear to be a certain amount of frontloading of spending ahead of people expecting price increases from tariffs. So ironically, that's actually somewhat supportive, all else equal. In terms of our corporate clients, obviously, they've been reacting to the changes in tariff policy.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (BART)\n",
    "bart = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "sample_text = banker_answers[0]\n",
    "summary_bart = bart(sample_text, max_length=80, min_length=30, do_sample=False)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f0d9b",
   "metadata": {},
   "source": [
    "- bart was able to extract ket ideas, focussing on fronloading of spending and tariff policy. \n",
    "- Compressed the response into two sentences and the summary is coherent, removing filler phrases.\n",
    "- However, the summary is not fully neutral (e.g. includes ironically) and preserves tone\n",
    "- Also there is a loss of context- e.g. consumer side vs wholesale side distinction is no longer explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "024bd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: Corporates are taking a wait-and-see approach to tariff policy. Some sectors are going to be much more exposed than others. Small business and smaller corporates are probably a little more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Prompt conditioning to make PRA relevant.\n",
    "prompt = \"Summarise this answer, focusing on risk, capital and evasion of detail: \" + sample_text\n",
    "summary_bart_prompted = bart(prompt, max_length=80, min_length=30)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart_prompted[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e7996",
   "metadata": {},
   "source": [
    "- Prompted summary shifts emphasis and includes interpretation around risk, even though those words were no explicit in the original\n",
    "- This version is more aligned to evasion detection but moves away from concrete detail \n",
    "- Improved approach would be to have a two stage-pipeline: first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaabd6",
   "metadata": {},
   "source": [
    "### **5.1.2 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f4b90",
   "metadata": {},
   "source": [
    "- Mistral model: mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Mistral-7B-Instruct model download: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF?show_file_info=mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Also saved in shared team folder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a368c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides. On the consumer side, there has been some frontloading of spending ahead of expected price increases from tariffs, which may distort the data and make it difficult to draw larger conclusions. On the corporate side, clients are reacting to changes in tariff policy by shifting their focus towards short-term work and optimizing supply chains. The speaker characterizes the attitude of corporate clients as a wait-and-see attitude, with smaller clients and smaller corporates being more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with basic prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral = output['choices'][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec467d",
   "metadata": {},
   "source": [
    "- Preserves details and nuance and is more contextual and interpretive than the BART baseline model.\n",
    "- However, the result is longer with heavier phrasing and includes phrases like 'distort the data' which is not explicit in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97b0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides of their business. On the consumer side, they have observed some frontloading of spending ahead of expected price increases from tariffs, which may distort data and make it difficult to draw larger conclusions. On the corporate side, clients are shifting their focus towards optimizing supply chains and responding to the current environment, rather than prioritizing more strategic work. The speaker notes that smaller clients and smaller corporates may be more challenged than larger ones, which have more experience dealing with these types of changes and more resources to manage them. Overall, the speaker suggests that it is difficult to make long-term decisions at this time due to the uncertainty surrounding the current environment.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with more detailed prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions. Focus on risk, capital and evasion of detail.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral_prompted = output['choices'][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral_prompted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1e33d",
   "metadata": {},
   "source": [
    "- This summary brings in risk- language and is closer to the task objective.\n",
    "- However, some interpretations are generated by the model rather than explicitly detailed in the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813acc2",
   "metadata": {},
   "source": [
    "## **5.2 Adding Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1b7a",
   "metadata": {},
   "source": [
    "Retrieve PRA risk categories to give greater PRA focus to summaries (local RAG loop).\n",
    "- measure cosine similarity between transcript chunks and PRA risk categories (vectors)\n",
    "- retrieve the top 2-3 most relevant risk categories \n",
    "- prepend them to the summarisation prompt to make summaries PRA-aligned instead of just summarised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2bd1",
   "metadata": {},
   "source": [
    "- Attempting to use BART resulted in prompt echoing.\n",
    "- New attempt using Mistral-7B-Instruct.\n",
    "- Using sentence-BERT vs TF-IDF for vectorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9dab",
   "metadata": {},
   "source": [
    "### **5.2.1 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56464c24",
   "metadata": {},
   "source": [
    "**Process**\n",
    "- Performed some light cleaning of the transcript to remove whitespace.\n",
    "- Split the transcript into smaller chunks that the model can summarise to avoid truncation\n",
    "- Loaded the PRA categories csv file (contains category and definition)\n",
    "- Embedded the PRA categories and chunks, evaluated the similarity to extract the PRA risk categories that were relevant to the text\n",
    "- Summarised the chunk using detailed prompted and relevant PRA categories as additional context. \n",
    "\n",
    "**Output File**:\n",
    "- The output file of this can be found in notebooks/summarisation_evasion_files, name = jpm_mistral_pra_summary.json\n",
    "- It is in the format: summary, evidence, PRA category that relates to summary and reasoning for selecting these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e695d9e",
   "metadata": {},
   "source": [
    "- Needed to use a lot of fine tuning for the prompt and set strict rules for the model\n",
    "- Need to be very clear about the output expected or else the model deviates a lot, especially as it processes more data.\n",
    "- Include lines about lack of evidence if not the model may hallucinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b05ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove whitespace in text.\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4f9ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the transcript into smaller chunks.\n",
    "def chunk_text(text: str, max_chars: int = 6000):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split into sentences \n",
    "    chunks, current_chunk, current_len = [], [], 0 # list of chunks, sentences collecting for current chunk, character count for current chunk\n",
    "\n",
    "    for s in sentences:\n",
    "        if current_len + len(s) + 1 <= max_chars: # if the characters of current chunk + new sentence is below the limit:\n",
    "            current_chunk.append(s) # add sentence to current chunk \n",
    "            current_len += len(s) + 1 # update running character count \n",
    "        \n",
    "        else: # if the characters is above the limit:\n",
    "            chunks.append(' '.join(current_chunk)) # add the current chunk to the final chunk list\n",
    "            current_chunk, current_len = [s], len(s) # start a new chunk containing the sentence and update current len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk)) # add any sentences in current chunk after loop ends \n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fe2b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PRA categories and definitions from CSV.\n",
    "def load_pra_categories(path: Path):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            (row.get('category', '').strip(), [row.get('definition', '').strip()])\n",
    "            for row in reader if row.get('category')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db60595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sentence-BERT embedding index for PRA categories.\n",
    "def build_embedding_index(pra_categories):\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    docs = [f\"{name} {' '.join(defs)}\" for name, defs in pra_categories]\n",
    "    pra_risk_embeddings = embedder.encode(docs, batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "    return embedder, np.asarray(pra_risk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the relevant PRA categories to the transcript chunks.\n",
    "def find_rel_categories(chunk, pra_categories, embedder, pra_risk_embeddings, top_k=2):\n",
    "    query_vec = embedder.encode([chunk], normalize_embeddings=True) # turns chunk into embedding\n",
    "    sims = cosine_similarity(query_vec, pra_risk_embeddings).ravel() # compares the chunk to each category doc \n",
    "    top_indices = np.argsort(-sims)[:top_k] # sorts scores descending and selected top k cateogories \n",
    "\n",
    "    return [pra_categories[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df8f1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON\n",
    "def parse_tagged_json(raw):\n",
    "    m = re.search(r\"<json>\\s*(\\{[\\s\\S]*?\\})\\s*</json>\", raw, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d1e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise the text chunks.\n",
    "def summarise_chunk(model, chunk, relevant_categories, max_evidence=5):\n",
    "\n",
    "    # Build PRA notes (limit to 2 bullets per category)\n",
    "    lines = []\n",
    "    for name, definition in relevant_categories:\n",
    "        lines.append(f'- {name}:')\n",
    "        for d in list(definition)[:2]:\n",
    "            lines.append(f'- {d}')\n",
    "    notes_block = '\\n'.join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a careful data extraction model. \"\n",
    "        \"Return ONLY valid JSON wrapped in <json>...</json> tags.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "TRANSCRIPT:\n",
    "{chunk}\n",
    "\n",
    "PRA NOTES:\n",
    "{notes_block}\n",
    "\n",
    "TASK:\n",
    "Return JSON ONLY, wrapped exactly like this:\n",
    "<json>{{\"summary\": \"...\", \"evidence\": [\"...\"], \"pra_categories\": [{{\"category\":\"...\",\"why\":\"...\"}}]}}</json>\n",
    "\n",
    "RULES:\n",
    "- 4-6 sentence neutral summary.\n",
    "- Up to {max_evidence} evidence bullets (quotes/facts).\n",
    "- 1-3 pra_categories objects.\n",
    "- If evidence is lacking, use a single bullet \"Insufficient evidence\".\n",
    "- Only choose categories supported by the evidence.\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = model.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=700,\n",
    "        repeat_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    raw = (response['choices'][0]['message']['content'] or '').strip()\n",
    "\n",
    "    # Parse the tagged JSON\n",
    "    parsed = parse_tagged_json(raw)\n",
    "\n",
    "    # Fallback if model didn’t follow instructions\n",
    "    if not parsed:\n",
    "        return (\n",
    "            {'summary': '', 'evidence': ['Insufficient evidence'], 'pra_categories': []},\n",
    "            raw,\n",
    "        )\n",
    "\n",
    "    # Light coercion to guarantee keys exist\n",
    "    result = {\n",
    "        'summary': parsed.get('summary', '') or '',\n",
    "        'evidence': parsed.get('evidence', []) or [],\n",
    "        'pra_categories': parsed.get('pra_categories', []) or []\n",
    "    }\n",
    "    return result, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e398a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables.\n",
    "MODEL_PATH = '/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "PRA_NOTES_PATH = '../data/RAG-resources/PRA_risk_categories.csv'\n",
    "TRANSCRIPT_PATH = '../data/processed/jpm/all_jpm_2025.csv'\n",
    "OUTPUT_PATH = pathlib.Path('../notebooks/summarisation_evasion_files/jpm_mistral_pra_summary_raw.json')\n",
    "TOP_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c63a4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 3559 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 110 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2495.33 MiB, ( 9858.70 / 10922.67)\n",
      "load_tensors: offloading 20 repeating layers to GPU\n",
      "load_tensors: offloaded 20/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  2495.33 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x4056e9470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x432543670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x432543970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x432543cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x40506eae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x40506ff70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x4056ea0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x40040aae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x4056e88d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x4321fe1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x4050702f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x4050706c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x405070a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x405070e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x4325440a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x432544500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x4325448e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x4321fc6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x4321f4930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x4050711c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x4056e8b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x432544cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x4056e8e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x432545080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x432545450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x405071590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x4050719f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x4321f2170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x4321ed620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x4321d5da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x405071d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x4050721f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x4321d6060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x4056ea770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x4321d6320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x4056eaa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x4056eae90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x405072550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x4321d65e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x4321d68a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x4050728d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x405072d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x432545a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x4056eb230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x405073090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x4056eb600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x432545e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x432546150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x432546410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x4056eb960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x4056ebdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x4056ec120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x432546720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x4056ec580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x432546b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x4050734f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x405073850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x405073cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x405074050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x4321d6b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x432546f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x4056ec950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x4050743b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x4056eccb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x4056ed110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x4056ed4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x4325472c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x432547720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x432547b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x405074780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x432547e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x405074b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x405074fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x432548200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x432548660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x432548a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x432548e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x405075310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x432549340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x4056ed840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x4050756e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x405075b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x4056edc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x4056ee070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x4056ee440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x4056ee810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x4056eebe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x405075ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x432549600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x4321d6e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x405076300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x432549910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x4050766d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x405076aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x4056eef40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x432549ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x4321d70e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x4056ef3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x43254a180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x405076e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x43254a500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x43254a910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x4056ef700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x4321d73a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x4050771d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x43254ace0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x4056efb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x4056eff30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x43254b0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x4056f0470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x4050775a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x4321d7660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x405077970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x405077dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x4050781a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x4056f0730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x4321d7920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x405078500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x43254b410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x4050788d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x405078d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x4056f0af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x43254b800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x4056f0e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x43254bbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x4321d7be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x4056f11d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x4056f1630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x405079090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x4050794f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x4321d7ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x4321d8160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x4050798c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x405079c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x40507a060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x40507a430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x4321d8420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x4321d86e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x4321d89a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x4321d8c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x4321d8f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x43254c210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x43254c4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x4056f1990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x4321d91e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x4056f1d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x43254c7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x43254cc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x4321d94a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x40507a790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x4321d9760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x4321d9a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x40507abf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x43254d210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x43254d740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x43254da00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x4056f2130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x4056f2590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x4321d9ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x4321d9fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x4056f2960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x40507af50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x4321da260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x4056f2cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x4056f3090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x4321da520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x40507b620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x43254ded0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x40507ba80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x40507bde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x4056f3460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x4056f38c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x4056f3c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x43254e270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x43254e680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x43254eae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x43254eef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x4056f41d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x4056f4490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x4056f4ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x4056f5290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x4056f5b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x40507c240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x43254f450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x4321da7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x4321daaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x4321dad60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x4321db020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x4321db2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x4321db5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x40507c5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x40507ca00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x4056f5e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x43254f710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x43254f9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x4321db860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x40507cd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x40507d1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x40507d590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x40507d960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x40507dcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x43254fe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x4325500f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x40507e120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x40507e4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x4321dbb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x40507ea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x4056f62f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x40507ecf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x4056f66c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x4056f6a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x4321dbde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x4056f6e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x4321dc0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x4321dc360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x432550550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x40507eff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x432550810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x40507f3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x4056f7230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x4056f7600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x432550c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x432550f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x4056f79d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x4056f7da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x4056f8170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x4056f8540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x4056f8910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x4325511f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x4325514f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x4056f8c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x4056f90d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x40507f790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x432551850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x40507fbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x40507ffc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x432551c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x4321dc620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x4321dc8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x4056f9530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x4056f9890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x4056f9c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x4056fa000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x4056fa3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x4056fa7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x4056fab70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x4056faf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x432551fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x432552460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x432552800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x405080320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x4056fb310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x432552b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x432552f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x4325533a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x432553770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x4056fb6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x432553b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x405080780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x4056fbab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x405080b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x432553ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x432554320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x4321dcba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x405080fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x4056fbe80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x405081410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x4321dce60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x4321dd120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x4321dd3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x4056fc1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x432554780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x4321dd6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x432554d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x4056fc5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x4325551d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x432555690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x432555aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x4056fc980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4056fcde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x432555e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x4321dd960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x4321ddc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x4321ddee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x4321de1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4321de460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x4321de720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x4321de9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x4321deca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x4321def60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x405081770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x432556100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x405081ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x4056fd140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x4056fd5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x4056fd970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x4325563c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x4321df220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x432556680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x4321df4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x4321df7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x432556a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x432556dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x4325571d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x432557810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x432557ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x4321dfa60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x432557d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x405081ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x4056fdcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x4321dfd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x405082320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x4056fe130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x4056fe500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x405082680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x405082ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x4056fe8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x432558430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x4325586f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x405082eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x4056fed30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x432558ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x432559050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x432559310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x4056ff190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x4321dffe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x4050833f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x4321e02a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x4321e0560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x432559720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x4325599e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x4050836b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x405083a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x405083e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x432559ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x432559fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x43255a390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x405084180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x43255a760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x4050845e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x4050849b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x4056ff630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x4056ff8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x4321e0820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x4321e0ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x4056ffd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x43255aac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x405084d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x43255af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x43255b570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x4321e0da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x4057041d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x4321e1060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x405704490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x4321e1320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x4050850d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x4057048a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x4321e15e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x405704c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x4321f0400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x4321fbd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x405705010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x405085ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x405085f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x405086330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x405086700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x405086ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x405086e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x4050873f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x405705370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x405087920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x405087be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x405087ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x4321e1910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x4050881f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x4050885c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x43255bad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x43255bd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x4321e1bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x43255c050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4321e1e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x405705860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x405705b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x405705f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x4321e2150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x4321e2410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x405706290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x4057066f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x405706ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x405706e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x43255c310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x43255c760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x43255cab0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   300.01 MiB\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 171 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   19269.80 ms /  1764 tokens (   10.92 ms per token,    91.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18958.85 ms /   279 runs   (   67.95 ms per token,    14.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   38302.21 ms /  2043 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1707 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   17957.47 ms /  1707 tokens (   10.52 ms per token,    95.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22157.87 ms /   343 runs   (   64.60 ms per token,    15.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   40210.43 ms /  2050 tokens\n",
      "llama_perf_context_print:    graphs reused =        331\n",
      "Llama.generate: 11 prefix-match hit, remaining 1996 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20953.05 ms /  1996 tokens (   10.50 ms per token,    95.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27714.58 ms /   414 runs   (   66.94 ms per token,    14.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   48791.33 ms /  2410 tokens\n",
      "llama_perf_context_print:    graphs reused =        400\n",
      "Llama.generate: 11 prefix-match hit, remaining 1923 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20506.85 ms /  1923 tokens (   10.66 ms per token,    93.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26804.17 ms /   373 runs   (   71.86 ms per token,    13.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   47425.25 ms /  2296 tokens\n",
      "llama_perf_context_print:    graphs reused =        360\n",
      "Llama.generate: 11 prefix-match hit, remaining 1799 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   21587.09 ms /  1799 tokens (   12.00 ms per token,    83.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24261.28 ms /   352 runs   (   68.92 ms per token,    14.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   45948.70 ms /  2151 tokens\n",
      "llama_perf_context_print:    graphs reused =        340\n",
      "Llama.generate: 11 prefix-match hit, remaining 2023 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   23097.64 ms /  2023 tokens (   11.42 ms per token,    87.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35135.03 ms /   484 runs   (   72.59 ms per token,    13.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   58396.71 ms /  2507 tokens\n",
      "llama_perf_context_print:    graphs reused =        468\n",
      "Llama.generate: 12 prefix-match hit, remaining 2044 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   21860.78 ms /  2044 tokens (   10.70 ms per token,    93.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34572.03 ms /   487 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   56596.64 ms /  2531 tokens\n",
      "llama_perf_context_print:    graphs reused =        471\n",
      "Llama.generate: 12 prefix-match hit, remaining 1918 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20716.98 ms /  1918 tokens (   10.80 ms per token,    92.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26811.18 ms /   385 runs   (   69.64 ms per token,    14.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   47645.59 ms /  2303 tokens\n",
      "llama_perf_context_print:    graphs reused =        372\n",
      "Llama.generate: 11 prefix-match hit, remaining 1770 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   18933.24 ms /  1770 tokens (   10.70 ms per token,    93.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27239.46 ms /   399 runs   (   68.27 ms per token,    14.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   46295.76 ms /  2169 tokens\n",
      "llama_perf_context_print:    graphs reused =        385\n",
      "Llama.generate: 11 prefix-match hit, remaining 1715 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   19641.99 ms /  1715 tokens (   11.45 ms per token,    87.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22144.01 ms /   319 runs   (   69.42 ms per token,    14.41 tokens per second)\n",
      "llama_perf_context_print:       total time =   41875.52 ms /  2034 tokens\n",
      "llama_perf_context_print:    graphs reused =        308\n",
      "Llama.generate: 11 prefix-match hit, remaining 1789 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20013.07 ms /  1789 tokens (   11.19 ms per token,    89.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21421.18 ms /   283 runs   (   75.69 ms per token,    13.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   41512.70 ms /  2072 tokens\n",
      "llama_perf_context_print:    graphs reused =        273\n",
      "Llama.generate: 11 prefix-match hit, remaining 1959 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20493.60 ms /  1959 tokens (   10.46 ms per token,    95.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49604.08 ms /   699 runs   (   70.96 ms per token,    14.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   70378.63 ms /  2658 tokens\n",
      "llama_perf_context_print:    graphs reused =        676\n",
      "Llama.generate: 11 prefix-match hit, remaining 1748 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   19429.52 ms /  1748 tokens (   11.12 ms per token,    89.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19707.40 ms /   301 runs   (   65.47 ms per token,    15.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   39213.42 ms /  2049 tokens\n",
      "llama_perf_context_print:    graphs reused =        290\n",
      "Llama.generate: 11 prefix-match hit, remaining 1856 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20163.49 ms /  1856 tokens (   10.86 ms per token,    92.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23275.70 ms /   334 runs   (   69.69 ms per token,    14.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   43534.53 ms /  2190 tokens\n",
      "llama_perf_context_print:    graphs reused =        323\n",
      "Llama.generate: 11 prefix-match hit, remaining 1881 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20354.72 ms /  1881 tokens (   10.82 ms per token,    92.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18753.27 ms /   279 runs   (   67.22 ms per token,    14.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   39176.07 ms /  2160 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1938 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20213.52 ms /  1938 tokens (   10.43 ms per token,    95.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48581.55 ms /   699 runs   (   69.50 ms per token,    14.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   69075.20 ms /  2637 tokens\n",
      "llama_perf_context_print:    graphs reused =        676\n",
      "Llama.generate: 11 prefix-match hit, remaining 1842 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   19190.58 ms /  1842 tokens (   10.42 ms per token,    95.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21441.84 ms /   313 runs   (   68.50 ms per token,    14.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   40715.53 ms /  2155 tokens\n",
      "llama_perf_context_print:    graphs reused =        302\n",
      "Llama.generate: 11 prefix-match hit, remaining 1898 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   20189.74 ms /  1898 tokens (   10.64 ms per token,    94.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21461.20 ms /   317 runs   (   67.70 ms per token,    14.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   41735.39 ms /  2215 tokens\n",
      "llama_perf_context_print:    graphs reused =        306\n",
      "Llama.generate: 11 prefix-match hit, remaining 1983 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   21165.86 ms /  1983 tokens (   10.67 ms per token,    93.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25565.65 ms /   364 runs   (   70.24 ms per token,    14.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   46837.73 ms /  2347 tokens\n",
      "llama_perf_context_print:    graphs reused =        352\n",
      "Llama.generate: 11 prefix-match hit, remaining 2050 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   22197.49 ms /  2050 tokens (   10.83 ms per token,    92.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26673.28 ms /   394 runs   (   67.70 ms per token,    14.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   48985.00 ms /  2444 tokens\n",
      "llama_perf_context_print:    graphs reused =        381\n",
      "Llama.generate: 11 prefix-match hit, remaining 1804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   19597.00 ms /  1804 tokens (   10.86 ms per token,    92.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27448.03 ms /   382 runs   (   71.85 ms per token,    13.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   47160.93 ms /  2186 tokens\n",
      "llama_perf_context_print:    graphs reused =        369\n",
      "Llama.generate: 11 prefix-match hit, remaining 2062 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =   22623.87 ms /  2062 tokens (   10.97 ms per token,    91.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29660.49 ms /   372 runs   (   79.73 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   52405.09 ms /  2434 tokens\n",
      "llama_perf_context_print:    graphs reused =        359\n",
      "Llama.generate: 11 prefix-match hit, remaining 853 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   19270.79 ms\n",
      "llama_perf_context_print: prompt eval time =    8869.33 ms /   853 tokens (   10.40 ms per token,    96.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16408.34 ms /   254 runs   (   64.60 ms per token,    15.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   25344.95 ms /  1107 tokens\n",
      "llama_perf_context_print:    graphs reused =        246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote final JSON to: /Users/laurenbrixey/Documents/GitHub Repositories/cam_ds_ep_FinSight/notebooks/summarisation_evasion_files/jpm_mistral_pra_summary_raw.json\n"
     ]
    }
   ],
   "source": [
    "# Runner code.\n",
    "pra_categories = load_pra_categories(Path(PRA_NOTES_PATH))\n",
    "embedder, category_embeddings = build_embedding_index(pra_categories)\n",
    "\n",
    "# Load and chunk transcript\n",
    "transcript_text = Path(TRANSCRIPT_PATH).read_text(encoding='utf-8')\n",
    "transcript_chunks = chunk_text(transcript_text)\n",
    "\n",
    "n_threads = max(4, (os.cpu_count() or 8) - 2)\n",
    "\n",
    "# Define the model.\n",
    "model = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=20,\n",
    "    chat_format='mistral-instruct',\n",
    "    n_threads=n_threads,\n",
    ")\n",
    "\n",
    "raw_outputs = []\n",
    "\n",
    "for i, chunk in enumerate(transcript_chunks, 1):\n",
    "    try:\n",
    "        top_categories = find_rel_categories(\n",
    "            chunk, pra_categories, embedder, category_embeddings, top_k=TOP_K\n",
    "        )\n",
    "        _, raw = summarise_chunk(\n",
    "            model, chunk, top_categories, max_evidence=5\n",
    "        )\n",
    "        raw_outputs.append({'chunk': i, 'raw': raw})\n",
    "\n",
    "    except Exception:\n",
    "        raw_outputs.append({'chunk': i, 'raw': ''})\n",
    "\n",
    "final_output = {'raw_outputs': raw_outputs}\n",
    "\n",
    "OUTPUT_PATH.write_text(json.dumps(final_output, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f'Wrote final JSON to: {OUTPUT_PATH.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ff6d7",
   "metadata": {},
   "source": [
    "- Need to preprocess the output so it is visually clearer (summary, evidence, PRA categories (name & why the model chose this))\n",
    "- Can this information be fed to the model again and can it detect any early PRA risk indicators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9d93f",
   "metadata": {},
   "source": [
    "# **6. Evasion Detection Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031e1b4",
   "metadata": {},
   "source": [
    "1. **Baseline Evasion score** (rule-based) is made up of three components:\n",
    "- **Cosine similarity**- similarity of the question and answer, lower similarity = more evasive\n",
    "- **Numeric specificity check**- does the question require a number, if so does the answer contain a number?, e.g. requests for financial data\n",
    "- **Evasive phrases**- does the answer contain evasive phrases?, presence = more evasive\n",
    "\n",
    "2. **LLM evasion score** (RoBERTa-MNLI) uses entailment/neutral/contradiction between the question and answer\n",
    "- Lower entailment (and higher neutral + contradiction) = more evasive\n",
    "  \n",
    "3. **Blended evasion score** combines both scores including a weight for the LLM component\n",
    "- Rationale is that baseline enforces precision while the LLM will capture semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a1455",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "da77e03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "1            qa              NaN            NaN  Steven Chubak   \n",
       "2            qa              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "1                             analyst    Wolfe Research LLC   \n",
       "2             Chief Financial Officer  JPMorgan Chase & Co.   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "1                                 Hey, good morning.  2023      Q1   \n",
       "2                               Good morning, Steve.  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "\n",
       "   is_pleasantry                                         source_pdf  \n",
       "0          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "1           True  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "2           True  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "3          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "4          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1411\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "all_jpm_2023_2025 = pd.read_csv('../data/processed/jpm/all_jpm_2023_2025.csv')\n",
    "\n",
    "# View dataset.\n",
    "display(all_jpm_2023_2025.head())\n",
    "\n",
    "# Number of rows.\n",
    "print('Number of rows:', all_jpm_2023_2025.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "94a06081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1241\n"
     ]
    }
   ],
   "source": [
    "# Remove pleasantries.\n",
    "all_jpm_2023_2025 = all_jpm_2023_2025[all_jpm_2023_2025['is_pleasantry'] == False]\n",
    "print('Number of rows:', all_jpm_2023_2025.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "88a56af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 23\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e7e42dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content.\n",
    "all_jpm_2023_2025 = all_jpm_2023_2025.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "44afee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 0\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5c7fc06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay',\n",
       "       \"We're fundamentally\", 'Thanks', 'Almost no chance.'], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View roles.\n",
    "all_jpm_2023_2025['role'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe99f2",
   "metadata": {},
   "source": [
    "- Some text has leaked into role column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "53bf13fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>qa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Who knows how important politics are in all th...</td>\n",
       "      <td>We're fundamentally</td>\n",
       "      <td>as I said, I think on the press call, happy to...</td>\n",
       "      <td>little bit cautious about the pull-forward dyn...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-1q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>qa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Chief Financial Officer, JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Thanks</td>\n",
       "      <td>Glenn.</td>\n",
       "      <td>Operator: Next, we'll go to the line of Matt O...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer, JPMorgan C...</td>\n",
       "      <td>Almost no chance.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Well, but having – it's very important. While ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q3</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-3q24-earnings-conference-call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     section  question_number  answer_number  \\\n",
       "305       qa             22.0            4.0   \n",
       "309       qa             23.0            3.0   \n",
       "650       qa             10.0            3.0   \n",
       "924       qa              8.0            2.0   \n",
       "1059      qa             22.0            4.0   \n",
       "1063      qa             23.0            3.0   \n",
       "1274      qa             23.0            1.0   \n",
       "\n",
       "                                           speaker_name  \\\n",
       "305              Chief Financial Officer, JPMorganChase   \n",
       "309              Chief Financial Officer, JPMorganChase   \n",
       "650   Who knows how important politics are in all th...   \n",
       "924       Chief Financial Officer, JPMorgan Chase & Co.   \n",
       "1059             Chief Financial Officer, JPMorganChase   \n",
       "1063             Chief Financial Officer, JPMorganChase   \n",
       "1274  Chairman & Chief Executive Officer, JPMorgan C...   \n",
       "\n",
       "                                             role  \\\n",
       "305   And then some. Theres a lot of value added.   \n",
       "309                                          Okay   \n",
       "650                           We're fundamentally   \n",
       "924                                        Thanks   \n",
       "1059  And then some. Theres a lot of value added.   \n",
       "1063                                         Okay   \n",
       "1274                            Almost no chance.   \n",
       "\n",
       "                                                company  \\\n",
       "305                                       JPMorganChase   \n",
       "309                                  there you have it.   \n",
       "650   as I said, I think on the press call, happy to...   \n",
       "924                                              Glenn.   \n",
       "1059                                      JPMorganChase   \n",
       "1063                                 there you have it.   \n",
       "1274                                      JPMorganChase   \n",
       "\n",
       "                                                content  year quarter  \\\n",
       "305   Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "309   But it's not like I thought it would do badly,...  2025      Q2   \n",
       "650   little bit cautious about the pull-forward dyn...  2024      Q1   \n",
       "924   Operator: Next, we'll go to the line of Matt O...  2024      Q2   \n",
       "1059  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "1063  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "1274  Well, but having – it's very important. While ...  2024      Q3   \n",
       "\n",
       "      is_pleasantry                                         source_pdf  \n",
       "305           False  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...  \n",
       "309           False  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...  \n",
       "650           False  data/raw/jpm/jpm-1q24-earnings-call-transcript...  \n",
       "924           False  data/raw/jpm/jpm-2q24-earnings-call-transcript...  \n",
       "1059          False  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "1063          False  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "1274          False  data/raw/jpm/jpm-3q24-earnings-conference-call...  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles. \n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = all_jpm_2023_2025[~all_jpm_2023_2025['role'].isin(valid_roles)]\n",
    "invalid_roles_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9915ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the correct role information.\n",
    "all_jpm_2023_2025.loc[[305, 309, 924, 1059, 1063], 'role'] = 'Chief Financial Officer'\n",
    "all_jpm_2023_2025.loc[[1274], 'role'] = 'Chairman & Chief Executive Officer'\n",
    "\n",
    "# Drop nonsence row.\n",
    "all_jpm_2023_2025 = all_jpm_2023_2025.drop(index=650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "90cbdf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the roles have been updated.\n",
    "all_jpm_2023_2025['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "aff80d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise role names.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Map roles.\n",
    "all_jpm_2023_2025['role_normalised'] = all_jpm_2023_2025['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "35773e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "5            qa              1.0            1.0  Steven Chubak   \n",
       "6            qa              1.0            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "5                             analyst    Wolfe Research LLC   \n",
       "6  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "5  Got it. And just in terms of appetite for the ...  2023      Q1   \n",
       "6                                          Oh, yeah.  2023      Q1   \n",
       "\n",
       "   is_pleasantry                                         source_pdf  \\\n",
       "0          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "3          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "4          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "5          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "6          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "\n",
       "  role_normalised  \n",
       "0          banker  \n",
       "3         analyst  \n",
       "4          banker  \n",
       "5         analyst  \n",
       "6          banker  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1217\n"
     ]
    }
   ],
   "source": [
    "# View dataset.\n",
    "display(all_jpm_2023_2025.head())\n",
    "print('Number of rows:', all_jpm_2023_2025.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4419f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test rows: 262\n",
      "Number of validation rows: 202\n",
      "Number of prediction rows: 753\n"
     ]
    }
   ],
   "source": [
    "# Split into test, validation and training.\n",
    "jpm_test = all_jpm_2023_2025[(all_jpm_2023_2025['year'] == 2023) & (all_jpm_2023_2025['quarter'].isin(['Q1', 'Q2']))]\n",
    "jpm_val = all_jpm_2023_2025[(all_jpm_2023_2025['year'] == 2023) & (all_jpm_2023_2025['quarter'].isin(['Q3', 'Q4']))]\n",
    "jpm_predict = all_jpm_2023_2025[(all_jpm_2023_2025['year'].isin([2024, 2025]))]\n",
    "\n",
    "print('Number of test rows:', jpm_test.shape[0])\n",
    "print('Number of validation rows:', jpm_val.shape[0])\n",
    "print('Number of prediction rows:', jpm_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a48b5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair each analyst question with all the banker's answers (one row per banker answer).\n",
    "def create_qa_pairs(df):\n",
    "    \n",
    "    # Keep only the Q&A section.\n",
    "    qa_df = df[df['section'].astype(str).str.lower() == 'qa'].copy()\n",
    "\n",
    "    # Split into roles.\n",
    "    analyst_rows = qa_df[qa_df['role_normalised'] == 'analyst']\n",
    "    banker_rows = qa_df[qa_df['role_normalised'] == 'banker']\n",
    "\n",
    "    # Build full question text per question_number (concatenate all analyst utterances)\n",
    "    question_text_map = (\n",
    "        analyst_rows.groupby('question_number')['content']\n",
    "        .apply(lambda parts: ' '.join(parts.astype(str)))\n",
    "        .rename('question')\n",
    "    )\n",
    "\n",
    "    # Ensure bankers have an answer_number — if missing, assign sequentially per question\n",
    "    if 'answer_number' not in banker_rows.columns or banker_rows['answer_number'].isna().any():\n",
    "        banker_rows = banker_rows.sort_index().copy()\n",
    "        banker_rows['answer_number'] = banker_rows.groupby('question_number').cumcount() + 1\n",
    "\n",
    "    # Combine multiple banker utterances belonging to the same answer\n",
    "    banker_answers = (\n",
    "        banker_rows.groupby(['question_number', 'answer_number'])\n",
    "        .agg({\n",
    "            'content': lambda parts: ' '.join(parts.astype(str)),\n",
    "            'speaker_name': 'first',\n",
    "            'role': 'first',\n",
    "            'role_normalised': 'first',\n",
    "            'year': 'first',\n",
    "            'quarter': 'first',\n",
    "            'source_pdf': 'first'\n",
    "        })\n",
    "        .rename(columns={'content': 'answer'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge the question text back onto each answer row\n",
    "    qa_pairs = banker_answers.merge(\n",
    "        question_text_map,\n",
    "        left_on='question_number',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Order columns for readability\n",
    "    column_order = [\n",
    "        'question_number', 'answer_number', 'question', 'answer',\n",
    "        'speaker_name', 'role', 'role_normalised',\n",
    "        'year', 'quarter', 'source_pdf'\n",
    "    ]\n",
    "    qa_pairs = qa_pairs.reindex(columns=[c for c in column_order if c in qa_pairs.columns])\n",
    "\n",
    "    # Sort nicely and reset index\n",
    "    qa_pairs = qa_pairs.sort_values(['question_number', 'answer_number']).reset_index(drop=True)\n",
    "\n",
    "    return qa_pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "663e968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create q&A pairs.\n",
    "jpm_test_qa = create_qa_pairs(jpm_test)\n",
    "jpm_val_qa = create_qa_pairs(jpm_val)\n",
    "jpm_predict_qa = create_qa_pairs(jpm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d659c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows.\n",
    "jpm_predict_qa = jpm_predict_qa.drop_duplicates(subset=['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "82e5eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 57\n",
      "Number of validation examples: 49\n",
      "Number of prediction examples: 279\n"
     ]
    }
   ],
   "source": [
    "# View number of examples.\n",
    "print('Number of test examples:', jpm_test_qa.shape[0])\n",
    "print('Number of validation examples:', jpm_val_qa.shape[0])\n",
    "print('Number of prediction examples:', jpm_predict_qa.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e76a2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned datasets.\n",
    "jpm_test_qa.to_csv('../data/processed/jpm/cleaned/jpm_test_qa.csv')\n",
    "jpm_val_qa.to_csv('../data/processed/jpm/cleaned/jpm_val_qa.csv')\n",
    "jpm_predict_qa.to_csv('../data/processed/jpm/cleaned/jpm_predict_qa.csv')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b998a",
   "metadata": {},
   "source": [
    "- Human label the validation and test dataset with evasive or direct labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f494",
   "metadata": {},
   "source": [
    "### **Load the Clean & Labelled Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and labelled validation & test datasets.\n",
    "jpm_predict_qa = pd.read_csv('../data/processed/jpm/cleaned/jpm_predict_qa.csv')\n",
    "jpm_test_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_test_qa_labelled.csv')\n",
    "jpm_val_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_val_qa_labelled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd1968",
   "metadata": {},
   "source": [
    "### **LLM Model Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16d6267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model name checkpoints.\n",
    "roberta_name = 'roberta-large-mnli'\n",
    "deberta_name = 'microsoft/deberta-large-mnli'\n",
    "zs_deberta_name = 'MoritzLaurer/deberta-v3-large-zeroshot-v2.0'\n",
    "\n",
    "# Load tokenizers and models.\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_name)\n",
    "roberta = AutoModelForSequenceClassification.from_pretrained(roberta_name)\n",
    "\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(deberta_name)\n",
    "deberta = AutoModelForSequenceClassification.from_pretrained(deberta_name)\n",
    "\n",
    "zs_deberta_tokenizer = AutoTokenizer.from_pretrained(zs_deberta_name)\n",
    "zs_deberta = AutoModelForSequenceClassification.from_pretrained(zs_deberta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9599a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta id2label: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n",
      "deberta id2label: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n",
      "zs_deberta id2label: {0: 'entailment', 1: 'not_entailment'}\n"
     ]
    }
   ],
   "source": [
    "# Verify label order per model.\n",
    "print(\"roberta id2label:\", roberta.config.id2label)\n",
    "print(\"deberta id2label:\", deberta.config.id2label)\n",
    "print(\"zs_deberta id2label:\", zs_deberta.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84565ed2",
   "metadata": {},
   "source": [
    "- Roberta and deberta have the standard 3 MNLI labels whereas zero shot deberta is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dcde65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add models and tokenizers to dictionary.\n",
    "models_and_tokenizers = {\n",
    "        'roberta': (roberta, roberta_tokenizer),\n",
    "        'deberta': (deberta, deberta_tokenizer),\n",
    "        'zs_deberta': (zs_deberta, zs_deberta_tokenizer)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e540367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # no MPS\n",
    "for model, tok in models_and_tokenizers.values():\n",
    "    model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c60cf",
   "metadata": {},
   "source": [
    "### **Baseline Evasion Score Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9c22283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of evasive phrases\n",
    "EVASIVE_PHRASES = [\n",
    "    r\"\\btoo early\\b\",\n",
    "    r\"\\bcan't (?:comment|share|discuss)\\b\",\n",
    "    r\"\\bwon't (?:comment|share|provide)\\b\",\n",
    "    r\"\\bno (?:update|comment)\\b\",\n",
    "    r\"\\bwe (?:don't|do not) (?:break out|provide guidance)\\b\",\n",
    "    r\"\\bnot (?:going to|able to) (?:comment|share|provide)\\b\",\n",
    "    r\"\\bwe'll (?:come back|circle back)\\b\",\n",
    "    r\"\\bnot something we disclose\\b\",\n",
    "    r\"\\bas (?:we|I) (?:said|mentioned)\\b\",\n",
    "    r\"\\bgenerally speaking\\b\",\n",
    "    r\"\\bit's premature\\b\",\n",
    "    r\"\\bit's difficult to say\\b\",\n",
    "    r\"\\bI (?:wouldn't|won't) want to (?:speculate|get into)\\b\",\n",
    "    r\"\\bI (?:think|guess|suppose)\\b\",\n",
    "    r\"\\bkind of\\b\",\n",
    "    r\"\\bsort of\\b\",\n",
    "    r\"\\baround\\b\",\n",
    "    r\"\\broughly\\b\",\n",
    "    r\"\\bwe (?:prefer|plan) not to\\b\",\n",
    "    r\"\\bwe're not prepared to\\b\",\n",
    "]\n",
    "\n",
    "# List of words that suggest the answer needs specific financial numbers to properly answer the question.\n",
    "SPECIFICITY_TRIGGERS = [\n",
    "    \"how much\",\"how many\",\"what is\",\"what are\",\"when\",\"which\",\"where\",\"who\",\"why\",\n",
    "    \"range\",\"guidance\",\"margin\",\"capex\",\"opex\",\"revenue\",\"sales\",\"eps\",\"ebitda\",\n",
    "    \"timeline\",\"date\",\"target\",\"growth\",\"update\",\"split\",\"dividend\",\"cost\",\"price\",\n",
    "    \"units\",\"volumes\",\"gross\",\"net\",\"tax\",\"percentage\",\"utilization\",\"order book\"\n",
    "]\n",
    "\n",
    "NUMERIC_PATTERN = r\"(?:\\d+(?:\\.\\d+)?%|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b|£|\\$|€)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70af0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between question and answers.\n",
    "def cosine_sim(q, a):\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform([q, a]) # converts text to vectors \n",
    "    sim = float(cosine_similarity(vec[0], vec[1])[0, 0]) # calculate the cosine similarity between the two vectors\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9a484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute baseline evasion score.\n",
    "def baseline_evasion_score(q, a):\n",
    "    # 1. Cosine similarity\n",
    "    sim = cosine_sim(q, a) # calculates cosine similarity using previous function\n",
    "    sim_component = (1 - sim) * 45 # less similar the answer is, the bigger the contribution to the evasion score, scaled by 45\n",
    "\n",
    "    # 2. Numerical specificity- Does the question require and answer with financial figures/ a specific answer?\n",
    "    needs_num = any(t in q.lower() for t in SPECIFICITY_TRIGGERS) # true if the question requires a numeric/ specific answer\n",
    "    has_num = bool(re.search(NUMERIC_PATTERN, a)) # true if the answer includes a number \n",
    "    numeric_component = 25 if needs_num and not has_num else 0 # score of 25 if the question needs a number but the answer doesn't give one\n",
    "\n",
    "    # 3. Evasive phrases- does the answer contain evasive phrases?\n",
    "    phrase_hits = sum(len(re.findall(p, a.lower())) for p in EVASIVE_PHRASES) # counts how many times an evasive phrase appears in the answer\n",
    "    phrase_component = min(3, phrase_hits) * 8 # max of 3 hits counted, each hit = 8 points \n",
    "\n",
    "    # Final evasion score.\n",
    "    score = min(100, sim_component + numeric_component + phrase_component) # adds components together and caps score at 100\n",
    "    \n",
    "    return score, sim, phrase_hits, needs_num, has_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f5e81",
   "metadata": {},
   "source": [
    "### **LLM and Blended Evasion Score Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04b61a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the premise for the model (question + answer).\n",
    "def build_premise(q, a):\n",
    "    return f'[QUESTION] {q} [ANSWER] {a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "98ee4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return probability of entailment, neutral and contradiction.\n",
    "def llm_probabilities(model, tokenizer, device, premise, hypothesis):\n",
    "    model.eval()\n",
    "    encoder = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, max_length=512)\n",
    "    encoder = {k: v.to(device) for k, v in encoder.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoder).logits.squeeze(0)\n",
    "    raw = F.softmax(logits, dim=-1).tolist()\n",
    "\n",
    "    # Label order.\n",
    "    id2label = {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n",
    "\n",
    "    # If num_labels = 3\n",
    "    if len(raw) == 3:\n",
    "        table = {id2label[i]: raw[i] for i in range(3)}\n",
    "        return {\n",
    "            'entailment': table.get('entailment', 0.0),\n",
    "            'neutral': table.get('neutral', 0.0),\n",
    "            'contradiction': table.get('contradiction', 0.0)\n",
    "        }\n",
    "\n",
    "    # If num_labels = 2 (treat as entailment vs not entailment)\n",
    "    elif len(raw) == 2:\n",
    "        table = {id2label[i]: raw[i] for i in range(2)}\n",
    "        ent = table.get('entailment', raw[1])\n",
    "        not_ent = table.get('entailment', raw[0])\n",
    "        return {\n",
    "            'entailment': ent,\n",
    "            'neutral': 0.0, \n",
    "            'contradiction': not_ent\n",
    "        }\n",
    "    else:\n",
    "        return {'entailment': 0.0,\n",
    "                'neutral': 1.0, \n",
    "                'contradiction': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5520da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function compute llm evasion scores.\n",
    "def llm_evasion_score(question, answer, model, tokenizer, device):\n",
    "    premise = build_premise(question, answer)\n",
    "\n",
    "    # Hypothesis.\n",
    "    H_DIRECT = 'The answer gives a direct and specific response to the question'\n",
    "    H_EVASIVE = 'The answer avoids giving a direct response to the question'\n",
    "\n",
    "    pd = llm_probabilities(model, tokenizer, device, premise, H_DIRECT)\n",
    "    pe = llm_probabilities(model, tokenizer, device, premise, H_EVASIVE)\n",
    "\n",
    "    e_direct = max(1e-9, pd['entailment'])\n",
    "    e_evasive = max(1e-9, pe['entailment'])\n",
    "\n",
    "    # Prevent both hypothesis being simultaneously high/low.\n",
    "    denom = math.exp(e_direct) + math.exp(e_evasive)\n",
    "    p_direct = math.exp(e_direct) / denom\n",
    "    p_evasive = math.exp(e_evasive) / denom\n",
    "\n",
    "    return {\n",
    "        'p_direct': p_direct,\n",
    "        'p_evasive': p_evasive\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9aa9911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute blended evasion score and return all scores.\n",
    "def compute_all_evasion_scores(q, a, *, models_and_tokenizers=models_and_tokenizers, device, LLM_WEIGHT=0.30):\n",
    "    \n",
    "    # Compute baseline evasion score.\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # Individual LLM scores.\n",
    "    llm_scores = {}\n",
    "    for name, (m, t) in models_and_tokenizers.items():\n",
    "        scores = llm_evasion_score(q, a, m, t, device)\n",
    "        llm_scores[name] = float(100.0 * scores['p_evasive'])\n",
    "\n",
    "    # Ensemble LLM score.\n",
    "    llm_avg = float(np.mean(list(llm_scores.values()))) if llm_scores else 0.0\n",
    "\n",
    "    # Compute blended score.\n",
    "    blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * llm_avg, 0.0, 100.0))\n",
    "\n",
    "    return {\n",
    "        'baseline': base_score,\n",
    "        'llm_individual': llm_scores,\n",
    "        'llm_avg': llm_avg,\n",
    "        'blended': blended_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1fd47",
   "metadata": {},
   "source": [
    "### **Main Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f819db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define preliminary thresholds.\n",
    "# LLM_WEIGHT = 0.30\n",
    "# EVASION_THRESHOLD_BASE = 60.0\n",
    "# EVASION_THRESHOLD_LLM = 50.0\n",
    "# EVASION_THRESHOLD_BLENDED = 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7618e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label 'Direct' or 'Evasive' based on the score.\n",
    "def label_from_score(score, threshold):\n",
    "    return 'Evasive' if score >= threshold else 'Direct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "81ea294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evasion Pipeline.\n",
    "def evasion_pipeline(df, models_and_tokenizers, device, LLM_WEIGHT, EVASION_THRESHOLD_BASE, EVASION_THRESHOLD_LLM, EVASION_THRESHOLD_BLENDED):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row['question']), str(row['answer'])\n",
    "        output = compute_all_evasion_scores(q=q, a=a, LLM_WEIGHT=LLM_WEIGHT, models_and_tokenizers=models_and_tokenizers, device=device)\n",
    "\n",
    "        pred_base = label_from_score(output['baseline'], EVASION_THRESHOLD_BASE)\n",
    "        pred_llm_avg = label_from_score(output['llm_avg'], EVASION_THRESHOLD_LLM)\n",
    "        pred_blended = label_from_score(output['blended'], EVASION_THRESHOLD_BLENDED)\n",
    "\n",
    "        record = {\n",
    "            'question_number': row.get('question_number'),\n",
    "            'question': q,\n",
    "            'answer': a,\n",
    "\n",
    "            # Evasion Scores\n",
    "            'evasion_score_baseline': int(output['baseline']),\n",
    "            'evasion_score_llm_avg': int(output['llm_avg']),\n",
    "            \"evasion_score_blended\": int(output['blended']),\n",
    "\n",
    "            # Predicted labels.\n",
    "            'prediction_baseline': pred_base,\n",
    "            'prediction_llm_avg': pred_llm_avg,\n",
    "            'prediction_blended': pred_blended,\n",
    "        }\n",
    "\n",
    "        for model_name, score in output['llm_individual'].items():\n",
    "            record[f'evasion_score_{model_name}'] = int(score)\n",
    "            record[f'prediction_{model_name}'] = label_from_score(score, EVASION_THRESHOLD_LLM)\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c0cfa",
   "metadata": {},
   "source": [
    "### **Fine-Tune Score Thresholds**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b64d5c",
   "metadata": {},
   "source": [
    "### **FORMAT FOR EVASION PIPELINE**\n",
    "- Prepare data splits (labelled data only): make a validation dataset for threshold search and model selection, test set for final reporting\n",
    "- Run evasion pipeline on validation set once- save result to disk\n",
    "- Tune thresholds per detector on the validation data set \n",
    "- Determine whether to use ensemble llm or individual llm and get best thresholds\n",
    "- Apply these locked thresholds and llm model to produce predictions on the test set & evaluate\n",
    "- Apply these to training set to predict labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb4cfd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm_avg</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_llm_avg</th>\n",
       "      <th>prediction_blended</th>\n",
       "      <th>evasion_score_roberta</th>\n",
       "      <th>prediction_roberta</th>\n",
       "      <th>evasion_score_deberta</th>\n",
       "      <th>prediction_deberta</th>\n",
       "      <th>evasion_score_zs_deberta</th>\n",
       "      <th>prediction_zs_deberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>64</td>\n",
       "      <td>52</td>\n",
       "      <td>60</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>51</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>56</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>50</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "      <td>52</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>54</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>61</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes. Good morning. This question is for Jamie....</td>\n",
       "      <td>I just – before Jamie answers that, Erika, I j...</td>\n",
       "      <td>35</td>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "      <td>49</td>\n",
       "      <td>Direct</td>\n",
       "      <td>61</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>48</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Got it. And a second follow-up question. And I...</td>\n",
       "      <td>Yeah, Erika, it's a good question. But the tru...</td>\n",
       "      <td>88</td>\n",
       "      <td>49</td>\n",
       "      <td>76</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>50</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>66</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>31</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you. Operator: Thank you. Our next quest...</td>\n",
       "      <td>Thanks, Erika. Operator: I apologize. Our next...</td>\n",
       "      <td>67</td>\n",
       "      <td>51</td>\n",
       "      <td>62</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>50</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>64</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>39</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  Good morning, Jeremy. Wondering if you could s...   \n",
       "1                2  Yeah. And just one question on the NII ex. Mar...   \n",
       "2                3  Yes. Good morning. This question is for Jamie....   \n",
       "3                4  Got it. And a second follow-up question. And I...   \n",
       "4                5  Thank you. Operator: Thank you. Our next quest...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Sure, Ken. So I mean, at a high level, I would...                      64   \n",
       "1  Yeah, that's a good question, Ken. You're righ...                      44   \n",
       "2  I just – before Jamie answers that, Erika, I j...                      35   \n",
       "3  Yeah, Erika, it's a good question. But the tru...                      88   \n",
       "4  Thanks, Erika. Operator: I apologize. Our next...                      67   \n",
       "\n",
       "   evasion_score_llm_avg  evasion_score_blended prediction_baseline  \\\n",
       "0                     52                     60             Evasive   \n",
       "1                     56                     47              Direct   \n",
       "2                     52                     40              Direct   \n",
       "3                     49                     76             Evasive   \n",
       "4                     51                     62             Evasive   \n",
       "\n",
       "  prediction_llm_avg prediction_blended  evasion_score_roberta  \\\n",
       "0            Evasive            Evasive                     51   \n",
       "1            Evasive             Direct                     52   \n",
       "2            Evasive             Direct                     49   \n",
       "3             Direct            Evasive                     50   \n",
       "4            Evasive            Evasive                     50   \n",
       "\n",
       "  prediction_roberta  evasion_score_deberta prediction_deberta  \\\n",
       "0            Evasive                     56            Evasive   \n",
       "1            Evasive                     54            Evasive   \n",
       "2             Direct                     61            Evasive   \n",
       "3            Evasive                     66            Evasive   \n",
       "4            Evasive                     64            Evasive   \n",
       "\n",
       "   evasion_score_zs_deberta prediction_zs_deberta  \n",
       "0                        50               Evasive  \n",
       "1                        61               Evasive  \n",
       "2                        48                Direct  \n",
       "3                        31                Direct  \n",
       "4                        39                Direct  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evasion pipeline.\n",
    "jpm_2025_evasion_results = evasion_pipeline(\n",
    "    jpm_2025_qa_pairs_df, \n",
    "    models_and_tokenizers=models_and_tokenizers, \n",
    "    device=device,\n",
    "    LLM_WEIGHT=0.30, \n",
    "    EVASION_THRESHOLD_BASE=60.0,\n",
    "    EVASION_THRESHOLD_LLM=50.0,\n",
    "    EVASION_THRESHOLD_BLENDED=60.0)\n",
    "\n",
    "# View results.\n",
    "jpm_2025_evasion_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43c32e",
   "metadata": {},
   "source": [
    "- Built a validation set of 26 examples from 2023 1q jpm results, human-labelled these evasive or direct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f492696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2023 transcript.\n",
    "jpm_1q_23_df = pd.read_csv('../data/processed/jpm/jpm-1q23-earnings-call-transcript_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a401a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_1q_23_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22775960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "1            qa              NaN            NaN  Steven Chubak   \n",
       "2            qa              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "1                             analyst    Wolfe Research LLC   \n",
       "2             Chief Financial Officer  JPMorgan Chase & Co.   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "1                                 Hey, good morning.  2023      Q1   \n",
       "2                               Good morning, Steve.  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "\n",
       "   is_pleasantry role_normalised  \n",
       "0          False          banker  \n",
       "1           True         analyst  \n",
       "2           True          banker  \n",
       "3          False         analyst  \n",
       "4          False          banker  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_1q_23_df['role_normalised'] = jpm_1q_23_df['role'].map(role_map)\n",
    "\n",
    "# View the dataset.\n",
    "jpm_1q_23_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2955b5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>...elevated macro uncertainties.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matt O'Connor</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Deutsche Bank Securities, Inc.</td>\n",
       "      <td>Okay. And then just separately to squeeze in –...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>That'll be every quarter for the rest of our l...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>qa</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matt O'Connor</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Deutsche Bank Securities, Inc.</td>\n",
       "      <td>Okay. All right. Thank you. Operator: We have ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   section  question_number  answer_number   speaker_name  \\\n",
       "3       qa              1.0            NaN  Steven Chubak   \n",
       "4       qa              1.0            1.0    Jamie Dimon   \n",
       "5       qa              1.0            1.0  Steven Chubak   \n",
       "6       qa              1.0            2.0    Jamie Dimon   \n",
       "7       qa              1.0            2.0  Steven Chubak   \n",
       "..     ...              ...            ...            ...   \n",
       "93      qa             26.0            NaN  Matt O'Connor   \n",
       "94      qa             26.0            1.0    Jamie Dimon   \n",
       "95      qa             26.0            2.0  Jeremy Barnum   \n",
       "96      qa             26.0            3.0    Jamie Dimon   \n",
       "97      qa             27.0            NaN  Matt O'Connor   \n",
       "\n",
       "                                  role                         company  \\\n",
       "3                              analyst              Wolfe Research LLC   \n",
       "4   Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "5                              analyst              Wolfe Research LLC   \n",
       "6   Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "7                              analyst              Wolfe Research LLC   \n",
       "..                                 ...                             ...   \n",
       "93                             analyst  Deutsche Bank Securities, Inc.   \n",
       "94  Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "95             Chief Financial Officer            JPMorgan Chase & Co.   \n",
       "96  Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "97                             analyst  Deutsche Bank Securities, Inc.   \n",
       "\n",
       "                                              content  year quarter  \\\n",
       "3   So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4   Well, I think you were already kind of complet...  2023      Q1   \n",
       "5   Got it. And just in terms of appetite for the ...  2023      Q1   \n",
       "6                                           Oh, yeah.  2023      Q1   \n",
       "7                    ...elevated macro uncertainties.  2023      Q1   \n",
       "..                                                ...   ...     ...   \n",
       "93  Okay. And then just separately to squeeze in –...  2023      Q1   \n",
       "94  That'll be every quarter for the rest of our l...  2023      Q1   \n",
       "95                                             Cheap.  2023      Q1   \n",
       "96                                             Cheap.  2023      Q1   \n",
       "97  Okay. All right. Thank you. Operator: We have ...  2023      Q1   \n",
       "\n",
       "    is_pleasantry role_normalised  \n",
       "3           False         analyst  \n",
       "4           False          banker  \n",
       "5           False         analyst  \n",
       "6           False          banker  \n",
       "7           False         analyst  \n",
       "..            ...             ...  \n",
       "93          False         analyst  \n",
       "94          False          banker  \n",
       "95          False          banker  \n",
       "96          False          banker  \n",
       "97          False         analyst  \n",
       "\n",
       "[85 rows x 11 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out presention and is_pleasantry == True\n",
    "jpm_1q_23_df = jpm_1q_23_df[jpm_1q_23_df['section'] == 'qa']\n",
    "jpm_1q_23_df = jpm_1q_23_df[jpm_1q_23_df['is_pleasantry'] == False]\n",
    "\n",
    "# View the dataset.\n",
    "jpm_1q_23_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d17d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0              1.0  So, Jamie, I was actually hoping to get your p...   \n",
       "1              2.0  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2              3.0  Yeah, and as a follow-up on the point about ra...   \n",
       "3              4.0  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4              5.0  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  \n",
       "0  Well, I think you were already kind of complet...  \n",
       "1  Yeah, sure. So let me just summarize the drive...  \n",
       "2  Well first of all, I don't quite believe it. S...  \n",
       "3  Yeah. John, it's a really good question, and w...  \n",
       "4  Yeah. I wouldn't use the word credit crunch if...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 26\n"
     ]
    }
   ],
   "source": [
    "# Create qa pairs.\n",
    "jpm_1q_23_qa_pairs_df = create_qa_pairs(jpm_1q_23_df)\n",
    "\n",
    "# View dataset.\n",
    "display(jpm_1q_23_qa_pairs_df.head())\n",
    "\n",
    "# View shape.\n",
    "print('Number of samples:', jpm_1q_23_qa_pairs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f9eacd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test set. \n",
    "jpm_1q_23_test_set_df = jpm_1q_23_qa_pairs_df.copy()\n",
    "\n",
    "# Create a blank label column and export to CSV for human to label.\n",
    "jpm_1q_23_test_set_df['label'] = ''  # fill with Direct or Evasive\n",
    "jpm_1q_23_test_set_df.to_csv('jpm_1q_23_test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacd1e4",
   "metadata": {},
   "source": [
    "- The test set was human labelled with either 'direct' or 'evasive'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "07a7d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer    label  \n",
       "0  Well, I think you were already kind of complet...  Evasive  \n",
       "1  Yeah, sure. So let me just summarize the drive...   Direct  \n",
       "2  Well first of all, I don't quite believe it. S...   Direct  \n",
       "3  Yeah. John, it's a really good question, and w...  Evasive  \n",
       "4  Yeah. I wouldn't use the word credit crunch if...   Direct  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the labelled test set. \n",
    "jpm_1q_23_test_set_labelled_df = pd.read_csv('../notebooks/summarisation_evasion_files/jpm_1q_23_test_set_labelled.csv')\n",
    "\n",
    "# View dataset.\n",
    "jpm_1q_23_test_set_labelled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef43f3c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evasion_pipeline() missing 6 required positional arguments: 'models_and_tokenizers', 'device', 'LLM_WEIGHT', 'EVASION_THRESHOLD_BASE', 'EVASION_THRESHOLD_LLM', and 'EVASION_THRESHOLD_BLENDED'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run evasion pipeline.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m jpm_1q_23_evasion_results \u001b[38;5;241m=\u001b[39m \u001b[43mevasion_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjpm_1q_23_test_set_labelled_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Reappend the human label.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m jpm_1q_23_evasion_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m jpm_1q_23_test_set_labelled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: evasion_pipeline() missing 6 required positional arguments: 'models_and_tokenizers', 'device', 'LLM_WEIGHT', 'EVASION_THRESHOLD_BASE', 'EVASION_THRESHOLD_LLM', and 'EVASION_THRESHOLD_BLENDED'"
     ]
    }
   ],
   "source": [
    "# Run evasion pipeline.\n",
    "jpm_1q_23_evasion_results = evasion_pipeline(jpm_1q_23_test_set_labelled_df)\n",
    "\n",
    "# Reappend the human label.\n",
    "jpm_1q_23_evasion_results['human_label'] = jpm_1q_23_test_set_labelled_df['label']\n",
    "\n",
    "# View results.\n",
    "jpm_1q_23_evasion_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9831d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the evasion scores vs true labels.\n",
    "def evaluate_evasion_scores(df):\n",
    "\n",
    "    # True labels: 1 = Evasive, 0 = Direct (using 'human_label').\n",
    "    y_true = (df[\"human_label\"].astype(str).str.strip().str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "    # Convert predicted label strings to binary (1 = Evasive, 0 = Direct).\n",
    "    def to_binary(pred_series):\n",
    "        return (pred_series.astype(str).str.strip().str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "    # Convert predicted labels to binary.\n",
    "    y_pred_base  = to_binary(df[\"prediction_baseline\"])\n",
    "    y_pred_llm   = to_binary(df[\"prediction_llm\"])\n",
    "    y_pred_blend = to_binary(df[\"prediction_blended\"])\n",
    "\n",
    "    return {\n",
    "        'baseline': {\n",
    "            'classification_report': classification_report(y_true, y_pred_base, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_base)\n",
    "        },\n",
    "        'llm': {\n",
    "            'classification_report': classification_report(y_true, y_pred_llm, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_llm)\n",
    "        },\n",
    "        'blended': {\n",
    "            'classification_report': classification_report(y_true, y_pred_blend, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_blend) \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results.\n",
    "eval_dict = evaluate_evasion_scores(jpm_1q_23_evasion_results)\n",
    "baseline_eval, llm_eval, blended_eval = eval_dict['baseline'], eval_dict['llm'], eval_dict['blended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.588     0.714     0.645        14\n",
      "     Evasive      0.556     0.417     0.476        12\n",
      "\n",
      "    accuracy                          0.577        26\n",
      "   macro avg      0.572     0.565     0.561        26\n",
      "weighted avg      0.573     0.577     0.567        26\n",
      "\n",
      "[[10  4]\n",
      " [ 7  5]]\n"
     ]
    }
   ],
   "source": [
    "# View baseline results.\n",
    "base_cr, base_cm = baseline_eval['classification_report'], baseline_eval['confusion_matrix']\n",
    "\n",
    "print(base_cr)\n",
    "print(base_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.000     0.000     0.000        14\n",
      "     Evasive      0.462     1.000     0.632        12\n",
      "\n",
      "    accuracy                          0.462        26\n",
      "   macro avg      0.231     0.500     0.316        26\n",
      "weighted avg      0.213     0.462     0.291        26\n",
      "\n",
      "[[ 0 14]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "# View llm results.\n",
    "llm_cr, llm_cm = llm_eval['classification_report'], llm_eval['confusion_matrix']\n",
    "\n",
    "print(llm_cr)\n",
    "print(llm_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      1.000     0.071     0.133        14\n",
      "     Evasive      0.480     1.000     0.649        12\n",
      "\n",
      "    accuracy                          0.500        26\n",
      "   macro avg      0.740     0.536     0.391        26\n",
      "weighted avg      0.760     0.500     0.371        26\n",
      "\n",
      "[[ 1 13]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "# View blended results.\n",
    "blended_cr, blended_cm = blended_eval['classification_report'], blended_eval['confusion_matrix']\n",
    "\n",
    "print(blended_cr)\n",
    "print(blended_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80e12c",
   "metadata": {},
   "source": [
    "- Baseline (rule-based) accuracy is currently the highest at 60%\n",
    "- Fine tune the thresholds using grid search to determine the optimal threshold to give the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d786791",
   "metadata": {},
   "source": [
    "### **Tuning threshold**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec7381",
   "metadata": {},
   "source": [
    "- Recall will be the priority over accuracy to prevent missing evasive answers. \n",
    "- Some false positives are tolerable and regulator can review flagged answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199945e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>human_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>100</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>99</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Well, I think you were already kind of complet...                      55   \n",
       "1  Yeah, sure. So let me just summarize the drive...                      80   \n",
       "2  Well first of all, I don't quite believe it. S...                      40   \n",
       "3  Yeah. John, it's a really good question, and w...                      78   \n",
       "4  Yeah. I wouldn't use the word credit crunch if...                      55   \n",
       "\n",
       "   evasion_score_llm  evasion_score_blended human_label  \n",
       "0                 79                     79     Evasive  \n",
       "1                 78                    100      Direct  \n",
       "2                 83                     65      Direct  \n",
       "3                 73                     99     Evasive  \n",
       "4                 81                     80      Direct  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain validation set with evasion scores and drop predictions.\n",
    "jpm_1q_23_tuning = jpm_1q_23_evasion_results.drop(['prediction_baseline', 'prediction_llm', 'prediction_blended'], axis=1)\n",
    "jpm_1q_23_tuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth (1 = Evasive, 0 = Direct)\n",
    "def extract_y_true(df):\n",
    "    return (df['human_label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculate metrics for each threshold.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def tune_threshold(df, score_col, thr_grid):\n",
    "    y_true = extract_y_true(df)                     # get true labels\n",
    "    scores = df[score_col].astype(float).values     # get raw evasion scores \n",
    "\n",
    "    rows = []\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (scores >= thr).astype(int) # label response evasive (1) if score is higher than threshold\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        rows.append({\n",
    "            'threshold': float(thr),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    results = pd.DataFrame(rows).sort_values(\n",
    "        by=['recall', 'f1', 'precision'],\n",
    "        ascending=[False, False, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold ranges around current thresholds.\n",
    "thr_base_grid = np.arange(40, 85, 5)\n",
    "thr_llm_grid = np.arange(35, 85, 5)\n",
    "thr_blend_grid = np.arange(40, 85, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search for each detector.\n",
    "base_results = tune_threshold(jpm_1q_23_tuning, 'evasion_score_baseline', thr_base_grid)\n",
    "llm_results = tune_threshold(jpm_1q_23_tuning, 'evasion_score_llm', thr_llm_grid)\n",
    "blend_results = tune_threshold(jpm_1q_23_tuning, 'evasion_score_blended', thr_blend_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Baseline Threshold: 40.0\n",
      "Best LLM Threshold: 70.0\n",
      "Best Blended Threshold 40.0\n"
     ]
    }
   ],
   "source": [
    "# Extract the best thresholds.\n",
    "best_base_thr = base_results.loc[0, 'threshold']\n",
    "best_llm_thr = llm_results.loc[0, 'threshold']\n",
    "best_blend_thr = blend_results.loc[0, 'threshold']\n",
    "\n",
    "print('Best Baseline Threshold:', best_base_thr)\n",
    "print('Best LLM Threshold:', best_llm_thr)\n",
    "print('Best Blended Threshold', best_base_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a41a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 baseline configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       40.0   0.454545  0.833333  0.588235  0.461538\n",
      "1       45.0   0.500000  0.666667  0.571429  0.538462\n",
      "2       50.0   0.583333  0.583333  0.583333  0.615385\n",
      "3       55.0   0.545455  0.500000  0.521739  0.576923\n",
      "4       65.0   0.714286  0.416667  0.526316  0.653846\n",
      "\n",
      "Top 5 llm configs:\n",
      "    threshold  precision  recall        f1  accuracy\n",
      "0       70.0   0.521739     1.0  0.685714  0.576923\n",
      "1       65.0   0.480000     1.0  0.648649  0.500000\n",
      "2       35.0   0.461538     1.0  0.631579  0.461538\n",
      "3       40.0   0.461538     1.0  0.631579  0.461538\n",
      "4       45.0   0.461538     1.0  0.631579  0.461538\n",
      "\n",
      "Top 5 blended configs:\n",
      "    threshold  precision  recall        f1  accuracy\n",
      "0       55.0   0.480000     1.0  0.648649  0.500000\n",
      "1       60.0   0.480000     1.0  0.648649  0.500000\n",
      "2       40.0   0.461538     1.0  0.631579  0.461538\n",
      "3       45.0   0.461538     1.0  0.631579  0.461538\n",
      "4       50.0   0.461538     1.0  0.631579  0.461538\n"
     ]
    }
   ],
   "source": [
    "# Inspect trade-offs.\n",
    "print('\\nTop 5 baseline configs:\\n', base_results.head())\n",
    "print('\\nTop 5 llm configs:\\n', llm_results.head())\n",
    "print('\\nTop 5 blended configs:\\n', blend_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51b9dc",
   "metadata": {},
   "source": [
    "**Baseline:**\n",
    "- 40.0 gave the highest recall (0.83) but lower precision (0.45)\n",
    "- 50.0 is more balanced (0.58 recall and 0.58 precision)\n",
    "\n",
    "**LLM**\n",
    "- Every top configuration has recall = 1.0 which shows the LLM detector is very sensitive \n",
    "- Raising the threshold increases precision slightly = 70.0\n",
    "\n",
    "**Blended**\n",
    "- Similar to LLM result, 55.0 or 60.0 gave slightly higher precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
