{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sGbUTbmV4Ck",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "921e5039-775f-4a53-f80c-808c3927e2f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'bootstrap_resample' from 'sklearn.model_selection' (/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2601410331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Advanced model evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalibration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalibration_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbootstrap_resample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'bootstrap_resample' from 'sklearn.model_selection' (/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# 05_model_comparison_jpm_enhanced.ipynb\n",
        "# Purpose: Enhanced model comparison with fine-tuned models and comprehensive evaluation\n",
        "# Input: Enhanced sentiment analysis results + fine-tuned models\n",
        "# Output: Comprehensive model comparison with performance metrics and insights\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr, wilcoxon, mannwhitneyu\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, cohen_kappa_score,\n",
        "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
        "    average_precision_score, balanced_accuracy_score\n",
        ")\n",
        "\n",
        "# Advanced model evaluation\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.model_selection import bootstrap_resample\n",
        "import itertools\n",
        "\n",
        "# Visualization enhancements\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Statistical tests\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "\n",
        "# Location A: Google Drive (Primary drive)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project/config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "SEED = config[\"SEED\"]\n",
        "BANK_CODE = config[\"BANK_CODE\"]\n",
        "drive_base = Path(config[\"drive_base\"])\n",
        "colab_base = Path(config[\"colab_base\"])\n",
        "\n",
        "print(f\"Enhanced model comparison analysis for bank: {BANK_CODE.upper()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Paths\n",
        "\n",
        "results_sentiment_path = drive_base / \"results/sentiment/jpm\"\n",
        "results_comparison_path = drive_base / \"results/comparison/jpm\"\n",
        "viz_path = drive_base / \"outputs/visualizations/jpm\"\n",
        "models_path = drive_base / \"models\"\n",
        "finetuned_models_path = models_path / \"finetuned\"\n",
        "\n",
        "# Ensure directories exist\n",
        "results_comparison_path.mkdir(parents=True, exist_ok=True)\n",
        "viz_path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "2PuqhjyeWbZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Enhanced Results and Fine-tuned Models\n",
        "\n",
        "def load_enhanced_sentiment_results(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"Load enhanced sentiment analysis results.\"\"\"\n",
        "    file_path = results_sentiment_path / filename\n",
        "    if not file_path.exists():\n",
        "        print(f\"Warning: File not found: {file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {filename}: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def load_finetuning_results() -> Dict:\n",
        "    \"\"\"Load fine-tuning results and model registry.\"\"\"\n",
        "    finetuning_results = {}\n",
        "\n",
        "    # Load fine-tuning results\n",
        "    results_path = finetuned_models_path / \"finetuning_results.json\"\n",
        "    if results_path.exists():\n",
        "        try:\n",
        "            with open(results_path, 'r') as f:\n",
        "                finetuning_results = json.load(f)\n",
        "            print(\"Loaded fine-tuning results\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load fine-tuning results: {e}\")\n",
        "\n",
        "    # Load model registry\n",
        "    registry_path = finetuned_models_path / \"model_registry.json\"\n",
        "    model_registry = {}\n",
        "    if registry_path.exists():\n",
        "        try:\n",
        "            with open(registry_path, 'r') as f:\n",
        "                model_registry = json.load(f)\n",
        "            print(\"Loaded model registry\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load model registry: {e}\")\n",
        "\n",
        "    return finetuning_results, model_registry\n",
        "\n",
        "print(\"Loading enhanced sentiment analysis results...\")\n",
        "\n",
        "# Load enhanced results\n",
        "enhanced_sentiment_q1_df = load_enhanced_sentiment_results(\"enhanced_sentiment_sentence_jpm_q1_2025.csv\")\n",
        "enhanced_sentiment_q2_df = load_enhanced_sentiment_results(\"enhanced_sentiment_sentence_jpm_q2_2025.csv\")\n",
        "enhanced_sentiment_multi_df = load_enhanced_sentiment_results(\"enhanced_sentiment_sentence_jpm_multi_2025.csv\")\n",
        "\n",
        "# Load aggregated results\n",
        "enhanced_qa_level_df = load_enhanced_sentiment_results(\"enhanced_sentiment_qa_jpm_multi_2025.csv\")\n",
        "enhanced_speaker_level_df = load_enhanced_sentiment_results(\"enhanced_sentiment_speaker_jpm_multi_2025.csv\")\n",
        "enhanced_topic_level_df = load_enhanced_sentiment_results(\"enhanced_sentiment_topic_jpm_multi_2025.csv\")\n",
        "\n",
        "# Load fine-tuning results\n",
        "finetuning_results, model_registry = load_finetuning_results()\n"
      ],
      "metadata": {
        "id": "LzbDLHyIWclp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Model Comparison Framework\n",
        "\n",
        "class EnhancedModelComparator:\n",
        "    \"\"\"Comprehensive model comparison with statistical rigor.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.models = self.identify_available_models()\n",
        "        self.comparison_results = {}\n",
        "\n",
        "    def identify_available_models(self) -> List[str]:\n",
        "        \"\"\"Identify all available model predictions in the dataset.\"\"\"\n",
        "        if self.df is None:\n",
        "            return []\n",
        "\n",
        "        model_columns = []\n",
        "\n",
        "        # Standard models\n",
        "        if 'finbert_tone_label' in self.df.columns:\n",
        "            model_columns.append('finbert_tone')\n",
        "        if 'prosus_label' in self.df.columns:\n",
        "            model_columns.append('prosus')\n",
        "        if 'ensemble_label' in self.df.columns:\n",
        "            model_columns.append('ensemble')\n",
        "\n",
        "        # Check for fine-tuned model results\n",
        "        for col in self.df.columns:\n",
        "            if col.endswith('_finetuned_label'):\n",
        "                model_name = col.replace('_finetuned_label', '_finetuned')\n",
        "                model_columns.append(model_name)\n",
        "\n",
        "        print(f\"Available models for comparison: {model_columns}\")\n",
        "        return model_columns\n",
        "\n",
        "    def calculate_comprehensive_agreement_metrics(self) -> Dict:\n",
        "        \"\"\"Calculate comprehensive agreement metrics between all model pairs.\"\"\"\n",
        "        if len(self.models) < 2:\n",
        "            return {}\n",
        "\n",
        "        print(\"Calculating comprehensive agreement metrics...\")\n",
        "\n",
        "        agreement_results = {}\n",
        "\n",
        "        # Pairwise comparisons\n",
        "        for model1, model2 in itertools.combinations(self.models, 2):\n",
        "            pair_name = f\"{model1}_vs_{model2}\"\n",
        "\n",
        "            # Get label columns\n",
        "            label_col1 = f\"{model1}_label\"\n",
        "            label_col2 = f\"{model2}_label\"\n",
        "\n",
        "            if label_col1 not in self.df.columns or label_col2 not in self.df.columns:\n",
        "                continue\n",
        "\n",
        "            # Filter valid predictions\n",
        "            valid_mask = (\n",
        "                self.df[label_col1].notna() &\n",
        "                self.df[label_col2].notna()\n",
        "            )\n",
        "            valid_df = self.df[valid_mask]\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                continue\n",
        "\n",
        "            labels1 = valid_df[label_col1].values\n",
        "            labels2 = valid_df[label_col2].values\n",
        "\n",
        "            # Basic agreement\n",
        "            agreement_rate = (labels1 == labels2).mean()\n",
        "\n",
        "            # Cohen's Kappa\n",
        "            kappa = cohen_kappa_score(labels1, labels2)\n",
        "\n",
        "            # Confidence correlations\n",
        "            score_col1 = f\"{model1}_calibrated\" if f\"{model1}_calibrated\" in self.df.columns else f\"{model1}_score\"\n",
        "            score_col2 = f\"{model2}_calibrated\" if f\"{model2}_calibrated\" in self.df.columns else f\"{model2}_score\"\n",
        "\n",
        "            score_correlation = None\n",
        "            if score_col1 in valid_df.columns and score_col2 in valid_df.columns:\n",
        "                scores1 = valid_df[score_col1].fillna(0.5)\n",
        "                scores2 = valid_df[score_col2].fillna(0.5)\n",
        "\n",
        "                pearson_r, pearson_p = pearsonr(scores1, scores2)\n",
        "                spearman_r, spearman_p = spearmanr(scores1, scores2)\n",
        "\n",
        "                score_correlation = {\n",
        "                    'pearson': {'r': pearson_r, 'p_value': pearson_p},\n",
        "                    'spearman': {'r': spearman_r, 'p_value': spearman_p}\n",
        "                }\n",
        "\n",
        "            # Class-wise agreement\n",
        "            class_agreement = {}\n",
        "            unique_labels = set(labels1) | set(labels2)\n",
        "            for label in unique_labels:\n",
        "                mask1 = labels1 == label\n",
        "                if mask1.sum() > 0:\n",
        "                    class_agreement[label] = (labels1[mask1] == labels2[mask1]).mean()\n",
        "\n",
        "            # Statistical significance test\n",
        "            try:\n",
        "                contingency_table = confusion_matrix(labels1, labels2)\n",
        "                chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "                statistical_test = {\n",
        "                    'test': 'chi_square',\n",
        "                    'statistic': chi2,\n",
        "                    'p_value': p_value,\n",
        "                    'degrees_of_freedom': dof\n",
        "                }\n",
        "            except Exception as e:\n",
        "                statistical_test = {'error': str(e)}\n",
        "\n",
        "            agreement_results[pair_name] = {\n",
        "                'model1': model1,\n",
        "                'model2': model2,\n",
        "                'sample_size': len(valid_df),\n",
        "                'agreement_rate': agreement_rate,\n",
        "                'cohen_kappa': kappa,\n",
        "                'score_correlation': score_correlation,\n",
        "                'class_agreement': class_agreement,\n",
        "                'statistical_test': statistical_test\n",
        "            }\n",
        "\n",
        "            print(f\"  {pair_name}: Agreement={agreement_rate:.3f}, Kappa={kappa:.3f}\")\n",
        "\n",
        "        return agreement_results\n",
        "\n",
        "    def analyze_prediction_distributions(self) -> Dict:\n",
        "        \"\"\"Analyze prediction distributions across models.\"\"\"\n",
        "        print(\"Analyzing prediction distributions...\")\n",
        "\n",
        "        distribution_results = {}\n",
        "\n",
        "        for model in self.models:\n",
        "            label_col = f\"{model}_label\"\n",
        "            if label_col not in self.df.columns:\n",
        "                continue\n",
        "\n",
        "            # Distribution\n",
        "            distribution = self.df[label_col].value_counts(normalize=True).sort_index()\n",
        "\n",
        "            # Entropy (uncertainty measure)\n",
        "            entropy = -sum(p * np.log(p + 1e-8) for p in distribution.values)\n",
        "\n",
        "            # Confidence statistics\n",
        "            confidence_stats = {}\n",
        "            score_cols = [f\"{model}_score\", f\"{model}_calibrated\"]\n",
        "            for score_col in score_cols:\n",
        "                if score_col in self.df.columns:\n",
        "                    scores = self.df[score_col].dropna()\n",
        "                    if len(scores) > 0:\n",
        "                        confidence_stats[score_col] = {\n",
        "                            'mean': scores.mean(),\n",
        "                            'std': scores.std(),\n",
        "                            'median': scores.median(),\n",
        "                            'min': scores.min(),\n",
        "                            'max': scores.max()\n",
        "                        }\n",
        "\n",
        "            distribution_results[model] = {\n",
        "                'distribution': distribution.to_dict(),\n",
        "                'entropy': entropy,\n",
        "                'confidence_stats': confidence_stats,\n",
        "                'total_predictions': self.df[label_col].notna().sum()\n",
        "            }\n",
        "\n",
        "        return distribution_results\n",
        "\n",
        "    def evaluate_calibration_quality(self) -> Dict:\n",
        "        \"\"\"Evaluate confidence calibration quality for each model.\"\"\"\n",
        "        print(\"Evaluating confidence calibration...\")\n",
        "\n",
        "        calibration_results = {}\n",
        "\n",
        "        # Need ground truth for calibration evaluation\n",
        "        if 'human_label' not in self.df.columns:\n",
        "            print(\"No ground truth labels available for calibration evaluation\")\n",
        "            return {}\n",
        "\n",
        "        human_labeled_mask = self.df['human_label'].notna()\n",
        "        eval_df = self.df[human_labeled_mask]\n",
        "\n",
        "        if len(eval_df) == 0:\n",
        "            return {}\n",
        "\n",
        "        for model in self.models:\n",
        "            label_col = f\"{model}_label\"\n",
        "            score_cols = [f\"{model}_calibrated\", f\"{model}_score\"]\n",
        "\n",
        "            if label_col not in eval_df.columns:\n",
        "                continue\n",
        "\n",
        "            model_mask = eval_df[label_col].notna()\n",
        "            model_eval_df = eval_df[model_mask]\n",
        "\n",
        "            if len(model_eval_df) == 0:\n",
        "                continue\n",
        "\n",
        "            calibration_results[model] = {}\n",
        "\n",
        "            for score_col in score_cols:\n",
        "                if score_col not in model_eval_df.columns:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    predictions = model_eval_df[label_col].values\n",
        "                    true_labels = model_eval_df['human_label'].values\n",
        "                    confidences = model_eval_df[score_col].values\n",
        "\n",
        "                    # Calculate Expected Calibration Error (ECE)\n",
        "                    n_bins = 10\n",
        "                    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "                    bin_lowers = bin_boundaries[:-1]\n",
        "                    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "                    ece = 0\n",
        "                    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "                        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
        "                        prop_in_bin = in_bin.mean()\n",
        "\n",
        "                        if prop_in_bin > 0:\n",
        "                            accuracy_in_bin = (predictions[in_bin] == true_labels[in_bin]).mean()\n",
        "                            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "                    calibration_results[model][score_col] = {\n",
        "                        'expected_calibration_error': ece,\n",
        "                        'sample_size': len(model_eval_df)\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    calibration_results[model][score_col] = {'error': str(e)}\n",
        "\n",
        "        return calibration_results\n",
        "\n",
        "    def analyze_disagreement_patterns(self) -> Dict:\n",
        "        \"\"\"Deep analysis of model disagreement patterns.\"\"\"\n",
        "        print(\"Analyzing disagreement patterns...\")\n",
        "\n",
        "        if len(self.models) < 2:\n",
        "            return {}\n",
        "\n",
        "        disagreement_results = {}\n",
        "\n",
        "        # Find cases where models disagree\n",
        "        for model1, model2 in itertools.combinations(self.models, 2):\n",
        "            pair_name = f\"{model1}_vs_{model2}\"\n",
        "\n",
        "            label_col1 = f\"{model1}_label\"\n",
        "            label_col2 = f\"{model2}_label\"\n",
        "\n",
        "            if label_col1 not in self.df.columns or label_col2 not in self.df.columns:\n",
        "                continue\n",
        "\n",
        "            valid_mask = (\n",
        "                self.df[label_col1].notna() &\n",
        "                self.df[label_col2].notna()\n",
        "            )\n",
        "            valid_df = self.df[valid_mask]\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Find disagreements\n",
        "            disagreement_mask = valid_df[label_col1] != valid_df[label_col2]\n",
        "            disagreements = valid_df[disagreement_mask]\n",
        "\n",
        "            if len(disagreements) == 0:\n",
        "                continue\n",
        "\n",
        "            # Disagreement patterns\n",
        "            disagreement_patterns = disagreements.groupby([label_col1, label_col2]).size()\n",
        "\n",
        "            # Analyze disagreement by various factors\n",
        "            disagreement_analysis = {\n",
        "                'total_disagreements': len(disagreements),\n",
        "                'disagreement_rate': len(disagreements) / len(valid_df),\n",
        "                'disagreement_patterns': disagreement_patterns.to_dict()\n",
        "            }\n",
        "\n",
        "            # By speaker role\n",
        "            if 'speaker_role' in disagreements.columns:\n",
        "                speaker_disagreements = disagreements['speaker_role'].value_counts()\n",
        "                disagreement_analysis['by_speaker_role'] = speaker_disagreements.to_dict()\n",
        "\n",
        "            # By topic\n",
        "            if 'primary_topic' in disagreements.columns:\n",
        "                topic_disagreements = disagreements['primary_topic'].value_counts()\n",
        "                disagreement_analysis['by_topic'] = topic_disagreements.to_dict()\n",
        "\n",
        "            # By text length\n",
        "            if 'sentence_length' in disagreements.columns:\n",
        "                avg_disagreement_length = disagreements['sentence_length'].mean()\n",
        "                agreement_df = valid_df[~disagreement_mask]\n",
        "                avg_agreement_length = agreement_df['sentence_length'].mean()\n",
        "\n",
        "                disagreement_analysis['text_length_analysis'] = {\n",
        "                    'avg_disagreement_length': avg_disagreement_length,\n",
        "                    'avg_agreement_length': avg_agreement_length,\n",
        "                    'length_difference': avg_disagreement_length - avg_agreement_length\n",
        "                }\n",
        "\n",
        "            # Confidence analysis for disagreements\n",
        "            score_col1 = f\"{model1}_calibrated\" if f\"{model1}_calibrated\" in disagreements.columns else f\"{model1}_score\"\n",
        "            score_col2 = f\"{model2}_calibrated\" if f\"{model2}_calibrated\" in disagreements.columns else f\"{model2}_score\"\n",
        "\n",
        "            if score_col1 in disagreements.columns and score_col2 in disagreements.columns:\n",
        "                disagreement_analysis['confidence_analysis'] = {\n",
        "                    'avg_confidence_model1': disagreements[score_col1].mean(),\n",
        "                    'avg_confidence_model2': disagreements[score_col2].mean(),\n",
        "                    'high_confidence_disagreements': (\n",
        "                        (disagreements[score_col1] > 0.8) &\n",
        "                        (disagreements[score_col2] > 0.8)\n",
        "                    ).sum()\n",
        "                }\n",
        "\n",
        "            disagreement_results[pair_name] = disagreement_analysis\n",
        "\n",
        "        return disagreement_results\n",
        "\n",
        "# Initialize enhanced comparator\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENHANCED MODEL COMPARISON ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "enhanced_comparator = None\n",
        "if enhanced_sentiment_multi_df is not None:\n",
        "    enhanced_comparator = EnhancedModelComparator(enhanced_sentiment_multi_df)\n",
        "\n",
        "    # Run comprehensive analysis\n",
        "    print(\"\\nRunning comprehensive model comparison...\")\n",
        "\n",
        "    # Agreement metrics\n",
        "    agreement_results = enhanced_comparator.calculate_comprehensive_agreement_metrics()\n",
        "\n",
        "    # Distribution analysis\n",
        "    distribution_results = enhanced_comparator.analyze_prediction_distributions()\n",
        "\n",
        "    # Calibration quality\n",
        "    calibration_results = enhanced_comparator.evaluate_calibration_quality()\n",
        "\n",
        "    # Disagreement patterns\n",
        "    disagreement_results = enhanced_comparator.analyze_disagreement_patterns()\n",
        "\n",
        "    enhanced_comparator.comparison_results = {\n",
        "        'agreement_metrics': agreement_results,\n",
        "        'distribution_analysis': distribution_results,\n",
        "        'calibration_quality': calibration_results,\n",
        "        'disagreement_patterns': disagreement_results\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Tdeer1W0WwKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Performance Evaluation with Manual Labels\n",
        "\n",
        "class PerformanceEvaluator:\n",
        "    \"\"\"Comprehensive performance evaluation against manual labels.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.evaluation_results = {}\n",
        "\n",
        "        # Filter to manually labeled data\n",
        "        if 'human_label' in df.columns:\n",
        "            self.eval_df = df[df['human_label'].notna()].copy()\n",
        "            print(f\"Performance evaluation on {len(self.eval_df)} manually labeled records\")\n",
        "        else:\n",
        "            self.eval_df = None\n",
        "            print(\"No manual labels available for performance evaluation\")\n",
        "\n",
        "    def evaluate_all_models(self) -> Dict:\n",
        "        \"\"\"Evaluate all available models against manual labels.\"\"\"\n",
        "        if self.eval_df is None or len(self.eval_df) == 0:\n",
        "            return {}\n",
        "\n",
        "        print(\"Evaluating all models against manual labels...\")\n",
        "\n",
        "        evaluation_results = {}\n",
        "\n",
        "        # Identify available models\n",
        "        model_columns = [col for col in self.eval_df.columns if col.endswith('_label')]\n",
        "        model_names = [col.replace('_label', '') for col in model_columns]\n",
        "\n",
        "        for model_name in model_names:\n",
        "            if model_name == 'human':  # Skip human labels\n",
        "                continue\n",
        "\n",
        "            label_col = f\"{model_name}_label\"\n",
        "\n",
        "            if label_col not in self.eval_df.columns:\n",
        "                continue\n",
        "\n",
        "            # Filter valid predictions\n",
        "            valid_mask = self.eval_df[label_col].notna()\n",
        "            valid_eval_df = self.eval_df[valid_mask]\n",
        "\n",
        "            if len(valid_eval_df) == 0:\n",
        "                continue\n",
        "\n",
        "            true_labels = valid_eval_df['human_label'].values\n",
        "            pred_labels = valid_eval_df[label_col].values\n",
        "\n",
        "            # Get confidence scores\n",
        "            confidence_scores = None\n",
        "            score_cols = [f\"{model_name}_calibrated\", f\"{model_name}_score\"]\n",
        "            for score_col in score_cols:\n",
        "                if score_col in valid_eval_df.columns:\n",
        "                    confidence_scores = valid_eval_df[score_col].values\n",
        "                    break\n",
        "\n",
        "            # Calculate comprehensive metrics\n",
        "            model_evaluation = self.calculate_comprehensive_metrics(\n",
        "                true_labels, pred_labels, confidence_scores, model_name\n",
        "            )\n",
        "\n",
        "            evaluation_results[model_name] = model_evaluation\n",
        "\n",
        "            # Print key metrics\n",
        "            print(f\"  {model_name}:\")\n",
        "            print(f\"    Accuracy: {model_evaluation['accuracy']:.3f}\")\n",
        "            print(f\"    F1 (weighted): {model_evaluation['f1_weighted']:.3f}\")\n",
        "            print(f\"    F1 (macro): {model_evaluation['f1_macro']:.3f}\")\n",
        "\n",
        "        return evaluation_results\n",
        "\n",
        "    def calculate_comprehensive_metrics(self, true_labels: np.ndarray, pred_labels: np.ndarray,\n",
        "                                      confidence_scores: np.ndarray = None, model_name: str = \"\") -> Dict:\n",
        "        \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
        "\n",
        "        # Basic metrics\n",
        "        accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        balanced_accuracy = balanced_accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "        # Precision, Recall, F1\n",
        "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "            true_labels, pred_labels, average='macro', zero_division=0\n",
        "        )\n",
        "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "            true_labels, pred_labels, average='weighted', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Per-class metrics\n",
        "        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
        "            true_labels, pred_labels, average=None, zero_division=0\n",
        "        )\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "        # Classification report\n",
        "        class_report = classification_report(true_labels, pred_labels, output_dict=True, zero_division=0)\n",
        "\n",
        "        # Cohen's Kappa\n",
        "        kappa = cohen_kappa_score(true_labels, pred_labels)\n",
        "\n",
        "        evaluation_result = {\n",
        "            'model_name': model_name,\n",
        "            'sample_size': len(true_labels),\n",
        "            'accuracy': accuracy,\n",
        "            'balanced_accuracy': balanced_accuracy,\n",
        "            'precision_macro': precision_macro,\n",
        "            'recall_macro': recall_macro,\n",
        "            'f1_macro': f1_macro,\n",
        "            'precision_weighted': precision_weighted,\n",
        "            'recall_weighted': recall_weighted,\n",
        "            'f1_weighted': f1_weighted,\n",
        "            'cohen_kappa': kappa,\n",
        "            'confusion_matrix': cm.tolist(),\n",
        "            'classification_report': class_report,\n",
        "            'per_class_metrics': {\n",
        "                'precision': precision_per_class.tolist(),\n",
        "                'recall': recall_per_class.tolist(),\n",
        "                'f1': f1_per_class.tolist(),\n",
        "                'support': support.tolist()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Confidence-based metrics\n",
        "        if confidence_scores is not None:\n",
        "            confidence_metrics = self.calculate_confidence_metrics(\n",
        "                true_labels, pred_labels, confidence_scores\n",
        "            )\n",
        "            evaluation_result['confidence_metrics'] = confidence_metrics\n",
        "\n",
        "        return evaluation_result\n",
        "\n",
        "    def calculate_confidence_metrics(self, true_labels: np.ndarray, pred_labels: np.ndarray,\n",
        "                                   confidence_scores: np.ndarray) -> Dict:\n",
        "        \"\"\"Calculate confidence-based evaluation metrics.\"\"\"\n",
        "\n",
        "        correct_predictions = (true_labels == pred_labels)\n",
        "\n",
        "        # Accuracy at different confidence thresholds\n",
        "        confidence_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "        accuracy_at_threshold = {}\n",
        "        coverage_at_threshold = {}\n",
        "\n",
        "        for threshold in confidence_thresholds:\n",
        "            high_conf_mask = confidence_scores >= threshold\n",
        "            if high_conf_mask.sum() > 0:\n",
        "                accuracy_at_threshold[threshold] = correct_predictions[high_conf_mask].mean()\n",
        "                coverage_at_threshold[threshold] = high_conf_mask.mean()\n",
        "            else:\n",
        "                accuracy_at_threshold[threshold] = None\n",
        "                coverage_at_threshold[threshold] = 0.0\n",
        "\n",
        "        # Expected Calibration Error (ECE)\n",
        "        n_bins = 10\n",
        "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_lowers = bin_boundaries[:-1]\n",
        "        bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "        ece = 0\n",
        "        bin_accuracies = []\n",
        "        bin_confidences = []\n",
        "        bin_counts = []\n",
        "\n",
        "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "            in_bin = (confidence_scores > bin_lower) & (confidence_scores <= bin_upper)\n",
        "            prop_in_bin = in_bin.mean()\n",
        "\n",
        "            if prop_in_bin > 0:\n",
        "                accuracy_in_bin = correct_predictions[in_bin].mean()\n",
        "                avg_confidence_in_bin = confidence_scores[in_bin].mean()\n",
        "                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "                bin_accuracies.append(accuracy_in_bin)\n",
        "                bin_confidences.append(avg_confidence_in_bin)\n",
        "                bin_counts.append(in_bin.sum())\n",
        "            else:\n",
        "                bin_accuracies.append(0.0)\n",
        "                bin_confidences.append(0.0)\n",
        "                bin_counts.append(0)\n",
        "\n",
        "        confidence_metrics = {\n",
        "            'accuracy_at_threshold': accuracy_at_threshold,\n",
        "            'coverage_at_threshold': coverage_at_threshold,\n",
        "            'expected_calibration_error': ece,\n",
        "            'average_confidence': confidence_scores.mean(),\n",
        "            'confidence_std': confidence_scores.std(),\n",
        "            'calibration_curve': {\n",
        "                'bin_accuracies': bin_accuracies,\n",
        "                'bin_confidences': bin_confidences,\n",
        "                'bin_counts': bin_counts\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return confidence_metrics\n",
        "\n",
        "    def rank_models_by_performance(self) -> Dict:\n",
        "        \"\"\"Rank models by various performance metrics.\"\"\"\n",
        "        if not self.evaluation_results:\n",
        "            return {}\n",
        "\n",
        "        print(\"Ranking models by performance...\")\n",
        "\n",
        "        # Different ranking criteria\n",
        "        ranking_criteria = ['accuracy', 'f1_weighted', 'f1_macro', 'cohen_kappa', 'balanced_accuracy']\n",
        "\n",
        "        rankings = {}\n",
        "\n",
        "        for criterion in ranking_criteria:\n",
        "            model_scores = []\n",
        "            for model_name, results in self.evaluation_results.items():\n",
        "                if criterion in results:\n",
        "                    model_scores.append((model_name, results[criterion]))\n",
        "\n",
        "            # Sort by score (descending)\n",
        "            model_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "            rankings[criterion] = model_scores\n",
        "\n",
        "            print(f\"\\nRanking by {criterion}:\")\n",
        "            for i, (model_name, score) in enumerate(model_scores[:5]):  # Top 5\n",
        "                print(f\"  {i+1}. {model_name}: {score:.3f}\")\n",
        "\n",
        "        # Overall ranking (average rank across criteria)\n",
        "        model_avg_ranks = {}\n",
        "        for model_name in self.evaluation_results.keys():\n",
        "            ranks = []\n",
        "            for criterion in ranking_criteria:\n",
        "                if criterion in self.evaluation_results[model_name]:\n",
        "                    criterion_ranking = rankings[criterion]\n",
        "                    for rank, (name, _) in enumerate(criterion_ranking):\n",
        "                        if name == model_name:\n",
        "                            ranks.append(rank + 1)  # 1-indexed rank\n",
        "                            break\n",
        "\n",
        "            if ranks:\n",
        "                model_avg_ranks[model_name] = np.mean(ranks)\n",
        "\n",
        "        # Sort by average rank (lower is better)\n",
        "        overall_ranking = sorted(model_avg_ranks.items(), key=lambda x: x[1])\n",
        "\n",
        "        print(f\"\\nOverall ranking (average across criteria):\")\n",
        "        for i, (model_name, avg_rank) in enumerate(overall_ranking):\n",
        "            print(f\"  {i+1}. {model_name}: {avg_rank:.1f}\")\n",
        "\n",
        "        return {\n",
        "            'rankings_by_criterion': rankings,\n",
        "            'overall_ranking': overall_ranking,\n",
        "            'best_model': overall_ranking[0][0] if overall_ranking else None\n",
        "        }\n",
        "\n",
        "# Run performance evaluation\n",
        "performance_evaluator = None\n",
        "performance_evaluation_results = {}\n",
        "\n",
        "if enhanced_sentiment_multi_df is not None:\n",
        "    performance_evaluator = PerformanceEvaluator(enhanced_sentiment_multi_df)\n",
        "    performance_evaluation_results = performance_evaluator.evaluate_all_models()\n",
        "    model_rankings = performance_evaluator.rank_models_by_performance()\n",
        "\n",
        "    performance_evaluator.evaluation_results = performance_evaluation_results\n"
      ],
      "metadata": {
        "id": "S1HclLd2W058"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Financial Context Analysis\n",
        "\n",
        "def analyze_financial_context_performance_enhanced(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Enhanced analysis of model performance in financial context.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(\"\\nEnhanced financial context analysis...\")\n",
        "\n",
        "    # Enhanced financial keyword analysis\n",
        "    financial_indicators = {\n",
        "        'bullish_indicators': [\n",
        "            'growth', 'profit', 'increase', 'strong', 'improved', 'positive',\n",
        "            'beat', 'exceed', 'outperform', 'robust', 'solid', 'expansion'\n",
        "        ],\n",
        "        'bearish_indicators': [\n",
        "            'loss', 'decline', 'decrease', 'weak', 'poor', 'negative',\n",
        "            'miss', 'underperform', 'below', 'concern', 'risk', 'challenge'\n",
        "        ],\n",
        "        'neutral_indicators': [\n",
        "            'stable', 'maintain', 'steady', 'consistent', 'unchanged', 'flat'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Calculate financial sentiment scores\n",
        "    df_analysis = df.copy()\n",
        "\n",
        "    for indicator_type, keywords in financial_indicators.items():\n",
        "        pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
        "        df_analysis[f'{indicator_type}_count'] = df_analysis['text'].str.lower().str.count(pattern)\n",
        "\n",
        "    # Create expected sentiment based on financial indicators\n",
        "    df_analysis['financial_sentiment_score'] = (\n",
        "        df_analysis['bullish_indicators_count'] * 1 +\n",
        "        df_analysis['neutral_indicators_count'] * 0 +\n",
        "        df_analysis['bearish_indicators_count'] * (-1)\n",
        "    )\n",
        "\n",
        "    # Categorize expected sentiment\n",
        "    df_analysis['expected_financial_sentiment'] = 'neutral'\n",
        "    df_analysis.loc[df_analysis['financial_sentiment_score'] > 0, 'expected_financial_sentiment'] = 'positive'\n",
        "    df_analysis.loc[df_analysis['financial_sentiment_score'] < 0, 'expected_financial_sentiment'] = 'negative'\n",
        "\n",
        "    # Analyze model alignment with financial context\n",
        "    financial_context_results = {}\n",
        "\n",
        "    # Identify available models\n",
        "    model_columns = [col for col in df_analysis.columns if col.endswith('_label')]\n",
        "    model_names = [col.replace('_label', '') for col in model_columns if col.replace('_label', '') != 'human']\n",
        "\n",
        "    for model_name in model_names:\n",
        "        label_col = f\"{model_name}_label\"\n",
        "\n",
        "        if label_col not in df_analysis.columns:\n",
        "            continue\n",
        "\n",
        "        valid_mask = (\n",
        "            df_analysis[label_col].notna() &\n",
        "            df_analysis['expected_financial_sentiment'].notna()\n",
        "        )\n",
        "        valid_df = df_analysis[valid_mask]\n",
        "\n",
        "        if len(valid_df) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate alignment with financial context\n",
        "        financial_alignment = (\n",
        "            valid_df[label_col] == valid_df['expected_financial_sentiment']\n",
        "        ).mean()\n",
        "\n",
        "        # Analyze by financial sentiment strength\n",
        "        strong_signals_mask = np.abs(valid_df['financial_sentiment_score']) >= 2\n",
        "        strong_signals_df = valid_df[strong_signals_mask]\n",
        "\n",
        "        strong_signal_alignment = None\n",
        "        if len(strong_signals_df) > 0:\n",
        "            strong_signal_alignment = (\n",
        "                strong_signals_df[label_col] == strong_signals_df['expected_financial_sentiment']\n",
        "            ).mean()\n",
        "\n",
        "        # Topic-specific financial alignment\n",
        "        topic_financial_alignment = {}\n",
        "        if 'primary_topic' in valid_df.columns:\n",
        "            for topic in valid_df['primary_topic'].unique():\n",
        "                topic_mask = valid_df['primary_topic'] == topic\n",
        "                topic_df = valid_df[topic_mask]\n",
        "\n",
        "                if len(topic_df) >= 5:  # Minimum sample size\n",
        "                    topic_alignment = (\n",
        "                        topic_df[label_col] == topic_df['expected_financial_sentiment']\n",
        "                    ).mean()\n",
        "                    topic_financial_alignment[topic] = topic_alignment\n",
        "\n",
        "        financial_context_results[model_name] = {\n",
        "            'overall_financial_alignment': financial_alignment,\n",
        "            'strong_signal_alignment': strong_signal_alignment,\n",
        "            'topic_financial_alignment': topic_financial_alignment,\n",
        "            'sample_size': len(valid_df),\n",
        "            'strong_signals_count': len(strong_signals_df) if strong_signals_df is not None else 0\n",
        "        }\n",
        "\n",
        "        print(f\"  {model_name} financial alignment: {financial_alignment:.3f}\")\n",
        "\n",
        "    return financial_context_results\n",
        "\n",
        "# Run enhanced financial context analysis\n",
        "financial_context_results = {}\n",
        "if enhanced_sentiment_multi_df is not None:\n",
        "    financial_context_results = analyze_financial_context_performance_enhanced(enhanced_sentiment_multi_df)\n"
      ],
      "metadata": {
        "id": "sTlsPSl7W41f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Research Questions Analysis Enhanced\n",
        "\n",
        "def analyze_enhanced_research_questions(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Enhanced analysis of key research questions with statistical rigor.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(\"\\nEnhanced research questions analysis...\")\n",
        "\n",
        "    research_results = {}\n",
        "\n",
        "    # Question 1: Enhanced banker vs analyst sentiment divergence\n",
        "    if 'speaker_role' in df.columns:\n",
        "        print(\"1. Enhanced Speaker Sentiment Divergence Analysis\")\n",
        "\n",
        "        speaker_analysis = {}\n",
        "\n",
        "        # Identify available models\n",
        "        model_columns = [col for col in df.columns if col.endswith('_label')]\n",
        "        model_names = [col.replace('_label', '') for col in model_columns if col.replace('_label', '') != 'human']\n",
        "\n",
        "        for model_name in model_names:\n",
        "            label_col = f\"{model_name}_label\"\n",
        "            score_col = f\"{model_name}_calibrated\" if f\"{model_name}_calibrated\" in df.columns else f\"{model_name}_score\"\n",
        "\n",
        "            if label_col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            # Filter valid data\n",
        "            valid_mask = df[label_col].notna() & df['speaker_role'].notna()\n",
        "            valid_df = df[valid_mask]\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Speaker sentiment distributions\n",
        "            speaker_dist = valid_df.groupby('speaker_role')[label_col].value_counts(normalize=True).unstack(fill_value=0)\n",
        "\n",
        "            # Calculate divergence metrics\n",
        "            if 'analyst' in speaker_dist.index and 'executive' in speaker_dist.index:\n",
        "                analyst_sentiment = speaker_dist.loc['analyst']\n",
        "                exec_sentiment = speaker_dist.loc['executive']\n",
        "\n",
        "                # Statistical test for difference in distributions\n",
        "                try:\n",
        "                    # Chi-square test for independence\n",
        "                    contingency_table = valid_df.groupby(['speaker_role', label_col]).size().unstack(fill_value=0)\n",
        "                    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "                    statistical_test = {\n",
        "                        'test': 'chi_square',\n",
        "                        'statistic': chi2,\n",
        "                        'p_value': p_value,\n",
        "                        'significant': p_value < 0.05\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    statistical_test = {'error': str(e)}\n",
        "\n",
        "                # Total variation distance\n",
        "                tvd = 0.5 * (analyst_sentiment - exec_sentiment).abs().sum()\n",
        "\n",
        "                # Confidence-weighted analysis\n",
        "                confidence_analysis = {}\n",
        "                if score_col in valid_df.columns:\n",
        "                    analyst_df = valid_df[valid_df['speaker_role'] == 'analyst']\n",
        "                    exec_df = valid_df[valid_df['speaker_role'] == 'executive']\n",
        "\n",
        "                    if len(analyst_df) > 0 and len(exec_df) > 0:\n",
        "                        analyst_avg_conf = analyst_df[score_col].mean()\n",
        "                        exec_avg_conf = exec_df[score_col].mean()\n",
        "\n",
        "                        # Weighted sentiment scores\n",
        "                        analyst_weighted_pos = (\n",
        "                            (analyst_df[label_col] == 'positive') * analyst_df[score_col]\n",
        "                        ).sum() / len(analyst_df)\n",
        "\n",
        "                        exec_weighted_pos = (\n",
        "                            (exec_df[label_col] == 'positive') * exec_df[score_col]\n",
        "                        ).sum() / len(exec_df)\n",
        "\n",
        "                        confidence_analysis = {\n",
        "                            'analyst_avg_confidence': analyst_avg_conf,\n",
        "                            'executive_avg_confidence': exec_avg_conf,\n",
        "                            'analyst_weighted_positive': analyst_weighted_pos,\n",
        "                            'executive_weighted_positive': exec_weighted_pos,\n",
        "                            'confidence_weighted_divergence': abs(analyst_weighted_pos - exec_weighted_pos)\n",
        "                        }\n",
        "\n",
        "                speaker_analysis[model_name] = {\n",
        "                    'analyst_distribution': analyst_sentiment.to_dict(),\n",
        "                    'executive_distribution': exec_sentiment.to_dict(),\n",
        "                    'total_variation_distance': tvd,\n",
        "                    'statistical_test': statistical_test,\n",
        "                    'confidence_analysis': confidence_analysis\n",
        "                }\n",
        "\n",
        "        research_results['speaker_divergence'] = speaker_analysis\n",
        "\n",
        "    # Question 2: Enhanced temporal sentiment analysis\n",
        "    if 'quarter' in df.columns:\n",
        "        print(\"2. Enhanced Temporal Sentiment Analysis\")\n",
        "\n",
        "        temporal_analysis = {}\n",
        "\n",
        "        for model_name in model_names:\n",
        "            label_col = f\"{model_name}_label\"\n",
        "            score_col = f\"{model_name}_calibrated\" if f\"{model_name}_calibrated\" in df.columns else f\"{model_name}_score\"\n",
        "\n",
        "            if label_col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            valid_mask = df[label_col].notna() & df['quarter'].notna()\n",
        "            valid_df = df[valid_mask]\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Temporal sentiment shifts\n",
        "            quarter_sentiment = valid_df.groupby('quarter')[label_col].value_counts(normalize=True).unstack(fill_value=0)\n",
        "\n",
        "            if len(quarter_sentiment) >= 2:\n",
        "                quarters = sorted(quarter_sentiment.index)\n",
        "                q1_sentiment = quarter_sentiment.loc[quarters[0]]\n",
        "                q2_sentiment = quarter_sentiment.loc[quarters[-1]]\n",
        "\n",
        "                # Calculate sentiment shift magnitude\n",
        "                sentiment_shift = q2_sentiment - q1_sentiment\n",
        "                shift_magnitude = sentiment_shift.abs().sum()\n",
        "\n",
        "                # Confidence-weighted temporal analysis\n",
        "                confidence_temporal = {}\n",
        "                if score_col in valid_df.columns:\n",
        "                    q1_df = valid_df[valid_df['quarter'] == quarters[0]]\n",
        "                    q2_df = valid_df[valid_df['quarter'] == quarters[-1]]\n",
        "\n",
        "                    if len(q1_df) > 0 and len(q2_df) > 0:\n",
        "                        q1_avg_conf = q1_df[score_col].mean()\n",
        "                        q2_avg_conf = q2_df[score_col].mean()\n",
        "\n",
        "                        confidence_temporal = {\n",
        "                            'q1_avg_confidence': q1_avg_conf,\n",
        "                            'q2_avg_confidence': q2_avg_conf,\n",
        "                            'confidence_change': q2_avg_conf - q1_avg_conf\n",
        "                        }\n",
        "\n",
        "                temporal_analysis[model_name] = {\n",
        "                    'q1_distribution': q1_sentiment.to_dict(),\n",
        "                    'q2_distribution': q2_sentiment.to_dict(),\n",
        "                    'sentiment_shift': sentiment_shift.to_dict(),\n",
        "                    'shift_magnitude': shift_magnitude,\n",
        "                    'confidence_temporal': confidence_temporal\n",
        "                }\n",
        "\n",
        "        research_results['temporal_shifts'] = temporal_analysis\n",
        "\n",
        "    # Question 3: Enhanced model consistency across contexts\n",
        "    print(\"3. Enhanced Model Consistency Analysis\")\n",
        "\n",
        "    consistency_analysis = {}\n",
        "\n",
        "    # Agreement rates across different contexts\n",
        "    if len(model_names) >= 2:\n",
        "        model_pairs = list(itertools.combinations(model_names, 2))\n",
        "\n",
        "        for model1, model2 in model_pairs:\n",
        "            pair_name = f\"{model1}_vs_{model2}\"\n",
        "\n",
        "            label_col1 = f\"{model1}_label\"\n",
        "            label_col2 = f\"{model2}_label\"\n",
        "\n",
        "            if label_col1 not in df.columns or label_col2 not in df.columns:\n",
        "                continue\n",
        "\n",
        "            valid_mask = df[label_col1].notna() & df[label_col2].notna()\n",
        "            valid_df = df[valid_mask]\n",
        "\n",
        "            if len(valid_df) == 0:\n",
        "                continue\n",
        "\n",
        "            pair_analysis = {}\n",
        "\n",
        "            # Overall agreement\n",
        "            overall_agreement = (valid_df[label_col1] == valid_df[label_col2]).mean()\n",
        "            pair_analysis['overall_agreement'] = overall_agreement\n",
        "\n",
        "            # Agreement by context\n",
        "            context_columns = ['speaker_role', 'primary_topic', 'quarter']\n",
        "\n",
        "            for context_col in context_columns:\n",
        "                if context_col in valid_df.columns:\n",
        "                    context_agreement = {}\n",
        "\n",
        "                    for context_value in valid_df[context_col].unique():\n",
        "                        context_mask = valid_df[context_col] == context_value\n",
        "                        context_df = valid_df[context_mask]\n",
        "\n",
        "                        if len(context_df) >= 5:  # Minimum sample size\n",
        "                            agreement_rate = (context_df[label_col1] == context_df[label_col2]).mean()\n",
        "                            context_agreement[str(context_value)] = agreement_rate\n",
        "\n",
        "                    pair_analysis[f'agreement_by_{context_col}'] = context_agreement\n",
        "\n",
        "            consistency_analysis[pair_name] = pair_analysis\n",
        "\n",
        "    research_results['model_consistency'] = consistency_analysis\n",
        "\n",
        "    return research_results\n",
        "\n",
        "# Run enhanced research questions analysis\n",
        "research_questions_results = {}\n",
        "if enhanced_sentiment_multi_df is not None:\n",
        "    research_questions_results = analyze_enhanced_research_questions(enhanced_sentiment_multi_df)\n"
      ],
      "metadata": {
        "id": "9LPESGY2W9h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Enhanced Comparison Results\n",
        "\n",
        "def save_enhanced_comparison_results():\n",
        "    \"\"\"Save all enhanced comparison results.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAVING ENHANCED COMPARISON RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Compile comprehensive results\n",
        "    comprehensive_results = {\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'bank_code': BANK_CODE,\n",
        "        'enhanced_features': True,\n",
        "        'fine_tuned_models_included': bool(finetuning_results),\n",
        "        'analysis_components': {\n",
        "            'enhanced_model_comparison': enhanced_comparator.comparison_results if enhanced_comparator else {},\n",
        "            'performance_evaluation': performance_evaluation_results,\n",
        "            'model_rankings': model_rankings if 'model_rankings' in locals() else {},\n",
        "            'financial_context_analysis': financial_context_results,\n",
        "            'research_questions_analysis': research_questions_results,\n",
        "            'fine_tuning_results': finetuning_results\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save main results\n",
        "    main_results_path = results_comparison_path / \"enhanced_model_comparison_results.json\"\n",
        "    with open(main_results_path, 'w') as f:\n",
        "        json.dump(comprehensive_results, f, indent=2, default=str)\n",
        "    print(f\"Enhanced comparison results: {main_results_path}\")\n",
        "\n",
        "    # Save performance summary\n",
        "    if performance_evaluation_results:\n",
        "        performance_summary = {}\n",
        "        for model_name, results in performance_evaluation_results.items():\n",
        "            performance_summary[model_name] = {\n",
        "                'accuracy': results.get('accuracy', 0),\n",
        "                'f1_weighted': results.get('f1_weighted', 0),\n",
        "                'f1_macro': results.get('f1_macro', 0),\n",
        "                'sample_size': results.get('sample_size', 0)\n",
        "            }\n",
        "\n",
        "        performance_path = results_comparison_path / \"enhanced_performance_summary.json\"\n",
        "        with open(performance_path, 'w') as f:\n",
        "            json.dump(performance_summary, f, indent=2, default=str)\n",
        "        print(f\"Performance summary: {performance_path}\")\n",
        "\n",
        "    # Save best model recommendation\n",
        "    if 'model_rankings' in locals() and model_rankings.get('best_model'):\n",
        "        best_model_recommendation = {\n",
        "            'best_model': model_rankings['best_model'],\n",
        "            'ranking_criteria': list(model_rankings.get('rankings_by_criterion', {}).keys()),\n",
        "            'performance_metrics': performance_evaluation_results.get(model_rankings['best_model'], {}),\n",
        "            'recommendation_timestamp': pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "\n",
        "        recommendation_path = results_comparison_path / \"best_model_recommendation.json\"\n",
        "        with open(recommendation_path, 'w') as f:\n",
        "            json.dump(best_model_recommendation, f, indent=2, default=str)\n",
        "        print(f\"Best model recommendation: {recommendation_path}\")\n",
        "\n",
        "# Save all results\n",
        "save_enhanced_comparison_results()\n"
      ],
      "metadata": {
        "id": "C6M8ZxKMXCSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Summary and Insights\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENHANCED MODEL COMPARISON COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Performance summary\n",
        "if performance_evaluation_results:\n",
        "    print(f\"\\nModel Performance Summary:\")\n",
        "    print(f\"{'Model':<20} {'Accuracy':<10} {'F1-Weighted':<12} {'F1-Macro':<10} {'Samples':<8}\")\n",
        "    print(f\"{'-'*60}\")\n",
        "\n",
        "    for model_name, results in performance_evaluation_results.items():\n",
        "        accuracy = results.get('accuracy', 0)\n",
        "        f1_weighted = results.get('f1_weighted', 0)\n",
        "        f1_macro = results.get('f1_macro', 0)\n",
        "        sample_size = results.get('sample_size', 0)\n",
        "\n",
        "        print(f\"{model_name:<20} {accuracy:<10.3f} {f1_weighted:<12.3f} {f1_macro:<10.3f} {sample_size:<8}\")\n",
        "\n",
        "# Best model summary\n",
        "if 'model_rankings' in locals() and model_rankings.get('best_model'):\n",
        "    best_model = model_rankings['best_model']\n",
        "    print(f\"\\nBest Performing Model: {best_model}\")\n",
        "\n",
        "    if best_model in performance_evaluation_results:\n",
        "        best_results = performance_evaluation_results[best_model]\n",
        "        print(f\"  Accuracy: {best_results.get('accuracy', 0):.3f}\")\n",
        "        print(f\"  F1-Weighted: {best_results.get('f1_weighted', 0):.3f}\")\n",
        "        print(f\"  F1-Macro: {best_results.get('f1_macro', 0):.3f}\")\n",
        "        print(f\"  Cohen's Kappa: {best_results.get('cohen_kappa', 0):.3f}\")\n",
        "\n",
        "# Model agreement summary\n",
        "if enhanced_comparator and enhanced_comparator.comparison_results.get('agreement_metrics'):\n",
        "    agreement_results = enhanced_comparator.comparison_results['agreement_metrics']\n",
        "    print(f\"\\nModel Agreement Summary:\")\n",
        "\n",
        "    for pair_name, metrics in agreement_results.items():\n",
        "        agreement_rate = metrics.get('agreement_rate', 0)\n",
        "        kappa = metrics.get('cohen_kappa', 0)\n",
        "        print(f\"  {pair_name}: Agreement={agreement_rate:.3f}, Kappa={kappa:.3f}\")\n",
        "\n",
        "# Financial context insights\n",
        "if financial_context_results:\n",
        "    print(f\"\\nFinancial Context Alignment:\")\n",
        "    for model_name, results in financial_context_results.items():\n",
        "        alignment = results.get('overall_financial_alignment', 0)\n",
        "        sample_size = results.get('sample_size', 0)\n",
        "        print(f\"  {model_name}: {alignment:.3f} ({sample_size} samples)\")\n",
        "\n",
        "# Research questions insights\n",
        "if research_questions_results:\n",
        "    print(f\"\\nKey Research Insights:\")\n",
        "\n",
        "    # Speaker divergence\n",
        "    if 'speaker_divergence' in research_questions_results:\n",
        "        print(f\"  Speaker Sentiment Divergence:\")\n",
        "        for model_name, analysis in research_questions_results['speaker_divergence'].items():\n",
        "            tvd = analysis.get('total_variation_distance', 0)\n",
        "            print(f\"    {model_name}: TVD={tvd:.3f}\")\n",
        "\n",
        "    # Temporal shifts\n",
        "    if 'temporal_shifts' in research_questions_results:\n",
        "        print(f\"  Temporal Sentiment Shifts:\")\n",
        "        for model_name, analysis in research_questions_results['temporal_shifts'].items():\n",
        "            shift_magnitude = analysis.get('shift_magnitude', 0)\n",
        "            print(f\"    {model_name}: Shift Magnitude={shift_magnitude:.3f}\")\n",
        "\n",
        "# Fine-tuning impact\n",
        "if finetuning_results and 'model_comparison' in finetuning_results:\n",
        "    print(f\"\\nFine-tuning Impact:\")\n",
        "    for model_name, comparison in finetuning_results['model_comparison'].items():\n",
        "        if 'f1_improvement' in comparison:\n",
        "            improvement = comparison['f1_improvement']\n",
        "            print(f\"  {model_name}: F1 improvement = +{improvement:.3f}\")\n",
        "\n",
        "print(f\"\\nEnhanced comparison analysis saved to: {results_comparison_path}\")\n",
        "print(f\"Ready for enhanced visualization in 06_results_visualization_jpm_enhanced.ipynb\")\n"
      ],
      "metadata": {
        "id": "Y7kEMYAaXJgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Export Key Metrics for Visualization\n",
        "\n",
        "enhanced_viz_metrics = {\n",
        "    'performance_summary': {},\n",
        "    'model_agreement': {},\n",
        "    'financial_context': {},\n",
        "    'research_insights': {},\n",
        "    'fine_tuning_impact': {}\n",
        "}\n",
        "\n",
        "# Performance metrics\n",
        "if performance_evaluation_results:\n",
        "    enhanced_viz_metrics['performance_summary'] = {\n",
        "        model_name: {\n",
        "            'accuracy': results.get('accuracy', 0),\n",
        "            'f1_weighted': results.get('f1_weighted', 0),\n",
        "            'f1_macro': results.get('f1_macro', 0),\n",
        "            'sample_size': results.get('sample_size', 0)\n",
        "        }\n",
        "        for model_name, results in performance_evaluation_results.items()\n",
        "    }\n",
        "\n",
        "# Model agreement\n",
        "if enhanced_comparator and enhanced_comparator.comparison_results.get('agreement_metrics'):\n",
        "    enhanced_viz_metrics['model_agreement'] = {\n",
        "        pair_name: {\n",
        "            'agreement_rate': metrics.get('agreement_rate', 0),\n",
        "            'cohen_kappa': metrics.get('cohen_kappa', 0)\n",
        "        }\n",
        "        for pair_name, metrics in enhanced_comparator.comparison_results['agreement_metrics'].items()\n",
        "    }\n",
        "\n",
        "# Financial context\n",
        "if financial_context_results:\n",
        "    enhanced_viz_metrics['financial_context'] = {\n",
        "        model_name: results.get('overall_financial_alignment', 0)\n",
        "        for model_name, results in financial_context_results.items()\n",
        "    }\n",
        "\n",
        "# Best model info\n",
        "if 'model_rankings' in locals() and model_rankings.get('best_model'):\n",
        "    enhanced_viz_metrics['best_model'] = {\n",
        "        'name': model_rankings['best_model'],\n",
        "        'metrics': performance_evaluation_results.get(model_rankings['best_model'], {})\n",
        "    }\n",
        "\n",
        "# Save visualization metrics\n",
        "viz_metrics_path = results_comparison_path / \"enhanced_viz_metrics.json\"\n",
        "with open(viz_metrics_path, 'w') as f:\n",
        "    json.dump(enhanced_viz_metrics, f, indent=2, default=str)\n",
        "\n",
        "print(f\"Enhanced visualization metrics saved: {viz_metrics_path}\")\n",
        "print(f\"\\nEnhanced model comparison analysis complete!\")"
      ],
      "metadata": {
        "id": "MMWJEAZYXOov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}