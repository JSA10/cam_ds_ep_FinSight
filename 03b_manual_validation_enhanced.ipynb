{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRzSzPOeD1IS",
        "outputId": "0fe9184e-cc87-4fe6-c8d1-fe3571581820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Enhanced manual validation for banks: JPM, HSBC\n",
            "Target models: 4 models for validation\n"
          ]
        }
      ],
      "source": [
        "# 03b_manual_validation.ipynb\n",
        "# Purpose: Enhanced manual validation for 4 models across JP Morgan and HSBC datasets\n",
        "# Banks: JP Morgan (JPM) and HSBC\n",
        "# Models: FinBERT (yiyanghkust), FinBERT (ProsusAI), DistilRoBERTa, CardiffNLP (Twitter-RoBERTa)\n",
        "# Input: Manual labels for both banks + processed datasets\n",
        "# Output: Validated manual labels for 4-model fine-tuning\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import io\n",
        "import csv\n",
        "import tempfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load enhanced configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/configs/enhanced_config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    enhanced_config = json.load(f)\n",
        "\n",
        "SEED = enhanced_config[\"SEED\"]\n",
        "BANKS = enhanced_config[\"BANKS\"]\n",
        "QUARTERS = enhanced_config[\"QUARTERS\"]\n",
        "MODELS = enhanced_config[\"MODELS\"]\n",
        "drive_base = Path(enhanced_config[\"drive_base\"])\n",
        "colab_base = Path(enhanced_config[\"colab_base\"])\n",
        "data_urls = enhanced_config[\"data_urls\"]\n",
        "\n",
        "print(f\"Enhanced manual validation for banks: {', '.join([bank.upper() for bank in BANKS])}\")\n",
        "print(f\"Target models: {len(MODELS)} models for validation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Enhanced Paths\n",
        "\n",
        "manual_validation_paths = {}\n",
        "for bank in BANKS:\n",
        "    manual_validation_paths[bank] = {\n",
        "        \"manual_validation\": drive_base / f\"data/manual_validation/{bank}\",\n",
        "        \"processed_data\": drive_base / f\"data/processed/{bank}\",\n",
        "        \"results_sentiment\": drive_base / f\"results/sentiment/{bank}\"\n",
        "    }\n",
        "\n",
        "    # Ensure directories exist\n",
        "    for path in manual_validation_paths[bank].values():\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "## Enhanced Manual Labels Download\n",
        "\n",
        "def extract_file_id_from_drive_url(url: str) -> str:\n",
        "    \"\"\"Extract file ID from Google Drive sharing URL.\"\"\"\n",
        "    if \"drive.google.com\" in url and \"/file/d/\" in url:\n",
        "        return url.split(\"/file/d/\")[1].split(\"/\")[0]\n",
        "    return None\n",
        "\n",
        "def download_manual_labels_enhanced(bank_code: str) -> Path:\n",
        "    \"\"\"Download manually labeled dataset for specific bank.\"\"\"\n",
        "    print(f\"\\nüì• Downloading {bank_code.upper()} manual labels...\")\n",
        "\n",
        "    if bank_code not in data_urls or \"manual_labels\" not in data_urls[bank_code]:\n",
        "        print(f\"‚ùå No manual labels URL configured for {bank_code.upper()}\")\n",
        "        return None\n",
        "\n",
        "    manual_labels_url = data_urls[bank_code][\"manual_labels\"]\n",
        "    file_id = extract_file_id_from_drive_url(manual_labels_url)\n",
        "\n",
        "    if not file_id:\n",
        "        print(f\"‚ùå Could not extract file ID from URL for {bank_code.upper()}\")\n",
        "        return None\n",
        "\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    filename = f\"manual_labels_{bank_code}_multi_2025.csv\"\n",
        "\n",
        "    try:\n",
        "        print(f\"  Downloading from Google Drive...\")\n",
        "        response = requests.get(download_url, timeout=300)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save to manual validation directory\n",
        "        manual_file_path = manual_validation_paths[bank_code][\"manual_validation\"] / filename\n",
        "        with open(manual_file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"  ‚úÖ Downloaded: {manual_file_path}\")\n",
        "        return manual_file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error downloading manual labels for {bank_code.upper()}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Download manual labels for all banks\n",
        "manual_label_paths = {}\n",
        "for bank in BANKS:\n",
        "    manual_label_paths[bank] = download_manual_labels_enhanced(bank)\n"
      ],
      "metadata": {
        "id": "4fioBAmFKLEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dba1d5c-5681-4a80-ccbc-ad05dd475bc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì• Downloading JPM manual labels...\n",
            "  Downloading from Google Drive...\n",
            "  ‚úÖ Downloaded: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/jpm/manual_labels_jpm_multi_2025.csv\n",
            "\n",
            "üì• Downloading HSBC manual labels...\n",
            "  Downloading from Google Drive...\n",
            "  ‚úÖ Downloaded: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/hsbc/manual_labels_hsbc_multi_2025.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Manual Labels Loading\n",
        "\n",
        "def load_manual_labels_enhanced(file_path: Path, bank_code: str) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced loading of manually labeled data with robust error handling.\"\"\"\n",
        "    if not file_path or not file_path.exists():\n",
        "        print(f\"‚ùå Manual labels file not found for {bank_code.upper()}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüìÇ Loading {bank_code.upper()} manual labels...\")\n",
        "\n",
        "    try:\n",
        "        # Multiple approaches for robust CSV loading\n",
        "        approaches = [\n",
        "            # Standard approach\n",
        "            lambda: pd.read_csv(file_path, encoding='utf-8'),\n",
        "\n",
        "            # Skip bad lines\n",
        "            lambda: pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip'),\n",
        "\n",
        "            # Alternative encodings\n",
        "            lambda: pd.read_csv(file_path, encoding='latin-1'),\n",
        "            lambda: pd.read_csv(file_path, encoding='cp1252'),\n",
        "\n",
        "            # Flexible quoting\n",
        "            lambda: pd.read_csv(file_path, encoding='utf-8', quoting=csv.QUOTE_ALL, on_bad_lines='skip')\n",
        "        ]\n",
        "\n",
        "        for i, approach in enumerate(approaches):\n",
        "            try:\n",
        "                df = approach()\n",
        "                print(f\"  ‚úÖ Loaded with approach {i+1}: {df.shape}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"  Approach {i+1} failed: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Manual cleanup approach\n",
        "        print(f\"  Attempting manual cleanup...\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        print(f\"  File has {len(lines)} lines\")\n",
        "\n",
        "        # Identify expected columns from header\n",
        "        if lines:\n",
        "            header_line = lines[0].strip()\n",
        "            expected_columns = len(header_line.split(','))\n",
        "            print(f\"  Expected columns: {expected_columns}\")\n",
        "\n",
        "            # Clean problematic lines\n",
        "            cleaned_lines = [lines[0]]  # Keep header\n",
        "\n",
        "            for i, line in enumerate(lines[1:], 1):\n",
        "                field_count = line.count(',') + 1\n",
        "                if field_count <= expected_columns * 1.5:  # Allow some tolerance\n",
        "                    cleaned_lines.append(line)\n",
        "                else:\n",
        "                    print(f\"    Skipping line {i+1} with {field_count} fields\")\n",
        "\n",
        "            # Save cleaned version and load\n",
        "            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8') as tmp_file:\n",
        "                tmp_file.writelines(cleaned_lines)\n",
        "                tmp_path = tmp_file.name\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(tmp_path, encoding='utf-8')\n",
        "                os.unlink(tmp_path)  # Clean up\n",
        "                print(f\"  ‚úÖ Manual cleanup successful: {df.shape}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                os.unlink(tmp_path)\n",
        "                print(f\"  ‚ùå Manual cleanup failed: {e}\")\n",
        "\n",
        "        print(f\"  ‚ùå All loading approaches failed for {bank_code.upper()}\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading manual labels for {bank_code.upper()}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Load manual labels for all banks\n",
        "manual_labels_dfs = {}\n",
        "for bank in BANKS:\n",
        "    if manual_label_paths[bank]:\n",
        "        manual_labels_dfs[bank] = load_manual_labels_enhanced(manual_label_paths[bank], bank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFj5YAIbKQE7",
        "outputId": "1b096398-4e7f-4753-b6f0-00ab11895680"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÇ Loading JPM manual labels...\n",
            "  ‚úÖ Loaded with approach 1: (1121, 27)\n",
            "\n",
            "üìÇ Loading HSBC manual labels...\n",
            "  ‚úÖ Loaded with approach 1: (858, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Manual Labels Validation\n",
        "\n",
        "def validate_manual_labels_enhanced(df: pd.DataFrame, bank_code: str) -> Dict:\n",
        "    \"\"\"Enhanced validation of manual labels structure and quality.\"\"\"\n",
        "    if df is None:\n",
        "        return {\"error\": f\"No manual labels data for {bank_code.upper()}\"}\n",
        "\n",
        "    print(f\"\\nüîç [{bank_code.upper()}] ENHANCED MANUAL LABELS VALIDATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    validation_report = {\n",
        "        'bank_code': bank_code,\n",
        "        'total_records': len(df),\n",
        "        'manually_labeled_count': 0,\n",
        "        'label_distribution': {},\n",
        "        'confidence_stats': {},\n",
        "        'annotator_info': {},\n",
        "        'data_quality': {},\n",
        "        'model_compatibility': {}\n",
        "    }\n",
        "\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Check for required columns with alternatives\n",
        "    required_columns = ['human_label', 'human_confidence', 'annotation_notes', 'annotator_id']\n",
        "    missing_columns = []\n",
        "    found_alternatives = {}\n",
        "\n",
        "    # Alternative column mappings\n",
        "    alternative_mappings = {\n",
        "        'human_label': ['label', 'sentiment', 'manual_label', 'sentiment_label'],\n",
        "        'human_confidence': ['confidence', 'manual_confidence', 'certainty', 'score'],\n",
        "        'annotation_notes': ['notes', 'comments', 'remarks', 'annotation_comment'],\n",
        "        'annotator_id': ['annotator', 'user_id', 'reviewer', 'labeler_id']\n",
        "    }\n",
        "\n",
        "    for req_col in required_columns:\n",
        "        if req_col not in df.columns:\n",
        "            # Look for alternatives\n",
        "            for alt_col in alternative_mappings.get(req_col, []):\n",
        "                if alt_col in df.columns:\n",
        "                    found_alternatives[req_col] = alt_col\n",
        "                    df = df.rename(columns={alt_col: req_col})\n",
        "                    print(f\"  Mapped '{alt_col}' ‚Üí '{req_col}'\")\n",
        "                    break\n",
        "            else:\n",
        "                missing_columns.append(req_col)\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"  ‚ö†Ô∏è Missing columns: {missing_columns}\")\n",
        "        validation_report['missing_columns'] = missing_columns\n",
        "\n",
        "    # Count manually labeled records\n",
        "    if 'human_label' in df.columns:\n",
        "        manually_labeled_mask = df['human_label'].notna() & (df['human_label'] != '')\n",
        "        manually_labeled_df = df[manually_labeled_mask].copy()\n",
        "        validation_report['manually_labeled_count'] = len(manually_labeled_df)\n",
        "\n",
        "        print(f\"Manually labeled: {len(manually_labeled_df):,} ({len(manually_labeled_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "        if len(manually_labeled_df) > 0:\n",
        "            # Label distribution analysis\n",
        "            label_dist = manually_labeled_df['human_label'].value_counts()\n",
        "            validation_report['label_distribution'] = label_dist.to_dict()\n",
        "\n",
        "            print(f\"Label distribution:\")\n",
        "            for label, count in label_dist.items():\n",
        "                pct = (count / len(manually_labeled_df)) * 100\n",
        "                print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "            # Enhanced label validation for 4 models\n",
        "            expected_labels = ['positive', 'negative', 'neutral']\n",
        "            valid_labels = set(manually_labeled_df['human_label'].unique())\n",
        "            unexpected_labels = valid_labels - set(expected_labels)\n",
        "\n",
        "            if unexpected_labels:\n",
        "                print(f\"  ‚ö†Ô∏è Unexpected labels found: {unexpected_labels}\")\n",
        "                validation_report['unexpected_labels'] = list(unexpected_labels)\n",
        "\n",
        "            # Model compatibility check\n",
        "            for model_key in MODELS.keys():\n",
        "                model_compatible = len(valid_labels.intersection(expected_labels)) > 0\n",
        "                validation_report['model_compatibility'][model_key] = {\n",
        "                    'compatible': model_compatible,\n",
        "                    'supported_labels': list(valid_labels.intersection(expected_labels))\n",
        "                }\n",
        "\n",
        "            # Confidence analysis\n",
        "            if 'human_confidence' in manually_labeled_df.columns:\n",
        "                confidence_scores = manually_labeled_df['human_confidence'].dropna()\n",
        "                if len(confidence_scores) > 0:\n",
        "                    validation_report['confidence_stats'] = {\n",
        "                        'count': len(confidence_scores),\n",
        "                        'mean': confidence_scores.mean(),\n",
        "                        'std': confidence_scores.std(),\n",
        "                        'min': confidence_scores.min(),\n",
        "                        'max': confidence_scores.max(),\n",
        "                        'median': confidence_scores.median()\n",
        "                    }\n",
        "\n",
        "                    print(f\"Confidence scores:\")\n",
        "                    print(f\"  Count: {len(confidence_scores)}\")\n",
        "                    print(f\"  Mean: {confidence_scores.mean():.3f}\")\n",
        "                    print(f\"  Range: {confidence_scores.min():.3f} - {confidence_scores.max():.3f}\")\n",
        "\n",
        "                    # Confidence quality assessment\n",
        "                    high_confidence = (confidence_scores >= 0.8).sum()\n",
        "                    low_confidence = (confidence_scores < 0.5).sum()\n",
        "\n",
        "                    print(f\"  High confidence (‚â•0.8): {high_confidence} ({high_confidence/len(confidence_scores)*100:.1f}%)\")\n",
        "                    print(f\"  Low confidence (<0.5): {low_confidence} ({low_confidence/len(confidence_scores)*100:.1f}%)\")\n",
        "\n",
        "            # Annotator analysis\n",
        "            if 'annotator_id' in manually_labeled_df.columns:\n",
        "                annotator_dist = manually_labeled_df['annotator_id'].value_counts()\n",
        "                validation_report['annotator_info'] = {\n",
        "                    'unique_annotators': len(annotator_dist),\n",
        "                    'annotations_per_annotator': annotator_dist.to_dict()\n",
        "                }\n",
        "\n",
        "                print(f\"Annotator information:\")\n",
        "                print(f\"  Unique annotators: {len(annotator_dist)}\")\n",
        "                for annotator, count in annotator_dist.head().items():\n",
        "                    print(f\"  {annotator}: {count} annotations\")\n",
        "\n",
        "                # Inter-annotator reliability (if multiple annotators)\n",
        "                if len(annotator_dist) > 1:\n",
        "                    print(f\"  Multi-annotator dataset detected\")\n",
        "                    validation_report['multi_annotator'] = True\n",
        "\n",
        "    # Data quality assessment\n",
        "    quality_issues = []\n",
        "\n",
        "    # Missing confidence scores\n",
        "    if 'human_confidence' in df.columns:\n",
        "        missing_confidence = df['human_confidence'].isna().sum()\n",
        "        if missing_confidence > 0:\n",
        "            quality_issues.append(f\"Missing confidence: {missing_confidence}\")\n",
        "\n",
        "    # Missing annotation notes\n",
        "    if 'annotation_notes' in df.columns:\n",
        "        missing_notes = df['annotation_notes'].isna().sum()\n",
        "        if missing_notes > 0:\n",
        "            quality_issues.append(f\"Missing notes: {missing_notes}\")\n",
        "\n",
        "    # Text matching capability\n",
        "    text_columns = ['text', 'content', 'sentence', 'statement']\n",
        "    text_col_found = None\n",
        "    for col in text_columns:\n",
        "        if col in df.columns:\n",
        "            text_col_found = col\n",
        "            break\n",
        "\n",
        "    if text_col_found:\n",
        "        validation_report['text_column'] = text_col_found\n",
        "        print(f\"  Text column found: {text_col_found}\")\n",
        "    else:\n",
        "        quality_issues.append(\"No text column found for matching\")\n",
        "\n",
        "    validation_report['data_quality']['issues'] = quality_issues\n",
        "\n",
        "    if quality_issues:\n",
        "        print(f\"Data quality issues:\")\n",
        "        for issue in quality_issues:\n",
        "            print(f\"  ‚ö†Ô∏è {issue}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ No major data quality issues\")\n",
        "\n",
        "    return validation_report\n",
        "\n",
        "# Validate manual labels for all banks\n",
        "validation_reports = {}\n",
        "for bank in BANKS:\n",
        "    if bank in manual_labels_dfs and manual_labels_dfs[bank] is not None:\n",
        "        validation_reports[bank] = validate_manual_labels_enhanced(manual_labels_dfs[bank], bank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHgeRdxBKUtg",
        "outputId": "8dd01990-1833-4abe-fc10-2fc457bd3cc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç [JPM] ENHANCED MANUAL LABELS VALIDATION\n",
            "--------------------------------------------------\n",
            "Total records: 1,121\n",
            "Columns: ['original_qa_id', 'sentence_id', 'text', 'speaker', 'speaker_role', 'quarter', 'bank_code', 'dataset_id', 'sentence_length', 'sentence_word_count', 'sentence_index_in_qa', 'text_quality_score', 'financial_relevance_score', 'has_revenue_mentions', 'has_risk_mentions', 'has_growth_mentions', 'has_performance_mentions', 'ready_for_finbert', 'ready_for_distilroberta', 'ready_for_cardiffnlp', 'has_financial_numbers', 'has_financial_entities', 'sentence_complexity', 'human_label', 'human_confidence', 'annotation_notes', 'annotator_id']\n",
            "Manually labeled: 49 (4.4%)\n",
            "Label distribution:\n",
            "  neutral: 19 (38.8%)\n",
            "  positive: 19 (38.8%)\n",
            "  negative: 11 (22.4%)\n",
            "Confidence scores:\n",
            "  Count: 49\n",
            "  Mean: 3.939\n",
            "  Range: 2.000 - 5.000\n",
            "  High confidence (‚â•0.8): 49 (100.0%)\n",
            "  Low confidence (<0.5): 0 (0.0%)\n",
            "Annotator information:\n",
            "  Unique annotators: 1\n",
            "  Annotator 1: 49 annotations\n",
            "  Text column found: text\n",
            "Data quality issues:\n",
            "  ‚ö†Ô∏è Missing confidence: 1072\n",
            "  ‚ö†Ô∏è Missing notes: 1120\n",
            "\n",
            "üîç [HSBC] ENHANCED MANUAL LABELS VALIDATION\n",
            "--------------------------------------------------\n",
            "Total records: 858\n",
            "Columns: ['original_qa_id', 'sentence_id', 'text', 'speaker', 'speaker_role', 'quarter', 'bank_code', 'dataset_id', 'sentence_length', 'sentence_word_count', 'sentence_index_in_qa', 'text_quality_score', 'financial_relevance_score', 'has_revenue_mentions', 'has_risk_mentions', 'has_growth_mentions', 'has_performance_mentions', 'ready_for_finbert', 'ready_for_distilroberta', 'ready_for_cardiffnlp', 'has_financial_numbers', 'has_financial_entities', 'sentence_complexity', 'human_label', 'human_confidence', 'annotation_notes', 'annotator_id']\n",
            "Manually labeled: 69 (8.0%)\n",
            "Label distribution:\n",
            "  positive: 39 (56.5%)\n",
            "  neutral: 20 (29.0%)\n",
            "  negative: 10 (14.5%)\n",
            "Confidence scores:\n",
            "  Count: 69\n",
            "  Mean: 4.391\n",
            "  Range: 2.000 - 5.000\n",
            "  High confidence (‚â•0.8): 69 (100.0%)\n",
            "  Low confidence (<0.5): 0 (0.0%)\n",
            "Annotator information:\n",
            "  Unique annotators: 1\n",
            "  Annotator 1: 69 annotations\n",
            "  Text column found: text\n",
            "Data quality issues:\n",
            "  ‚ö†Ô∏è Missing confidence: 789\n",
            "  ‚ö†Ô∏è Missing notes: 855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Model Comparison Analysis\n",
        "\n",
        "def compare_manual_vs_model_predictions_enhanced(bank_code: str) -> Dict:\n",
        "    \"\"\"Enhanced comparison of manual labels with existing model predictions for 4 models.\"\"\"\n",
        "    if bank_code not in manual_labels_dfs or manual_labels_dfs[bank_code] is None:\n",
        "        return {\"error\": f\"No manual labels for {bank_code.upper()}\"}\n",
        "\n",
        "    print(f\"\\nüîç [{bank_code.upper()}] MANUAL VS 4 MODELS COMPARISON\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    manual_df = manual_labels_dfs[bank_code]\n",
        "\n",
        "    # Filter to manually labeled records\n",
        "    manually_labeled_mask = manual_df['human_label'].notna() & (manual_df['human_label'] != '')\n",
        "    manual_eval_df = manual_df[manually_labeled_mask].copy()\n",
        "\n",
        "    if len(manual_eval_df) == 0:\n",
        "        print(\"‚ùå No manually labeled records for comparison\")\n",
        "        return {\"error\": \"No manually labeled records\"}\n",
        "\n",
        "    print(f\"Evaluating {len(manual_eval_df)} manually labeled records\")\n",
        "\n",
        "    # Load processed data to check for existing model predictions\n",
        "    processed_file = drive_base / f\"data/processed/{bank_code}\" / f\"processed_{bank_code}_combined_sentence_level.csv\"\n",
        "\n",
        "    comparison_results = {}\n",
        "\n",
        "    if processed_file.exists():\n",
        "        try:\n",
        "            processed_df = pd.read_csv(processed_file)\n",
        "            print(f\"Loaded processed data: {processed_df.shape}\")\n",
        "\n",
        "            # Try to merge with manual labels\n",
        "            merge_columns = ['text', 'sentence_id', 'original_qa_id']\n",
        "            merged_df = None\n",
        "\n",
        "            for merge_col in merge_columns:\n",
        "                if merge_col in manual_eval_df.columns and merge_col in processed_df.columns:\n",
        "                    try:\n",
        "                        merged_df = manual_eval_df.merge(\n",
        "                            processed_df,\n",
        "                            on=merge_col,\n",
        "                            how='inner',\n",
        "                            suffixes=('_manual', '_processed')\n",
        "                        )\n",
        "                        if len(merged_df) > 0:\n",
        "                            print(f\"  Merged on '{merge_col}': {len(merged_df)} records\")\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Merge on '{merge_col}' failed: {e}\")\n",
        "\n",
        "            if merged_df is not None and len(merged_df) > 0:\n",
        "                # Check for model predictions in processed data\n",
        "                model_mappings = {\n",
        "                    'finbert_yiyanghkust': ['finbert_tone_label', 'finbert_label'],\n",
        "                    'finbert_prosusai': ['prosus_label', 'prosus_finbert_label'],\n",
        "                    'distilroberta': ['distilroberta_label', 'roberta_label'],\n",
        "                    'cardiffnlp_roberta': ['cardiff_label', 'twitter_roberta_label']\n",
        "                }\n",
        "\n",
        "                for model_key, possible_cols in model_mappings.items():\n",
        "                    model_col = None\n",
        "                    for col in possible_cols:\n",
        "                        if col in merged_df.columns:\n",
        "                            model_col = col\n",
        "                            break\n",
        "\n",
        "                    if model_col:\n",
        "                        comparison_results[model_key] = analyze_model_vs_manual_enhanced(\n",
        "                            merged_df, model_col, 'human_label', model_key, bank_code\n",
        "                        )\n",
        "                    else:\n",
        "                        print(f\"  ‚ö†Ô∏è No predictions found for {model_key}\")\n",
        "                        comparison_results[model_key] = {\"error\": \"No predictions found\"}\n",
        "            else:\n",
        "                print(\"  ‚ùå Could not merge manual labels with processed data\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error loading processed data: {e}\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è No processed data found at {processed_file}\")\n",
        "\n",
        "    # If no existing predictions, create placeholder analysis\n",
        "    if not comparison_results:\n",
        "        print(\"  Creating placeholder analysis for 4 models...\")\n",
        "        for model_key in MODELS.keys():\n",
        "            comparison_results[model_key] = {\n",
        "                \"status\": \"no_predictions_available\",\n",
        "                \"manual_label_count\": len(manual_eval_df),\n",
        "                \"ready_for_training\": True\n",
        "            }\n",
        "\n",
        "    return comparison_results\n",
        "\n",
        "# Compare manual labels with model predictions for all banks\n",
        "model_comparison_results = {}\n",
        "for bank in BANKS:\n",
        "    model_comparison_results[bank] = compare_manual_vs_model_predictions_enhanced(bank)\n",
        "\n",
        "def analyze_model_vs_manual_enhanced(df: pd.DataFrame, model_col: str, manual_col: str,\n",
        "                                   model_name: str, bank_code: str) -> Dict:\n",
        "    \"\"\"Enhanced analysis of agreement between model predictions and manual labels.\"\"\"\n",
        "\n",
        "    # Filter valid comparisons\n",
        "    valid_mask = df[model_col].notna() & df[manual_col].notna()\n",
        "    comparison_df = df[valid_mask].copy()\n",
        "\n",
        "    if len(comparison_df) == 0:\n",
        "        return {'error': f'No valid comparisons for {model_name}'}\n",
        "\n",
        "    print(f\"  {model_name}: {len(comparison_df)} comparisons\")\n",
        "\n",
        "    # Calculate agreement\n",
        "    agreement = (comparison_df[model_col] == comparison_df[manual_col]).mean()\n",
        "    print(f\"    Agreement: {agreement:.3f}\")\n",
        "\n",
        "    # Classification report\n",
        "    try:\n",
        "        report = classification_report(\n",
        "            comparison_df[manual_col],\n",
        "            comparison_df[model_col],\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f\"    F1-Score: {report['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Classification report failed: {e}\")\n",
        "        report = {}\n",
        "\n",
        "    # Confusion matrix analysis\n",
        "    try:\n",
        "        labels = sorted(list(set(comparison_df[manual_col].unique()) | set(comparison_df[model_col].unique())))\n",
        "        cm = confusion_matrix(comparison_df[manual_col], comparison_df[model_col], labels=labels)\n",
        "\n",
        "        print(f\"    Labels analyzed: {labels}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Confusion matrix failed: {e}\")\n",
        "        cm = None\n",
        "        labels = []\n",
        "\n",
        "    # Cohen's Kappa\n",
        "    try:\n",
        "        kappa = cohen_kappa_score(comparison_df[manual_col], comparison_df[model_col])\n",
        "        print(f\"    Cohen's Kappa: {kappa:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Kappa calculation failed: {e}\")\n",
        "        kappa = None\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'bank_code': bank_code,\n",
        "        'total_comparisons': len(comparison_df),\n",
        "        'agreement_rate': agreement,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': cm.tolist() if cm is not None else None,\n",
        "        'confusion_matrix_labels': labels,\n",
        "        'cohen_kappa': kappa\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwonTBgMKgDy",
        "outputId": "e869a5fa-2100-4a12-af3d-a0f57aee0315"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç [JPM] MANUAL VS 4 MODELS COMPARISON\n",
            "--------------------------------------------------\n",
            "Evaluating 49 manually labeled records\n",
            "Loaded processed data: (752, 21)\n",
            "  Merged on 'original_qa_id': 49 records\n",
            "  ‚ö†Ô∏è No predictions found for finbert_yiyanghkust\n",
            "  ‚ö†Ô∏è No predictions found for finbert_prosusai\n",
            "  ‚ö†Ô∏è No predictions found for distilroberta\n",
            "  ‚ö†Ô∏è No predictions found for cardiffnlp_roberta\n",
            "\n",
            "üîç [HSBC] MANUAL VS 4 MODELS COMPARISON\n",
            "--------------------------------------------------\n",
            "Evaluating 69 manually labeled records\n",
            "Loaded processed data: (640, 21)\n",
            "  Merged on 'text': 1 records\n",
            "  ‚ö†Ô∏è No predictions found for finbert_yiyanghkust\n",
            "  ‚ö†Ô∏è No predictions found for finbert_prosusai\n",
            "  ‚ö†Ô∏è No predictions found for distilroberta\n",
            "  ‚ö†Ô∏è No predictions found for cardiffnlp_roberta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Training/Validation Split Preparation\n",
        "\n",
        "def prepare_enhanced_training_data(bank_code: str) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
        "    \"\"\"Prepare manually labeled data for 4-model fine-tuning.\"\"\"\n",
        "    if bank_code not in manual_labels_dfs or manual_labels_dfs[bank_code] is None:\n",
        "        return None, None, {\"error\": f\"No manual labels for {bank_code.upper()}\"}\n",
        "\n",
        "    print(f\"\\nüéØ [{bank_code.upper()}] PREPARING ENHANCED TRAINING DATA\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    manual_df = manual_labels_dfs[bank_code]\n",
        "\n",
        "    # Filter to manually labeled records\n",
        "    manually_labeled_mask = manual_df['human_label'].notna() & (manual_df['human_label'] != '')\n",
        "    manual_clean_df = manual_df[manually_labeled_mask].copy()\n",
        "\n",
        "    if len(manual_clean_df) == 0:\n",
        "        print(\"‚ùå No manually labeled data available\")\n",
        "        return None, None, {\"error\": \"No manually labeled data\"}\n",
        "\n",
        "    print(f\"Total manually labeled records: {len(manual_clean_df)}\")\n",
        "\n",
        "    # Enhanced stratified split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Create stratification groups\n",
        "    stratify_col = 'human_label'\n",
        "\n",
        "    if 'human_confidence' in manual_clean_df.columns:\n",
        "        # Add confidence-based stratification\n",
        "        manual_clean_df['confidence_group'] = pd.cut(\n",
        "            manual_clean_df['human_confidence'].fillna(0.5),\n",
        "            bins=[0, 0.6, 0.8, 1.0],\n",
        "            labels=['low', 'medium', 'high'],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "        manual_clean_df['stratify_group'] = (\n",
        "            manual_clean_df['human_label'].astype(str) + '_' +\n",
        "            manual_clean_df['confidence_group'].astype(str)\n",
        "        )\n",
        "        stratify_col = 'stratify_group'\n",
        "\n",
        "    # Enhanced split (80/20 for training/validation)\n",
        "    try:\n",
        "        train_df, val_df = train_test_split(\n",
        "            manual_clean_df,\n",
        "            test_size=0.2,\n",
        "            random_state=SEED,\n",
        "            stratify=manual_clean_df[stratify_col]\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {len(train_df)} records\")\n",
        "        print(f\"Validation set: {len(val_df)} records\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Stratification failed ({e}), using random split\")\n",
        "        train_df, val_df = train_test_split(\n",
        "            manual_clean_df,\n",
        "            test_size=0.2,\n",
        "            random_state=SEED\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {len(train_df)} records\")\n",
        "        print(f\"Validation set: {len(val_df)} records\")\n",
        "\n",
        "    # Enhanced analysis\n",
        "    split_analysis = {\n",
        "        'bank_code': bank_code,\n",
        "        'total_manual_labels': len(manual_clean_df),\n",
        "        'train_size': len(train_df),\n",
        "        'val_size': len(val_df),\n",
        "        'train_label_dist': train_df['human_label'].value_counts().to_dict(),\n",
        "        'val_label_dist': val_df['human_label'].value_counts().to_dict(),\n",
        "        'model_readiness': {}\n",
        "    }\n",
        "\n",
        "    # Model-specific readiness assessment\n",
        "    for model_key in MODELS.keys():\n",
        "        # Check text requirements for each model\n",
        "        text_col = 'text' if 'text' in train_df.columns else None\n",
        "\n",
        "        if text_col:\n",
        "            # Model-specific text length requirements\n",
        "            model_requirements = {\n",
        "                'finbert_yiyanghkust': {'min_length': 20, 'min_words': 4},\n",
        "                'finbert_prosusai': {'min_length': 20, 'min_words': 4},\n",
        "                'distilroberta': {'min_length': 10, 'min_words': 2},\n",
        "                'cardiffnlp_roberta': {'min_length': 10, 'min_words': 2}\n",
        "            }\n",
        "\n",
        "            if model_key in model_requirements:\n",
        "                req = model_requirements[model_key]\n",
        "\n",
        "                train_ready = (\n",
        "                    (train_df[text_col].str.len() >= req['min_length']) &\n",
        "                    (train_df[text_col].str.split().str.len() >= req['min_words'])\n",
        "                ).sum()\n",
        "\n",
        "                val_ready = (\n",
        "                    (val_df[text_col].str.len() >= req['min_length']) &\n",
        "                    (val_df[text_col].str.split().str.len() >= req['min_words'])\n",
        "                ).sum()\n",
        "\n",
        "                split_analysis['model_readiness'][model_key] = {\n",
        "                    'train_ready': train_ready,\n",
        "                    'val_ready': val_ready,\n",
        "                    'train_ready_pct': train_ready / len(train_df) * 100,\n",
        "                    'val_ready_pct': val_ready / len(val_df) * 100\n",
        "                }\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(f\"\\nTraining Label Distribution:\")\n",
        "    for label, count in split_analysis['train_label_dist'].items():\n",
        "        pct = (count / len(train_df)) * 100\n",
        "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nValidation Label Distribution:\")\n",
        "    for label, count in split_analysis['val_label_dist'].items():\n",
        "        pct = (count / len(val_df)) * 100\n",
        "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nModel Readiness:\")\n",
        "    for model_key, readiness in split_analysis['model_readiness'].items():\n",
        "        print(f\"  {model_key}:\")\n",
        "        print(f\"    Train: {readiness['train_ready']}/{len(train_df)} ({readiness['train_ready_pct']:.1f}%)\")\n",
        "        print(f\"    Val: {readiness['val_ready']}/{len(val_df)} ({readiness['val_ready_pct']:.1f}%)\")\n",
        "\n",
        "    return train_df, val_df, split_analysis\n",
        "\n",
        "# Prepare training data for all banks\n",
        "training_splits = {}\n",
        "for bank in BANKS:\n",
        "    train_df, val_df, split_analysis = prepare_enhanced_training_data(bank)\n",
        "    training_splits[bank] = {\n",
        "        'train_df': train_df,\n",
        "        'val_df': val_df,\n",
        "        'split_analysis': split_analysis\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPtjtk-PKm1U",
        "outputId": "29ba71d9-cd86-4267-c0be-6e2a5fc9e6ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ [JPM] PREPARING ENHANCED TRAINING DATA\n",
            "--------------------------------------------------\n",
            "Total manually labeled records: 49\n",
            "Training set: 39 records\n",
            "Validation set: 10 records\n",
            "\n",
            "Training Label Distribution:\n",
            "  positive: 15 (38.5%)\n",
            "  neutral: 15 (38.5%)\n",
            "  negative: 9 (23.1%)\n",
            "\n",
            "Validation Label Distribution:\n",
            "  neutral: 4 (40.0%)\n",
            "  positive: 4 (40.0%)\n",
            "  negative: 2 (20.0%)\n",
            "\n",
            "Model Readiness:\n",
            "  finbert_yiyanghkust:\n",
            "    Train: 39/39 (100.0%)\n",
            "    Val: 10/10 (100.0%)\n",
            "  finbert_prosusai:\n",
            "    Train: 39/39 (100.0%)\n",
            "    Val: 10/10 (100.0%)\n",
            "  distilroberta:\n",
            "    Train: 39/39 (100.0%)\n",
            "    Val: 10/10 (100.0%)\n",
            "  cardiffnlp_roberta:\n",
            "    Train: 39/39 (100.0%)\n",
            "    Val: 10/10 (100.0%)\n",
            "\n",
            "üéØ [HSBC] PREPARING ENHANCED TRAINING DATA\n",
            "--------------------------------------------------\n",
            "Total manually labeled records: 69\n",
            "Training set: 55 records\n",
            "Validation set: 14 records\n",
            "\n",
            "Training Label Distribution:\n",
            "  positive: 31 (56.4%)\n",
            "  neutral: 16 (29.1%)\n",
            "  negative: 8 (14.5%)\n",
            "\n",
            "Validation Label Distribution:\n",
            "  positive: 8 (57.1%)\n",
            "  neutral: 4 (28.6%)\n",
            "  negative: 2 (14.3%)\n",
            "\n",
            "Model Readiness:\n",
            "  finbert_yiyanghkust:\n",
            "    Train: 55/55 (100.0%)\n",
            "    Val: 14/14 (100.0%)\n",
            "  finbert_prosusai:\n",
            "    Train: 55/55 (100.0%)\n",
            "    Val: 14/14 (100.0%)\n",
            "  distilroberta:\n",
            "    Train: 55/55 (100.0%)\n",
            "    Val: 14/14 (100.0%)\n",
            "  cardiffnlp_roberta:\n",
            "    Train: 55/55 (100.0%)\n",
            "    Val: 14/14 (100.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Quality Assessment\n",
        "\n",
        "def assess_enhanced_manual_data_quality() -> Dict:\n",
        "    \"\"\"Comprehensive quality assessment across all banks and models.\"\"\"\n",
        "    print(f\"\\nüèÜ ENHANCED MULTI-BANK QUALITY ASSESSMENT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    assessment = {\n",
        "        'overall_quality': 'unknown',\n",
        "        'recommendations': [],\n",
        "        'fine_tuning_readiness': {},\n",
        "        'bank_summaries': {},\n",
        "        'model_compatibility': {},\n",
        "        'cross_bank_analysis': {}\n",
        "    }\n",
        "\n",
        "    total_manual_labels = 0\n",
        "    banks_with_sufficient_data = 0\n",
        "    models_ready_count = {model: 0 for model in MODELS.keys()}\n",
        "\n",
        "    # Bank-by-bank assessment\n",
        "    for bank in BANKS:\n",
        "        bank_assessment = {\n",
        "            'manual_label_count': 0,\n",
        "            'label_distribution': {},\n",
        "            'model_readiness': {},\n",
        "            'quality_score': 0,\n",
        "            'recommendations': []\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüìä [{bank.upper()}] Assessment:\")\n",
        "\n",
        "        if bank in validation_reports and 'manually_labeled_count' in validation_reports[bank]:\n",
        "            manual_count = validation_reports[bank]['manually_labeled_count']\n",
        "            bank_assessment['manual_label_count'] = manual_count\n",
        "            total_manual_labels += manual_count\n",
        "\n",
        "            print(f\"  Manual labels: {manual_count}\")\n",
        "\n",
        "            # Assess quantity\n",
        "            if manual_count >= 100:\n",
        "                banks_with_sufficient_data += 1\n",
        "                bank_assessment['quality_score'] += 3\n",
        "                bank_assessment['recommendations'].append(\"Good dataset size for fine-tuning\")\n",
        "            elif manual_count >= 50:\n",
        "                bank_assessment['quality_score'] += 2\n",
        "                bank_assessment['recommendations'].append(\"Moderate dataset size - suitable with care\")\n",
        "            else:\n",
        "                bank_assessment['quality_score'] += 1\n",
        "                bank_assessment['recommendations'].append(\"Small dataset - consider data augmentation\")\n",
        "\n",
        "            # Assess label balance\n",
        "            if 'label_distribution' in validation_reports[bank]:\n",
        "                label_dist = validation_reports[bank]['label_distribution']\n",
        "                bank_assessment['label_distribution'] = label_dist\n",
        "\n",
        "                if label_dist:\n",
        "                    label_counts = list(label_dist.values())\n",
        "                    min_count = min(label_counts)\n",
        "                    max_count = max(label_counts)\n",
        "                    balance_ratio = min_count / max_count if max_count > 0 else 0\n",
        "\n",
        "                    if balance_ratio >= 0.3:\n",
        "                        bank_assessment['quality_score'] += 2\n",
        "                        bank_assessment['recommendations'].append(\"Well-balanced labels\")\n",
        "                    elif balance_ratio >= 0.1:\n",
        "                        bank_assessment['quality_score'] += 1\n",
        "                        bank_assessment['recommendations'].append(\"Moderately balanced - usable\")\n",
        "                    else:\n",
        "                        bank_assessment['recommendations'].append(\"Imbalanced labels - use class weights\")\n",
        "\n",
        "            # Model readiness assessment\n",
        "            if bank in training_splits and training_splits[bank]['split_analysis']:\n",
        "                split_analysis = training_splits[bank]['split_analysis']\n",
        "                if 'model_readiness' in split_analysis:\n",
        "                    bank_assessment['model_readiness'] = split_analysis['model_readiness']\n",
        "\n",
        "                    for model_key, readiness in split_analysis['model_readiness'].items():\n",
        "                        train_ready_pct = readiness.get('train_ready_pct', 0)\n",
        "                        if train_ready_pct >= 80:\n",
        "                            models_ready_count[model_key] += 1\n",
        "                            print(f\"    {model_key}: Ready ({train_ready_pct:.1f}%)\")\n",
        "                        else:\n",
        "                            print(f\"    {model_key}: Needs attention ({train_ready_pct:.1f}%)\")\n",
        "\n",
        "        else:\n",
        "            bank_assessment['recommendations'].append(\"No manual labels available\")\n",
        "\n",
        "        # Overall bank quality\n",
        "        if bank_assessment['quality_score'] >= 6:\n",
        "            bank_quality = 'excellent'\n",
        "            assessment['fine_tuning_readiness'][bank] = True\n",
        "        elif bank_assessment['quality_score'] >= 4:\n",
        "            bank_quality = 'good'\n",
        "            assessment['fine_tuning_readiness'][bank] = True\n",
        "        elif bank_assessment['quality_score'] >= 2:\n",
        "            bank_quality = 'fair'\n",
        "            assessment['fine_tuning_readiness'][bank] = True\n",
        "        else:\n",
        "            bank_quality = 'poor'\n",
        "            assessment['fine_tuning_readiness'][bank] = False\n",
        "\n",
        "        bank_assessment['overall_quality'] = bank_quality\n",
        "        assessment['bank_summaries'][bank] = bank_assessment\n",
        "\n",
        "        print(f\"  Quality: {bank_quality}\")\n",
        "        print(f\"  Fine-tuning ready: {assessment['fine_tuning_readiness'][bank]}\")\n",
        "\n",
        "    # Cross-bank analysis\n",
        "    assessment['cross_bank_analysis'] = {\n",
        "        'total_manual_labels': total_manual_labels,\n",
        "        'banks_with_sufficient_data': banks_with_sufficient_data,\n",
        "        'avg_labels_per_bank': total_manual_labels / len(BANKS) if len(BANKS) > 0 else 0\n",
        "    }\n",
        "\n",
        "    # Model compatibility across banks\n",
        "    for model_key in MODELS.keys():\n",
        "        ready_banks = models_ready_count[model_key]\n",
        "        assessment['model_compatibility'][model_key] = {\n",
        "            'ready_banks': ready_banks,\n",
        "            'total_banks': len(BANKS),\n",
        "            'compatibility_score': ready_banks / len(BANKS) if len(BANKS) > 0 else 0\n",
        "        }\n",
        "\n",
        "    # Overall recommendations\n",
        "    if banks_with_sufficient_data == len(BANKS):\n",
        "        assessment['overall_quality'] = 'excellent'\n",
        "        assessment['recommendations'].append(\"All banks ready for fine-tuning\")\n",
        "    elif banks_with_sufficient_data >= len(BANKS) // 2:\n",
        "        assessment['overall_quality'] = 'good'\n",
        "        assessment['recommendations'].append(\"Most banks ready - proceed with available data\")\n",
        "    else:\n",
        "        assessment['overall_quality'] = 'needs_improvement'\n",
        "        assessment['recommendations'].append(\"Consider collecting more manual labels\")\n",
        "\n",
        "    # Model-specific recommendations\n",
        "    for model_key, compatibility in assessment['model_compatibility'].items():\n",
        "        if compatibility['compatibility_score'] >= 0.8:\n",
        "            assessment['recommendations'].append(f\"{model_key}: Ready for multi-bank fine-tuning\")\n",
        "        elif compatibility['compatibility_score'] >= 0.5:\n",
        "            assessment['recommendations'].append(f\"{model_key}: Partial readiness - focus on ready banks\")\n",
        "        else:\n",
        "            assessment['recommendations'].append(f\"{model_key}: Needs more preparation\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nüèÜ OVERALL ASSESSMENT:\")\n",
        "    print(f\"  Quality: {assessment['overall_quality'].upper()}\")\n",
        "    print(f\"  Total manual labels: {total_manual_labels:,}\")\n",
        "    print(f\"  Banks with sufficient data: {banks_with_sufficient_data}/{len(BANKS)}\")\n",
        "\n",
        "    print(f\"\\nüìã RECOMMENDATIONS:\")\n",
        "    for rec in assessment['recommendations']:\n",
        "        print(f\"  ‚Ä¢ {rec}\")\n",
        "\n",
        "    return assessment\n",
        "\n",
        "# Run enhanced quality assessment\n",
        "enhanced_quality_assessment = assess_enhanced_manual_data_quality()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlUtTxMEKs-j",
        "outputId": "3be12007-a045-4130-9ed3-3b87b136783f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ ENHANCED MULTI-BANK QUALITY ASSESSMENT\n",
            "============================================================\n",
            "\n",
            "üìä [JPM] Assessment:\n",
            "  Manual labels: 49\n",
            "    finbert_yiyanghkust: Ready (100.0%)\n",
            "    finbert_prosusai: Ready (100.0%)\n",
            "    distilroberta: Ready (100.0%)\n",
            "    cardiffnlp_roberta: Ready (100.0%)\n",
            "  Quality: fair\n",
            "  Fine-tuning ready: True\n",
            "\n",
            "üìä [HSBC] Assessment:\n",
            "  Manual labels: 69\n",
            "    finbert_yiyanghkust: Ready (100.0%)\n",
            "    finbert_prosusai: Ready (100.0%)\n",
            "    distilroberta: Ready (100.0%)\n",
            "    cardiffnlp_roberta: Ready (100.0%)\n",
            "  Quality: fair\n",
            "  Fine-tuning ready: True\n",
            "\n",
            "üèÜ OVERALL ASSESSMENT:\n",
            "  Quality: NEEDS_IMPROVEMENT\n",
            "  Total manual labels: 118\n",
            "  Banks with sufficient data: 0/2\n",
            "\n",
            "üìã RECOMMENDATIONS:\n",
            "  ‚Ä¢ Consider collecting more manual labels\n",
            "  ‚Ä¢ finbert_yiyanghkust: Ready for multi-bank fine-tuning\n",
            "  ‚Ä¢ finbert_prosusai: Ready for multi-bank fine-tuning\n",
            "  ‚Ä¢ distilroberta: Ready for multi-bank fine-tuning\n",
            "  ‚Ä¢ cardiffnlp_roberta: Ready for multi-bank fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Enhanced Validation Results\n",
        "\n",
        "def save_enhanced_validation_results():\n",
        "    \"\"\"Save all enhanced validation results and prepared data.\"\"\"\n",
        "    print(f\"\\nüíæ SAVING ENHANCED VALIDATION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    saved_files = {}\n",
        "\n",
        "    for bank in BANKS:\n",
        "        bank_saved = {}\n",
        "\n",
        "        print(f\"\\nüìÅ Saving {bank.upper()} validation results...\")\n",
        "\n",
        "        # Save validation report\n",
        "        if bank in validation_reports:\n",
        "            validation_path = manual_validation_paths[bank][\"manual_validation\"] / f\"validation_report_{bank}.json\"\n",
        "            with open(validation_path, 'w') as f:\n",
        "                json.dump(validation_reports[bank], f, indent=2, default=str)\n",
        "            bank_saved['validation_report'] = str(validation_path)\n",
        "            print(f\"  ‚úÖ Validation report: {validation_path}\")\n",
        "\n",
        "        # Save model comparison results\n",
        "        if bank in model_comparison_results:\n",
        "            comparison_path = manual_validation_paths[bank][\"manual_validation\"] / f\"model_comparison_{bank}.json\"\n",
        "            with open(comparison_path, 'w') as f:\n",
        "                json.dump(model_comparison_results[bank], f, indent=2, default=str)\n",
        "            bank_saved['model_comparison'] = str(comparison_path)\n",
        "            print(f\"  ‚úÖ Model comparison: {comparison_path}\")\n",
        "\n",
        "        # Save training/validation splits\n",
        "        if bank in training_splits:\n",
        "            splits = training_splits[bank]\n",
        "\n",
        "            if splits['train_df'] is not None:\n",
        "                train_path = manual_validation_paths[bank][\"manual_validation\"] / f\"train_manual_labels_{bank}.csv\"\n",
        "                splits['train_df'].to_csv(train_path, index=False)\n",
        "                bank_saved['train_data'] = str(train_path)\n",
        "                print(f\"  ‚úÖ Training data: {train_path} ({len(splits['train_df'])} records)\")\n",
        "\n",
        "            if splits['val_df'] is not None:\n",
        "                val_path = manual_validation_paths[bank][\"manual_validation\"] / f\"val_manual_labels_{bank}.csv\"\n",
        "                splits['val_df'].to_csv(val_path, index=False)\n",
        "                bank_saved['val_data'] = str(val_path)\n",
        "                print(f\"  ‚úÖ Validation data: {val_path} ({len(splits['val_df'])} records)\")\n",
        "\n",
        "            # Save split analysis\n",
        "            if splits['split_analysis']:\n",
        "                analysis_path = manual_validation_paths[bank][\"manual_validation\"] / f\"split_analysis_{bank}.json\"\n",
        "                with open(analysis_path, 'w') as f:\n",
        "                    json.dump(splits['split_analysis'], f, indent=2, default=str)\n",
        "                bank_saved['split_analysis'] = str(analysis_path)\n",
        "                print(f\"  ‚úÖ Split analysis: {analysis_path}\")\n",
        "\n",
        "        # Save validated manual labels to results directory\n",
        "        if bank in manual_labels_dfs and manual_labels_dfs[bank] is not None:\n",
        "            validated_path = manual_validation_paths[bank][\"results_sentiment\"] / f\"manual_labels_{bank}_validated.csv\"\n",
        "            manual_labels_dfs[bank].to_csv(validated_path, index=False)\n",
        "            bank_saved['validated_labels'] = str(validated_path)\n",
        "            print(f\"  ‚úÖ Validated labels: {validated_path}\")\n",
        "\n",
        "        saved_files[bank] = bank_saved\n",
        "\n",
        "    # Save overall quality assessment\n",
        "    overall_assessment_path = drive_base / \"configs\" / \"enhanced_manual_validation_assessment.json\"\n",
        "    with open(overall_assessment_path, 'w') as f:\n",
        "        json.dump(enhanced_quality_assessment, f, indent=2, default=str)\n",
        "    print(f\"\\n‚úÖ Overall assessment: {overall_assessment_path}\")\n",
        "\n",
        "    return saved_files\n",
        "\n",
        "# Save all enhanced validation results\n",
        "saved_validation_files = save_enhanced_validation_results()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C_VyEbcKzdI",
        "outputId": "2260312e-52e2-4187-a504-ca248a44b24c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ SAVING ENHANCED VALIDATION RESULTS\n",
            "==================================================\n",
            "\n",
            "üìÅ Saving JPM validation results...\n",
            "  ‚úÖ Validation report: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/jpm/validation_report_jpm.json\n",
            "  ‚úÖ Model comparison: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/jpm/model_comparison_jpm.json\n",
            "  ‚úÖ Training data: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/jpm/train_manual_labels_jpm.csv (39 records)\n",
            "  ‚úÖ Validation data: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/jpm/val_manual_labels_jpm.csv (10 records)\n",
            "  ‚úÖ Split analysis: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/jpm/split_analysis_jpm.json\n",
            "  ‚úÖ Validated labels: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/results/sentiment/jpm/manual_labels_jpm_validated.csv\n",
            "\n",
            "üìÅ Saving HSBC validation results...\n",
            "  ‚úÖ Validation report: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/hsbc/validation_report_hsbc.json\n",
            "  ‚úÖ Model comparison: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/hsbc/model_comparison_hsbc.json\n",
            "  ‚úÖ Training data: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/hsbc/train_manual_labels_hsbc.csv (55 records)\n",
            "  ‚úÖ Validation data: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/hsbc/val_manual_labels_hsbc.csv (14 records)\n",
            "  ‚úÖ Split analysis: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/data/manual_validation/hsbc/split_analysis_hsbc.json\n",
            "  ‚úÖ Validated labels: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/results/sentiment/hsbc/manual_labels_hsbc_validated.csv\n",
            "\n",
            "‚úÖ Overall assessment: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/configs/enhanced_manual_validation_assessment.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Summary and Next Steps\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ENHANCED MANUAL VALIDATION COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Summary statistics\n",
        "total_manual_labels = sum(\n",
        "    validation_reports[bank].get('manually_labeled_count', 0)\n",
        "    for bank in BANKS if bank in validation_reports\n",
        ")\n",
        "\n",
        "banks_ready_for_finetuning = sum(\n",
        "    1 for bank in BANKS\n",
        "    if enhanced_quality_assessment['fine_tuning_readiness'].get(bank, False)\n",
        ")\n",
        "\n",
        "models_with_high_compatibility = sum(\n",
        "    1 for model_compatibility in enhanced_quality_assessment['model_compatibility'].values()\n",
        "    if model_compatibility.get('compatibility_score', 0) >= 0.8\n",
        ")\n",
        "\n",
        "print(f\"üìä Validation Summary:\")\n",
        "print(f\"  Banks processed: {len(BANKS)} ({', '.join([b.upper() for b in BANKS])})\")\n",
        "print(f\"  Total manual labels: {total_manual_labels:,}\")\n",
        "print(f\"  Banks ready for fine-tuning: {banks_ready_for_finetuning}/{len(BANKS)}\")\n",
        "print(f\"  Models with high compatibility: {models_with_high_compatibility}/{len(MODELS)}\")\n",
        "\n",
        "print(f\"\\nüè¶ Bank-specific Summary:\")\n",
        "for bank in BANKS:\n",
        "    if bank in validation_reports:\n",
        "        manual_count = validation_reports[bank].get('manually_labeled_count', 0)\n",
        "        quality = enhanced_quality_assessment['bank_summaries'].get(bank, {}).get('overall_quality', 'unknown')\n",
        "        ready = enhanced_quality_assessment['fine_tuning_readiness'].get(bank, False)\n",
        "\n",
        "        print(f\"  {bank.upper()}:\")\n",
        "        print(f\"    Manual labels: {manual_count:,}\")\n",
        "        print(f\"    Quality: {quality}\")\n",
        "        print(f\"    Fine-tuning ready: {'Yes' if ready else 'No'}\")\n",
        "\n",
        "        # Show training/validation split if available\n",
        "        if bank in training_splits and training_splits[bank]['train_df'] is not None:\n",
        "            train_size = len(training_splits[bank]['train_df'])\n",
        "            val_size = len(training_splits[bank]['val_df'])\n",
        "            print(f\"    Train/Val split: {train_size}/{val_size}\")\n",
        "\n",
        "print(f\"\\nüéØ Model Compatibility:\")\n",
        "for model_key in MODELS.keys():\n",
        "    compatibility = enhanced_quality_assessment['model_compatibility'].get(model_key, {})\n",
        "    ready_banks = compatibility.get('ready_banks', 0)\n",
        "    score = compatibility.get('compatibility_score', 0)\n",
        "    status = \"Ready\" if score >= 0.8 else \"Partial\" if score >= 0.5 else \"Needs work\"\n",
        "\n",
        "    print(f\"  {model_key}:\")\n",
        "    print(f\"    Ready banks: {ready_banks}/{len(BANKS)}\")\n",
        "    print(f\"    Compatibility: {score:.1%} - {status}\")\n",
        "\n",
        "print(f\"\\nüìã Key Recommendations:\")\n",
        "for rec in enhanced_quality_assessment['recommendations']:\n",
        "    print(f\"  ‚Ä¢ {rec}\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "if enhanced_quality_assessment['overall_quality'] in ['excellent', 'good']:\n",
        "    print(f\"  1. ‚úÖ Proceed to 04_sentiment_analysis.ipynb for 4-model analysis\")\n",
        "    print(f\"  2. ‚úÖ Continue to 04b_model_finetuning.ipynb for enhanced fine-tuning\")\n",
        "    print(f\"  3. ‚úÖ Manual validation data ready for all {len(MODELS)} models\")\n",
        "else:\n",
        "    print(f\"  1. ‚ö†Ô∏è Consider collecting more manual labels\")\n",
        "    print(f\"  2. ‚ö†Ô∏è Review data quality issues before fine-tuning\")\n",
        "    print(f\"  3. ‚úÖ Continue with available data for baseline analysis\")\n",
        "\n",
        "print(f\"\\nüìÅ Files Created:\")\n",
        "total_files = sum(len(bank_files) for bank_files in saved_validation_files.values())\n",
        "print(f\"  Total files saved: {total_files}\")\n",
        "print(f\"  Validation reports: Available for all banks\")\n",
        "print(f\"  Training/validation splits: Ready for fine-tuning\")\n",
        "\n",
        "print(f\"\\nüéâ Enhanced manual validation process complete!\")\n",
        "print(f\"   Multi-bank, multi-model validation framework established\")\n",
        "print(f\"   Ready for advanced sentiment analysis with {len(MODELS)} models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTN91-LPK7pv",
        "outputId": "8ed5739d-5c7a-459f-a821-e621094424d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED MANUAL VALIDATION COMPLETE\n",
            "============================================================\n",
            "üìä Validation Summary:\n",
            "  Banks processed: 2 (JPM, HSBC)\n",
            "  Total manual labels: 118\n",
            "  Banks ready for fine-tuning: 2/2\n",
            "  Models with high compatibility: 4/4\n",
            "\n",
            "üè¶ Bank-specific Summary:\n",
            "  JPM:\n",
            "    Manual labels: 49\n",
            "    Quality: fair\n",
            "    Fine-tuning ready: Yes\n",
            "    Train/Val split: 39/10\n",
            "  HSBC:\n",
            "    Manual labels: 69\n",
            "    Quality: fair\n",
            "    Fine-tuning ready: Yes\n",
            "    Train/Val split: 55/14\n",
            "\n",
            "üéØ Model Compatibility:\n",
            "  finbert_yiyanghkust:\n",
            "    Ready banks: 2/2\n",
            "    Compatibility: 100.0% - Ready\n",
            "  finbert_prosusai:\n",
            "    Ready banks: 2/2\n",
            "    Compatibility: 100.0% - Ready\n",
            "  distilroberta:\n",
            "    Ready banks: 2/2\n",
            "    Compatibility: 100.0% - Ready\n",
            "  cardiffnlp_roberta:\n",
            "    Ready banks: 2/2\n",
            "    Compatibility: 100.0% - Ready\n",
            "\n",
            "üìã Key Recommendations:\n",
            "  ‚Ä¢ Consider collecting more manual labels\n",
            "  ‚Ä¢ finbert_yiyanghkust: Ready for multi-bank fine-tuning\n",
            "  ‚Ä¢ finbert_prosusai: Ready for multi-bank fine-tuning\n",
            "  ‚Ä¢ distilroberta: Ready for multi-bank fine-tuning\n",
            "  ‚Ä¢ cardiffnlp_roberta: Ready for multi-bank fine-tuning\n",
            "\n",
            "üöÄ Next Steps:\n",
            "  1. ‚ö†Ô∏è Consider collecting more manual labels\n",
            "  2. ‚ö†Ô∏è Review data quality issues before fine-tuning\n",
            "  3. ‚úÖ Continue with available data for baseline analysis\n",
            "\n",
            "üìÅ Files Created:\n",
            "  Total files saved: 12\n",
            "  Validation reports: Available for all banks\n",
            "  Training/validation splits: Ready for fine-tuning\n",
            "\n",
            "üéâ Enhanced manual validation process complete!\n",
            "   Multi-bank, multi-model validation framework established\n",
            "   Ready for advanced sentiment analysis with 4 models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szx6M6r-LBWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}