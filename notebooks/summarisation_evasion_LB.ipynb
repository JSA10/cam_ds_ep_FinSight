{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b820ac6",
   "metadata": {},
   "source": [
    "# **Summarisation & Evasion Notebook (LB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cb78c",
   "metadata": {},
   "source": [
    "# **1. Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b76a8",
   "metadata": {},
   "source": [
    "- Summarise banker answers into short, PRA relevant insights.\n",
    "- Generate an evasion score to tag summaries.\n",
    "- RAG pipeline to bring in relevant external documents (e.g. PRA risk definitions, regulatory news).\n",
    "- Optional extension: validate flagged risks with external/regulatory news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670282",
   "metadata": {},
   "source": [
    "# **2. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f5f5884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import spacy\n",
    "from llama_cpp import Llama \n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "from bert_score import score as bertscore \n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import faiss\n",
    "import chromadb\n",
    "import langchain\n",
    "import llama_index\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91bb8c",
   "metadata": {},
   "source": [
    "# **3. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "743f6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "jpm_2025_df = pd.read_csv('../data/processed/jpm/all_jpm_2025.csv')\n",
    "\n",
    "# View the data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75628c",
   "metadata": {},
   "source": [
    "# **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bc49b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0ec70c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_number  answer_number                            speaker_name  \\\n",
       "201               35            5.0  Chief Financial Officer, JPMorganChase   \n",
       "205               36            3.0  Chief Financial Officer, JPMorganChase   \n",
       "\n",
       "                                            role             company  \\\n",
       "201  And then some. Theres a lot of value added.       JPMorganChase   \n",
       "205                                         Okay  there you have it.   \n",
       "\n",
       "                                               content  year quarter  \\\n",
       "201  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "205  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "\n",
       "                                            source_pdf  \n",
       "201  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "205  data/raw/jpm/jpm-2q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles.\n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = jpm_2025_df[~jpm_2025_df['role'].isin(valid_roles)]\n",
    "\n",
    "# Number of rows with invalid roles.\n",
    "print('Number of rows:', invalid_roles_df.shape[0])\n",
    "\n",
    "# View the rows.\n",
    "invalid_roles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e31cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.'], dtype=object)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the correct role information.\n",
    "jpm_2025_df.at[205, 'role'] = 'Chief Financial Officer'\n",
    "jpm_2025_df.at[209, 'role'] = 'Chief Financial Officer'\n",
    "\n",
    "# Verify the roles have been updated.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e44043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_2025_df['role_normalised'] = jpm_2025_df['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "755f26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf565",
   "metadata": {},
   "source": [
    "# **5. Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401764b",
   "metadata": {},
   "source": [
    "## **5.1 Baseline**\n",
    "- Model = BART (not instruction tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "671f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the w\n"
     ]
    }
   ],
   "source": [
    "# Filter data to banker answers only.\n",
    "banker_answers = jpm_2025_df[jpm_2025_df['role_normalised'] == 'banker']['content'].tolist()\n",
    "print(banker_answers[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "54559054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The main thing that we see there, what would appear to be a certain amount of frontloading of spending ahead of people expecting price increases from tariffs. So ironically, that's actually somewhat supportive, all else equal. In terms of our corporate clients, obviously, they've been reacting to the changes in tariff policy.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (BART)\n",
    "summariser = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "sample_text = banker_answers[0]\n",
    "summary = summariser(sample_text, max_length=80, min_length=30, do_sample=False)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "024bd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: Corporates are taking a wait-and-see approach to tariff policy. Some sectors are going to be much more exposed than others. Small business and smaller corporates are probably a little more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Prompt conditioning to make PRA relevant.\n",
    "prompt = \"Summarise this answer, focusing on risk, capital and evasion of detail: \" + sample_text\n",
    "summary = summariser(prompt, max_length=80, min_length=30)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813acc2",
   "metadata": {},
   "source": [
    "## **5.2 RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cba37",
   "metadata": {},
   "source": [
    "### **Action point:** make runner code reproducible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1b7a",
   "metadata": {},
   "source": [
    "Retrieve PRA risk categories to give greater PRA focus to summaries (local RAG loop).\n",
    "- measure cosine similarity between transcript chunks and PRA risk categories (vectors)\n",
    "- retrieve the top 2-3 most relevant risk categories \n",
    "- prepend them to the summarisation prompt to make summaries PRA-aligned instead of just summarised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2bd1",
   "metadata": {},
   "source": [
    "- Attempting to use BART resulted in prompt echoing.\n",
    "- New attempt using Mistral-7B-Instruct.\n",
    "- Using sentence-BERT vs TF-IDF for vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b05ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove whitespace in text.\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c4f9ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the transcript into smaller chunks.\n",
    "def chunk_text(text: str, max_chars: int = 6000):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split into sentences \n",
    "    chunks, current_chunk, current_len = [], [], 0 # list of chunks, sentences collecting for current chunk, character count for current chunk\n",
    "\n",
    "    for s in sentences:\n",
    "        if current_len + len(s) + 1 <= max_chars: # if the characters of current chunk + new sentence is below the limit:\n",
    "            current_chunk.append(s) # add sentence to current chunk \n",
    "            current_len += len(s) + 1 # update running character count \n",
    "        \n",
    "        else: # if the characters is above the limit:\n",
    "            chunks.append(' '.join(current_chunk)) # add the current chunk to the final chunk list\n",
    "            current_chunk, current_len = [s], len(s) # start a new chunk containing the sentence and update current len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk)) # add any sentences in current chunk after loop ends \n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "73c5478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load PRA risk categories.\n",
    "# pra_risk_df = pd.read_csv('../data/RAG-resources/PRA_risk_categories.csv')\n",
    "# pra_risk_df['doc'] = pra_risk_df['category'] + ': ' + pra_risk_df['definition']\n",
    "\n",
    "# # View dataframe.\n",
    "# pra_risk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4fe2b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pra_categories(path: Path):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            (row.get('category', '').strip(), [row.get('definition', '').strip()])\n",
    "            for row in reader if row.get('category')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "db60595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sentence-BERT embedding index for PRA categories.\n",
    "def build_embedding_index(pra_categories):\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    pra_risk_embeddings = embedder.encode(pra_risk_df['doc'], batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "    return embedder, np.asarray(pra_risk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9f02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the relevant PRA categories to the transcript chunks.\n",
    "def find_rel_categories(chunk, pra_categories, embedder, pra_risk_embeddings, top_k=2):\n",
    "    query_vec = embedder.encode([chunk], normalize_embeddings=True) # turns chunk into embedding\n",
    "    sims = cosine_similarity(query_vec, pra_risk_embeddings).ravel() # compares the chunk to each category doc \n",
    "    top_indices = np.argsort(-sims)[:top_k] # sorts scores descending and selected top k cateogories \n",
    "\n",
    "    return [pra_categories[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a7d85bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_block(model_output: str) -> str:\n",
    "    \"\"\"If the model adds extra text, grab the first JSON object.\"\"\"\n",
    "    a = model_output.find(\"{\")\n",
    "    b = model_output.rfind(\"}\")\n",
    "    return model_output[a:b+1] if a != -1 and b != -1 and b > a else model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "77206958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise the chunk.\n",
    "def summarise_chunk(model, chunk, relevant_categories, max_evidence=5):\n",
    "    lines = []\n",
    "    for name, definition in relevant_categories:\n",
    "        lines.append(f'- {name}:')\n",
    "        for d in definition:\n",
    "            lines.append(f'- {d}')\n",
    "    notes_block = '\\n'.join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        'You are a Data Scientist producing PRA-aligned summaries.'\n",
    "        'Use only the transcript text as evidence. Map content to the PRA categories provided.'\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "TRANSCRIPT:\n",
    "{chunk}\n",
    "\n",
    "PRA NOTES:\n",
    "{notes_block}\n",
    "\n",
    "TASK:\n",
    "Return STRICT JSON with keys:\n",
    "- evidence: list of up to {max_evidence} bullet strings\n",
    "- pra categories: list of objects {{ 'name': '...', 'why': '...' }}\n",
    "- summary: 4-6 sentences, neutral tone\n",
    "\n",
    "RULES:\n",
    "- Do not invent facts. If evidence is lacking, use a single bullet 'Insufficient evidence'.\n",
    "- Only choose categories supported by the evidence.\n",
    "- No markdown. Output JSON only.\n",
    "\"\"\"\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': clean_text(user_prompt)}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=512,\n",
    "        repeat_penalty=1.1\n",
    "    ) \n",
    "\n",
    "    text = response['choices'][0]['message']['content'].strip()\n",
    "    try:\n",
    "        return json.loads(exytact_json_block(text))\n",
    "    except Exception:\n",
    "        return {'raw': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e398a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables.\n",
    "MODEL_PATH = '/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "PRA_NOTES_PATH = '../data/RAG-resources/PRA_risk_categories.csv'\n",
    "TRANSCRIPT_PATH = '../data/processed/jpm/all_jpm_2025.csv'\n",
    "OUTPUT_PATH = \"jpm_mistral_pra_summary.json\"\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 207 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 110 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2495.33 MiB, (13210.70 / 10922.67)\n",
      "ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "load_tensors: offloading 20 repeating layers to GPU\n",
      "load_tensors: offloaded 20/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  2495.33 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x14c6ae900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x3eb54e250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x14c6aeda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x14c6af0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x1494e9c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x1494f1450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x3eb54e590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x3eb54d9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x14c6af500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x1494ce220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x14c6af810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x14c6afcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x14c6b0090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x46d5c1fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x46d5ef700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x1494b25c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1494c14c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x14940fae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x14c6b05d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x14c6b0990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x46d5ef9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x14c6b0c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x3eb54dc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x3eb54ba90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x14c6b1060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x3eb54b130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x3eb54b3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x3eb547340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x3eb547600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x3eb5479a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x14c6b1500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x46d5efc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x14c6b18a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x14c6b1de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x1494f2bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x14c6b20a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x14c6b23d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x14c6b2730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c6b2a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x14c6b2f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x14c6b32d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x46d5f1440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x46d5bfd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x1494b03d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x14c6b3690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x14bfc0af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x3eb547e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x3eb546ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x325d457f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x3eb546e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x46d5dae90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x325d8de10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x325d21ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x3eb546070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x3eb545280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x46d5f0530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x46d5f07f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x46d5fc8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x3eb545540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x46d5fcba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x325d62e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x325d7a580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c6b39c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x325d7aa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c6b3d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c6b4160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x46d5fce60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c6b4600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x325d8d360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x46d5fd120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x46d5fd3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c6b4a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x39d0055c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c6b4de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x39d0d82a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x46d5fd6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x14c6b51a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x14c6b54b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x14c6b59f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x46d5bf690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x14c6b5cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x39d0ecce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x3999911f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x3eb545900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x3999ce9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x3999cec60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x14c6b5fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x3eb5446c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x14c6b64f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x46d5c2390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x14c6b67b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x3999cef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x3999aa2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x3999bf5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x14c6b6bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x399985bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c6b6f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x3999ad6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x46d5e84a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x46d5e6d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x3eb544a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x3eb543df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3eb5440b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x46d5e55c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x46d5e3e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x3999f3870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c6b73e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c6b7740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x3999b9470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x3eb541e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c6b7b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c6b7eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x3999b9730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c6b8170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x46d50c6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x3999b99f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x3eb542110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x46d50b740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x3999a4430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x3eb542490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x46d50ba00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x399960f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x46d5f0180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c6b8430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x3999e0bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x46d5f0be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c6b86f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c6b89b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ff44850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x3eb540690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c6b8c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c6b9040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x399975b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x3eb540950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x3eb53ef40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x399975dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x399976090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x46d509940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x3eb53f2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x46d504080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c6b93a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x3eb53f5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x46d504340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x3eb53f9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x3eb53d460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c6b9760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x3999c1b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x3999f7130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x39998bd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c6b9ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x3eb53e6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c6b9e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x3999daa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x399975150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x3999cb970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3eb53d770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x399998d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x399998ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c6ba210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x3eb53bf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x3eb53c230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c6ba5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x3999992b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x3eb53b0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x3eb53b3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x3eb53b780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x3eb53a060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x3eb53a320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x3999e5c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x3eb53a5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x46d504600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x46d5048c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c6baab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x3eb53a910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x3eb539210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x3999e5ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c6bb0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c6bb3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x3eb5394d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x3eb537ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x3eb404080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x3eb5383c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x3eb538790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x3eb538b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c6bb8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c6bbbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x3eb404340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x3999e61b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c6bbee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x3999ed590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x399973f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x3eb404600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x3eb4048c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c6bc420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c6bc6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x3eb536960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x3999a4d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x14c6bca20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x3eb404b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x3eb535570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x3eb535830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x3eb535af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x3eb535eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x3eb536280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x3eb534cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x3eb534f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c6bcec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x3eb404e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x3eb533fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x3999fadf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x3999d31a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x3eb405100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x3eb534370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x3eb532be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x14c6bd290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x399993510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x3eb4053c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x3eb533070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x3eb405680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x14c6bd6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x14c6bda50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x3eb533440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x3eb531820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x3999cd9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x3eb405940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x39999f680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x14c6bddb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x3eb405c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x14c6be210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x3eb405ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x3eb406180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x3eb406440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x3eb406700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x3eb530580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x3eb530890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x3999ee910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x3999d3d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x3999b7130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x3eb52fd30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x14c6be6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x3eb52e610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x3999948e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x3eb52e8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x3999e2a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x39996f4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x3eb406a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x3eb52db10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x3999b3b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x3eb406de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c6be970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x14c6bed10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x14c6bf0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x3999e9aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x399989b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x3eb52ddd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c6bf540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x3eb52bad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x3999c1580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x3eb52bd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x3eb4071b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x3eb407610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x3eb4079e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x3999baac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x399982540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x3eb407e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x3eb52c090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x3eb52af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x3eb52b360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x3eb4081a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x3eb529e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c6bf890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x3999bbb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x3eb408640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x3eb408900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x3eb408c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x3eb4090b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x3eb52a0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x3999fef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c6bfc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x3eb409510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c6bffc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x14c6c04a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x399991a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3eb4098d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x3eb527830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x3eb409c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x3eb40a090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c6c0870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x399991d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c6c0c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x14c6c1010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x399991ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x3999ae010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x3eb40a3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x3eb40a850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x3eb40ac20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x3eb40aff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c6c13e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c6c17b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x14c6c1b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x3999e8af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x3999726c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x3eb40b350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x399990c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c6c1ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x3999c54b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x3eb40b720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c6c22b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x3999f0180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x3eb40bb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x3999aa870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x3999c5f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x3eb40bf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x3eb527af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x3eb527e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x3eb40c320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x3eb40c650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x3eb40c910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x14c6c27f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x39d1faf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x14c6c2ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x3eb528260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x3eb528630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x3999c6250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x3eb40cdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c6c2f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x3999c6510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x3eb40d210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x3eb40d5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x3eb40d980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x3eb40dd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x3999d93c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x14c6c3320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x3eb40e0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x3eb528a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x14c6c3760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x46ff140a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x399969140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x3eb528d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x3eb529150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x14c6c3b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x3999975b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x3eb40e5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x39997dc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x3eb525870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x3eb525b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x3eb525df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x3eb526100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x3eb40ea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x14c6c3e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x3eb40ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x3eb40f040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x3eb40f410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x46ff14360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x14c6c42d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x3eb40f870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c6c46a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x3eb524d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x3eb525130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c6c4a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c6c4e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x39997dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x3eb523d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x39997e180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x3eb523fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c6c5270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x39997e440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x3eb40fcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x3eb410030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x39997e700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14c6c56d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14c6c59e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x3eb4103b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14c6c5cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14c6c6150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x3eb410850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3eb524330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x39997e9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x3eb410cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x3eb410fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x14c6c64b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x3eb411380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x14c6c6870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x14c6c6bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x3999b7930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x3eb522310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x3999f0a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x3eb4117e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x3eb411b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x3eb411f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x3eb4122d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x3eb412630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x3999cd3a0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   300.01 MiB\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 171 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   21216.53 ms /  1729 tokens (   12.27 ms per token,    81.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48846.96 ms /   469 runs   (  104.15 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   70333.38 ms /  2198 tokens\n",
      "llama_perf_context_print:    graphs reused =        454\n",
      "Llama.generate: 10 prefix-match hit, remaining 1673 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   26675.08 ms /  1673 tokens (   15.94 ms per token,    62.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24136.74 ms /   321 runs   (   75.19 ms per token,    13.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   50939.10 ms /  1994 tokens\n",
      "llama_perf_context_print:    graphs reused =        310\n",
      "Llama.generate: 10 prefix-match hit, remaining 1962 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   27175.26 ms /  1962 tokens (   13.85 ms per token,    72.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22221.23 ms /   330 runs   (   67.34 ms per token,    14.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   49504.28 ms /  2292 tokens\n",
      "llama_perf_context_print:    graphs reused =        319\n",
      "Llama.generate: 10 prefix-match hit, remaining 1889 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   22247.44 ms /  1889 tokens (   11.78 ms per token,    84.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18487.62 ms /   272 runs   (   67.97 ms per token,    14.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   40808.24 ms /  2161 tokens\n",
      "llama_perf_context_print:    graphs reused =        263\n",
      "Llama.generate: 10 prefix-match hit, remaining 1765 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   20611.88 ms /  1765 tokens (   11.68 ms per token,    85.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22401.19 ms /   311 runs   (   72.03 ms per token,    13.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   43112.36 ms /  2076 tokens\n",
      "llama_perf_context_print:    graphs reused =        300\n",
      "Llama.generate: 10 prefix-match hit, remaining 1989 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   28323.08 ms /  1989 tokens (   14.24 ms per token,    70.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35006.24 ms /   318 runs   (  110.08 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   63494.11 ms /  2307 tokens\n",
      "llama_perf_context_print:    graphs reused =        307\n",
      "Llama.generate: 11 prefix-match hit, remaining 2010 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   28773.27 ms /  2010 tokens (   14.32 ms per token,    69.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34657.78 ms /   451 runs   (   76.85 ms per token,    13.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   63626.07 ms /  2461 tokens\n",
      "llama_perf_context_print:    graphs reused =        436\n",
      "Llama.generate: 11 prefix-match hit, remaining 1884 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   26880.22 ms /  1884 tokens (   14.27 ms per token,    70.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32385.05 ms /   339 runs   (   95.53 ms per token,    10.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   59446.91 ms /  2223 tokens\n",
      "llama_perf_context_print:    graphs reused =        328\n",
      "Llama.generate: 10 prefix-match hit, remaining 1736 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   27253.07 ms /  1736 tokens (   15.70 ms per token,    63.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40717.56 ms /   243 runs   (  167.56 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   68088.02 ms /  1979 tokens\n",
      "llama_perf_context_print:    graphs reused =        234\n",
      "Llama.generate: 10 prefix-match hit, remaining 1681 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   20864.79 ms /  1681 tokens (   12.41 ms per token,    80.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33654.01 ms /   511 runs   (   65.86 ms per token,    15.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   54704.63 ms /  2192 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n",
      "Llama.generate: 10 prefix-match hit, remaining 1755 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   19616.25 ms /  1755 tokens (   11.18 ms per token,    89.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22571.53 ms /   341 runs   (   66.19 ms per token,    15.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   42283.25 ms /  2096 tokens\n",
      "llama_perf_context_print:    graphs reused =        330\n",
      "Llama.generate: 10 prefix-match hit, remaining 1924 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   22073.30 ms /  1924 tokens (   11.47 ms per token,    87.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24519.63 ms /   366 runs   (   66.99 ms per token,    14.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   46697.86 ms /  2290 tokens\n",
      "llama_perf_context_print:    graphs reused =        354\n",
      "Llama.generate: 10 prefix-match hit, remaining 1714 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   20279.91 ms /  1714 tokens (   11.83 ms per token,    84.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24539.26 ms /   364 runs   (   67.42 ms per token,    14.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   44926.02 ms /  2078 tokens\n",
      "llama_perf_context_print:    graphs reused =        351\n",
      "Llama.generate: 10 prefix-match hit, remaining 1826 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   20969.75 ms /  1826 tokens (   11.48 ms per token,    87.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14650.41 ms /   221 runs   (   66.29 ms per token,    15.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   35677.34 ms /  2047 tokens\n",
      "llama_perf_context_print:    graphs reused =        213\n",
      "Llama.generate: 10 prefix-match hit, remaining 1847 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   21034.40 ms /  1847 tokens (   11.39 ms per token,    87.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16485.85 ms /   240 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   37588.87 ms /  2087 tokens\n",
      "llama_perf_context_print:    graphs reused =        232\n",
      "Llama.generate: 10 prefix-match hit, remaining 1904 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   24976.15 ms /  1904 tokens (   13.12 ms per token,    76.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18124.96 ms /   245 runs   (   73.98 ms per token,    13.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   43182.18 ms /  2149 tokens\n",
      "llama_perf_context_print:    graphs reused =        236\n",
      "Llama.generate: 10 prefix-match hit, remaining 1808 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   21813.57 ms /  1808 tokens (   12.07 ms per token,    82.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19413.19 ms /   296 runs   (   65.59 ms per token,    15.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   41306.74 ms /  2104 tokens\n",
      "llama_perf_context_print:    graphs reused =        285\n",
      "Llama.generate: 10 prefix-match hit, remaining 1864 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   20302.80 ms /  1864 tokens (   10.89 ms per token,    91.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25178.90 ms /   373 runs   (   67.50 ms per token,    14.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   45595.73 ms /  2237 tokens\n",
      "llama_perf_context_print:    graphs reused =        360\n",
      "Llama.generate: 10 prefix-match hit, remaining 1951 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   21265.60 ms /  1951 tokens (   10.90 ms per token,    91.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18890.25 ms /   281 runs   (   67.23 ms per token,    14.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   40230.98 ms /  2232 tokens\n",
      "llama_perf_context_print:    graphs reused =        271\n",
      "Llama.generate: 10 prefix-match hit, remaining 2025 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   22712.64 ms /  2025 tokens (   11.22 ms per token,    89.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25696.51 ms /   377 runs   (   68.16 ms per token,    14.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   48524.83 ms /  2402 tokens\n",
      "llama_perf_context_print:    graphs reused =        364\n",
      "Llama.generate: 10 prefix-match hit, remaining 1770 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   20016.11 ms /  1770 tokens (   11.31 ms per token,    88.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34342.23 ms /   511 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   54530.16 ms /  2281 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n",
      "Llama.generate: 10 prefix-match hit, remaining 2027 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   23388.10 ms /  2027 tokens (   11.54 ms per token,    86.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28857.97 ms /   339 runs   (   85.13 ms per token,    11.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   52386.61 ms /  2366 tokens\n",
      "llama_perf_context_print:    graphs reused =        327\n",
      "Llama.generate: 10 prefix-match hit, remaining 819 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   21220.52 ms\n",
      "llama_perf_context_print: prompt eval time =   14004.61 ms /   819 tokens (   17.10 ms per token,    58.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19326.63 ms /   248 runs   (   77.93 ms per token,    12.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   33480.19 ms /  1067 tokens\n",
      "llama_perf_context_print:    graphs reused =        239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chunks\": [\n",
      "    {\n",
      "      \"chunk\": 1,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Jeremy Barnum, Chief Financial Officer of JPMorganChase, mentioned that some consumers are frontloading their spending ahead of expected price increases from tariffs.\\\",\\n    \\\"On the corporate side, clients are shifting their focus away from strategic priorities towards more short-term work and supply chain optimization due to uncertainty caused by tariff policy changes.\\\",\\n    \\\"Jeremy Barnum also noted that smaller clients, such as small businesses and smaller corporates, may be more challenged during this period of elevated uncertainty.\\\",\\n    \\\"The new curve used by JPMorganChase has three cuts instead of one, which is expected to produce a notable headwind in the bank's NII ex. Markets due to lower weighted-average IORB.\\\",\\n    \\\"Jeremy Barnum stated that across all the puts and takes, JPMorganChase's number is a tiny bit lower than what would be expected given the lower expected front-end rates, but there are some offsets such as higher wholesale deposit balances and favorable balance effects.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Customer outcomes and market integrity\\\",\\n      \\\"why\\\": \\\"Jeremy Barnum mentioned that some consumers are frontloading their spending ahead of expected price increases from tariffs, which could impact customer outcomes and market integrity.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Jeremy Barnum noted that lower weighted-average IORB due to the new curve is expected to produce a notable headwind in JPMorganChase's NII ex. Markets, which could impact the bank's ability to meet obligations as they fall due.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"Jeremy Barnum discussed how uncertainty caused by tariff policy changes is affecting consumer and corporate behavior, which in turn is impacting JPMorganChase's growth and reserving expectations. He also mentioned some offsets that are helping to mitigate the headwind from lower expected front-end rates, such as higher wholesale deposit balances and favorable balance effects.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 2,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"The banking system should be a source of strength in the economy.\\\",\\n    \\\"Banks doing their job to support the economy is important for financial stability.\\\",\\n    \\\"A recessionary environment can negatively impact banks' equity performance.\\\",\\n    \\\"Credit losses and changes in volumes and deal curves can affect banks during an economic downturn.\\\",\\n    \\\"The analyst community has reduced its earnings estimates for the S&P 500.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Capital adequacy\\\",\\n      \\\"why\\\": \\\"Sufficiency of capital to absorb losses; CET1 ratio, buffers, ICAAP.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"The banking system's role in supporting the economy is crucial for financial stability. While a recessionary environment can negatively impact banks' equity performance, it is important to remember that credit losses and changes in volumes and deal curves can also affect banks during an economic downturn. The analyst community has reduced its earnings estimates for the S&P 500, indicating a potential negative impact on the economy. However, the banking system's capital adequacy and liquidity are important categories to consider when assessing financial stability.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 3,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"The credit card net charge-off forecast for the full year has not changed.\\\",\\n    \\\"There is a wide range of potential outcomes based on what's happening today.\\\",\\n    \\\"The next couple of quarters will determine how much the charge-offs will increase.\\\",\\n    \\\"The investment in banks, branches, technology, AI will continue regardless of the environment.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Risk frameworks\\\",\\n      \\\"why\\\": \\\"JPMorganChase has a risk management framework that includes risk appetite, limits, aggregation, and challenge.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"JPMorganChase is monitoring counterparty default/credit deterioration, provisioning, non-performing loans (NPLs), and concentration in its credit card business.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorganChase has not changed its full year credit card net charge-off forecast, but it acknowledges that there is a wide range of potential outcomes based on current events. The next couple of quarters will determine how much the charge-offs will increase. JPMorganChase plans to continue investing in banks, branches, technology, and AI regardless of the environment. The company is monitoring credit risk through its risk management framework and is keeping an eye on counterparty default/credit deterioration, provisioning, NPLs, and concentration in its credit card business.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 4,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Margin and cash buffers are lower\\\",\\n    \\\"Rotation of spend is a little bit weaker than it was in the peak spending moments\\\",\\n    \\\"Increases in spending coming from lower income segment\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Stress testing\\\",\\n      \\\"why\\\": \\\"Outcomes under stress scenarios; capital and liquidity impacts.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase reported lower margin and cash buffers in the first quarter of 2025, with some rotation of spend being weaker than it was during peak spending moments. However, increases in spending are coming from the lower income segment. The bank also mentioned that delinquencies for home lending have increased both quarter-on-quarter and year-over-year, but this may be due to noise from the First Republic deal. The bank is managing rate risk holistically across the firm and does not take more interest rate exposure to this in any way, shape or form.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 5,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase has been performing exceptionally well in recent quarters\\\",\\n    \\\"Elevated volatility in financial markets is a factor affecting the business\\\",\\n    \\\"Some indications suggest that clients are taking down risk\\\",\\n    \\\"The bank's Markets business benefits from a volatile environment if markets operate relatively normally\\\",\\n    \\\"JPMorgan Chase has been open to lending through cycles and has the underwriting capability, capital, liquidity, and experience to be reliable lenders in serving their clients\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Customer outcomes and market integrity\\\",\\n      \\\"why\\\": \\\"The bank's Markets business benefits from a volatile environment if markets operate relatively normally\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Market risk\\\",\\n      \\\"why\\\": \\\"Elevated volatility in financial markets is a factor affecting the business\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase's Markets business has been performing exceptionally well in recent quarters, benefiting from a volatile environment if markets operate relatively normally. However, elevated volatility and some indications that clients are taking down risk could impact the business over the coming quarters. The bank is open to lending through cycles and has the underwriting capability, capital, liquidity, and experience to be reliable lenders in serving their clients.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 6,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"The uncertainty surrounding tariffs and trade agreements is a major concern for businesses.\\\",\\n    \\\"The Fed's decision to pause quantitative tightening (QT) has contributed to market volatility.\\\",\\n    \\\"Inflation remains a significant challenge for the economy, with sticky inflation expected to continue.\\\",\\n    \\\"The U.S. dollar remains the reserve currency and will not change significantly in the near term.\\\",\\n    \\\"The Fed's actions in response to market conditions are likely to be inconsistent and may cause further volatility.\\\"\\n  ],\\n  \\\"pra categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Governance\\\",\\n      \\\"why\\\": \\\"Board oversight, controls, risk culture, remuneration alignment\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"The uncertainty surrounding tariffs and trade agreements, inflation, and the Fed's actions are major concerns for businesses. The Fed's decision to pause quantitative tightening (QT) has contributed to market volatility, and the U.S. dollar remains the reserve currency and will not change significantly in the near term. The Fed's actions in response to market conditions are likely to be inconsistent and may cause further volatility.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 7,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase expects to see a back-loaded cut in interest rates, which will introduce a headwind on an exit rate going into next year.\\\",\\n    \\\"Corporates and investors are going to cash as volatility tends to drive higher deposits. Wholesale deposits outperformed for JPMorgan Chase relative to expectations this quarter.\\\",\\n    \\\"JPMorgan Chase is preparing for a full range of outcomes and has a lot of issues out there that need to be resolved, such as credit losses, investments going down, and what looks like a recession.\\\",\\n    \\\"The global economy and the Western world staying together economically and militarily are the most important things for JPMorgan Chase in the current economic climate.\\\",\\n    \\\"JPMorgan Chase is following the law of the land regarding the China issue but does not know how it will turn out.\\\"\\n  ],\\n  \\\"pra categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Capital adequacy\\\",\\n      \\\"why\\\": \\\"Sufficiency of capital to absorb losses; CET1 ratio, buffers, ICAAP\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase expects to see a back-loaded cut in interest rates, which will introduce a headwind on an exit rate going into next year. Corporates and investors are going to cash as volatility tends to drive higher deposits. JPMorgan Chase is preparing for a full range of outcomes and has a lot of issues out there that need to be resolved, such as credit losses, investments going down, and what looks like a recession. The global economy and the Western world staying together economically and militarily are the most important things for JPMorgan Chase in the current economic climate. JPMorgan Chase is following the law of the land regarding the China issue but does not know how it will turn out.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 8,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase's average loan growth was running at about 2% year-on-year.\\\",\\n    \\\"End of period loans were up 5% and wholesale loans were up 7%.\\\",\\n    \\\"There was no observable draws from clients in the wholesale market.\\\",\\n    \\\"Interestingly, JPMorgan Chase did not see heightened anxiety that people are more just focusing on addressing their supply chain issues right now.\\\",\\n    \\\"JPMorgan Chase is seeing a bit more growth in Markets loans as opposed to traditional C&I loans in the current moment.\\\"\\n  ],\\n  \\\"pra categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Conduct risk\\\",\\n      \\\"why\\\": \\\"Customer outcomes and market integrity; mis-selling, conflicts.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase's average loan growth was running at about 2% year-on-year. End of period loans were up 5% and wholesale loans were up 7%. There was no observable draws from clients in the wholesale market, indicating that there may not be heightened anxiety that people are more just focusing on addressing their supply chain issues right now. JPMorgan Chase is seeing a bit more growth in Markets loans as opposed to traditional C&I loans in the current moment.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 9,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Loans to deposits are now 70% for the banking system writ large.\\\",\\n    \\\"JPMorgan Chase has a safer system, lends more money, and has more liquidity.\\\",\\n    \\\"European regulations could be used as a model for better regulations in the US.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase has a safer system due to its lending practices and liquidity. European regulations could be used as a model for better regulations in the US. However, credit risk remains a concern due to counterparty default/credit deterioration, provisioning, NPLs, and concentration.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 10,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase reported strong revenue performance in the first quarter of 2025.\\\",\\n    \\\"The company's Chief Financial Officer, Jeremy Barnum, emphasized the importance of financial resource deployment by regulated banks to supporting the capital markets ecosystem.\\\",\\n    \\\"Barnum also noted that the revenue performance should not make the company lose focus on the larger fixes around financial resource deployment.\\\",\\n    \\\"Glenn Schorr, an analyst at Evercore Group LLC, agreed with Barnum's points and emphasized the importance of policy priorities in this area.\\\",\\n    \\\"Saul Martinez, an analyst at HSBC Securities (USA), Inc., asked about cost optimization efforts and whether it makes sense to pull back on investments in certain areas.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"SLR\\\",\\n      \\\"why\\\": \\\"The Senior Managers and Certification Regime (SMCR) is a regulatory framework that aims to improve accountability and responsibility among bankers. It requires senior managers to be personally responsible for the risks they take and to certify their compliance with the bank's risk management policies.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"LCR\\\",\\n      \\\"why\\\": \\\"The Liquidity Coverage Ratio (LCR) is a regulatory framework that aims to ensure that banks have sufficient liquidity to meet short-term obligations during a crisis. It requires banks to maintain a minimum level of high-quality liquid assets that can be converted into cash quickly.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"ILST\\\",\\n      \\\"why\\\": \\\"The Intermediate Liquidity Support (ILS) framework is a regulatory framework that aims to provide additional liquidity support to banks during a crisis. It requires banks to maintain a minimum level of intermediate-term funding sources that can be converted into cash quickly.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"G-SIB\\\",\\n      \\\"why\\\": \\\"The Global Systemically Important Banks (G-SIB) framework is a regulatory framework that aims to identify and address the risks posed by banks that are critical to the global financial system. It requires these banks to maintain higher levels of capital and liquidity than other banks.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Basel III Endgame\\\",\\n      \\\"why\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 11,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase is working on streamlining its operations to reduce costs.\\\",\\n    \\\"The company has a significant amount of savings and other cost-saving measures in place.\\\",\\n    \\\"Jamie Dimon, Chairman & CEO of JPMorgan Chase, apologizes to shareholders for not doing this sooner.\\\",\\n    \\\"HSBC Securities (USA), Inc. analyst Saul Martinez thanks the company for being clear and helpful.\\\",\\n    \\\"Jeremy Barnum, Chief Financial Officer of JPMorgan Chase, thanks the analysts for their questions.\\\"\\n  ],\\n  \\\"pra categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Governance\\\",\\n      \\\"why\\\": \\\"Board oversight, controls, risk culture, remuneration alignment.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase is working on streamlining its operations to reduce costs. The company has a significant amount of savings and other cost-saving measures in place. Jamie Dimon, Chairman & CEO of JPMorgan Chase, apologizes to shareholders for not doing this sooner. HSBC Securities (USA), Inc. analyst Saul Martinez thanks the company for being clear and helpful. Jeremy Barnum, Chief Financial Officer of JPMorgan Chase, thanks the analysts for their questions.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 12,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Private credit is growing.\\\",\\n    \\\"JPMorganChase is looking at ways to structure private credit.\\\",\\n    \\\"There has been an increasing focus on regulation of private credit for the better part of 15 years.\\\",\\n    \\\"Jamie Dimon believes that private credit should take a step back and look at the system and answer the question, how can we make it better and stronger for the economy and all involved?\\\",\\n    \\\"Jeremy Barnum believes that one of the worst things is G-SIB, in the sense of both the original gold plating, the deep conceptual flaws in the framework itself, and the failure to recalibrate it for growth since it was put into effect.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Governance\\\",\\n      \\\"why\\\": \\\"Board oversight, controls, risk culture, remuneration alignment.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorganChase is looking at ways to structure private credit and believes that private credit should take a step back and look at the system. There has been an increasing focus on regulation of private credit for the better part of 15 years. Jamie Dimon believes that one of the worst things is G-SIB, in the sense of both the original gold plating, the deep conceptual flaws in the framework itself, and the failure to recalibrate it for growth since it was put into effect.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 13,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan is involved in fintech and payment systems.\\\",\\n    \\\"JPMorgan is cognizant of the importance of open banking and is waiting for clarity from the CFPB.\\\",\\n    \\\"JPMorgan is in favor of customer-centric pricing and data sharing, but wants to ensure customers understand what information is being shared and has a time limit.\\\",\\n    \\\"JPMorgan believes payment costs should be done properly and that third parties should be responsible for any fraud or scams resulting from their use of bank passcodes.\\\",\\n    \\\"JPMorgan is considering inorganic opportunities, but Jamie Dimon does not think it is likely to make sense.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Risk management\\\",\\n      \\\"why\\\": \\\"Risk frameworks, appetite, limits, aggregation, challenge.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan is involved in fintech and payment systems, but is waiting for clarity from the CFPB on open banking. The bank is in favor of customer-centric pricing and data sharing, but wants to ensure customers understand what information is being shared and has a time limit. JPMorgan believes payment costs should be done properly and that third parties should be responsible for any fraud or scams resulting from their use of bank passcodes. The bank is considering inorganic opportunities, but Jamie Dimon does not think it is likely to make sense.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 14,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase is considering issuing a joint stablecoin with other banks, similar to Zelle.\\\",\\n    \\\"Interoperability of stablecoins and deposits is an important consideration for JPMorgan Chase.\\\",\\n    \\\"The company is thinking about all the factors related to interoperability.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase is considering issuing a joint stablecoin with other banks and is thinking about all the factors related to interoperability. The company is also focusing on liquidity and credit risk management.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 15,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Commercial loan growth was much stronger in the second quarter.\\\",\\n    \\\"There is no mention of any specific strength by geography.\\\",\\n    \\\"Regulators could potentially have banks lend more in the future, consistent with what the Treasury Secretary said is his goal.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Conduct risk\\\",\\n      \\\"why\\\": \\\"Customer outcomes and market integrity; mis-selling, conflicts.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Governance\\\",\\n      \\\"why\\\": \\\"Board oversight, controls, risk culture, remuneration alignment.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"The narrative of things being fine with different signals pointing in slightly different directions, but nothing particularly concerning, applies to both consumer and commercial lending. Commercial loan growth was much stronger in the second quarter, but there is no mention of any specific strength by geography. Regulators could potentially have banks lend more in the future, consistent with what the Treasury Secretary said is his goal.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 16,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"G-SIFI inhibits lending and market making\\\",\\n    \\\"LCR inhibits both lending and market making\\\",\\n    \\\"CCAR capital, the FSRT, the fundamental book\\\",\\n    \\\"Total loan to deposits ratio for community banks\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"Jamie Dimon discussed the impact of regulatory construct on lending and liquidity for JPMorgan Chase. He suggested that G-SIFI, LCR, and CCAR could be modified to increase lending and reduce risk in the system. He also mentioned the importance of reducing the total loan to deposits ratio for community banks.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 17,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n    \\\"evidence\\\": [\\n        \\\"JPMorgan's natural ROTCE is 17% through-the-cycle.\\\",\\n        \\\"A more simplified regulatory construct that addresses both the capital and liquidity constraints could potentially move up JPMorgan's natural ROTCE.\\\",\\n        \\\"Optimizing the denominator of the natural ROTCE calculation could be additive to the company's returns, but it is important to consider other factors such as competition and market conditions.\\\"\\n    ],\\n    \\\"pra categories\\\": [\\n        {\\n            \\\"name\\\": \\\"Liquidity\\\",\\n            \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"Capital adequacy\\\",\\n            \\\"why\\\": \\\"Sufficiency of capital to absorb losses; CET1 ratio, buffers, ICAAP.\\\"\\n        }\\n    ],\\n    \\\"summary\\\": \\\"JPMorgan's natural ROTCE is 17% through-the-cycle. A more simplified regulatory construct that addresses both the capital and liquidity constraints could potentially move up JPMorgan's natural ROTCE. However, it is important to consider other factors such as competition and market conditions when evaluating the potential impact of such a change on the company's returns.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 18,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase has been reducing its asset sensitivity to the short end of the curve by adding duration and extending maturities.\\\",\\n    \\\"The market expects rate cuts later this year, but JPMorgan Chase cannot hedge ahead of these cuts as they are already priced in.\\\",\\n    \\\"JPMorgan Chase is in the process of expanding its balance sheet into lower risk assets with an SLR not being constrained.\\\",\\n    \\\"Regulators view reducing the SLR as a way to encourage banks to expand their balance sheets into lower risk assets.\\\",\\n    \\\"JPMorgan Chase supports the proposal to fix the SLR, but other actors in the market may be more bound by it.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase has been taking steps to reduce its asset sensitivity to the short end of the curve and expand its balance sheet into lower risk assets. The market expects rate cuts later this year, but JPMorgan Chase cannot hedge ahead of these cuts as they are already priced in. Regulators view reducing the SLR as a way to encourage banks to expand their balance sheets into lower risk assets. JPMorgan Chase supports the proposal to fix the SLR, but other actors in the market may be more bound by it.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 19,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Sapphire price changes have been well received by customers and the market.\\\",\\n    \\\"The customer value proposition associated with the Sapphire card has increased dramatically.\\\",\\n    \\\"The ratio of customer value to annual fee is clearly market leading.\\\",\\n    \\\"The trading side of the business has shown strong results despite environmental factors.\\\",\\n    \\\"JPMorgan Chase has deployed capital and other resources to boost results.\\\"\\n  ],\\n  \\\"pra categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Risk management\\\",\\n      \\\"why\\\": \\\"Risk frameworks, appetite, limits, aggregation, challenge.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase has recently refreshed one of its important products, the Sapphire card. The initial response has been positive and the customer value proposition associated with the card has increased dramatically. The ratio of customer value to annual fee is market leading. Despite environmental factors, the trading side of the business has shown strong results and JPMorgan Chase has deployed capital and other resources to boost results.\\\"\\n}\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 20,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n\\\"evidence\\\": [\\n\\\"JPMorgan Chase is experiencing pressure from commercial and corporate customers to offset the tariff impact.\\\",\\n\\\"JPMorgan Chase is not offsetting tariff pressure with pressure on loans or deposits.\\\",\\n\\\"Corporates and sponsors need to get things done, there is a ton of dry powder waiting for these pipelines to come through in fuller force.\\\",\\n\\\"Sponsors are still reluctant to use the public markets for IPOs.\\\",\\n\\\"There is pressure to kind of recycle capital and get things done after the initial shock of tariff policy changes.\\\",\\n\\\"JPMorgan Chase is making a lot of money, its capital base keeps rising.\\\"\\n],\\n\\\"pra categories\\\": [\\n{\\n\\\"name\\\": \\\"Customer outcomes and market integrity; mis-selling, conflicts\\\",\\n\\\"why\\\": \\\"JPMorgan Chase is experiencing pressure from commercial and corporate customers to offset the tariff impact.\\\"\\n},\\n{\\n\\\"name\\\": \\\"Liquidity\\\",\\n\\\"why\\\": \\\"There is ongoing pricing conversations as there should be, but I haven't heard anything to support a tariff linked narrative. Operator: Thank you.\\\"\\n}\\n],\\n\\\"summary\\\": \\\"JPMorgan Chase is experiencing pressure from commercial and corporate customers to offset the tariff impact, but is not offsetting it with pressure on loans or deposits. Corporates and sponsors need to get things done, there is a ton of dry powder waiting for these pipelines to come through in fuller force. Sponsors are still reluctant to use the public markets for IPOs. There is pressure to kind of recycle capital and get things done after the initial shock of tariff policy changes. JPMorgan Chase is making a lot of money, its capital base keeps rising.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 21,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"JPMorgan Chase has a strong return on tangible common equity (ROTE) of 20% in Q2, exceeding the targeted level of 17%.\\\",\\n    \\\"The company's capital adequacy is sufficient to absorb losses, with a CET1 ratio of 16.5% and buffers above regulatory requirements.\\\",\\n    \\\"JPMorgan Chase has a strong risk culture and governance structures in place, as evidenced by its successful implementation of the Basel III framework and ongoing focus on cybersecurity.\\\",\\n    \\\"The company's remuneration practices are aligned with long-term value creation, with executive compensation tied to performance metrics such as ROTE and return on equity (ROE).\\\",\\n    \\\"JPMorgan Chase has a diverse range of businesses and revenue streams, including investment banking, commercial banking, and wealth management, which provides resilience in the face of market fluctuations.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Capital adequacy\\\",\\n      \\\"why\\\": \\\"JPMorgan Chase has a sufficient level of capital to absorb losses, with a CET1 ratio of 16.5% and buffers above regulatory requirements.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Governance\\\",\\n      \\\"why\\\": \\\"The company has strong governance structures in place, including an independent board of directors and ongoing focus on risk management and cybersecurity.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Risk culture\\\",\\n      \\\"why\\\": \\\"JPMorgan Chase has a strong risk culture, as evidenced by its successful implementation of the Basel III framework and ongoing focus on cybersecurity.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Remuneration alignment\\\",\\n      \\\"why\\\": \\\"The company's remuneration practices are aligned with long-term value creation, with executive compensation tied to performance metrics such as ROTE and return on equity (ROE).\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Diversity of revenue streams\\\",\\n      \\\"why\\\": \\\"JPMorgan Chase has a diverse range of businesses and revenue streams, which provides resilience in the face of market fluctuations.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase has a strong capital adequacy position,\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 22,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Jeremy Barnum, Chief Financial Officer of JPMorganChase, pointed out that if you could compound at 17% for 20 years, you would have a significant portion of the GDP of the United States.\\\",\\n    \\\"Gerard Cassidy, an analyst at RBC Capital Markets LLC, asked Jeremy Barnum about his use of HP 12C and Jeremy confirmed that he still uses it.\\\",\\n    \\\"Jeremy Barnum, Chief Financial Officer of JPMorganChase, stated that there were fewer opportunities in Securitized Products and Fixed Income Financing in the second half of the quarter relative to emerging markets and the macro space.\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Capital adequacy\\\",\\n      \\\"why\\\": \\\"Sufficiency of capital to absorb losses; CET1 ratio, buffers, ICAAP.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"Jeremy Barnum, Chief Financial Officer of JPMorganChase, discussed the company's strong loan growth in the Commercial & Investment bank and the reasons for the decline in net interest income and lending income. He also mentioned the importance of liquidity and capital adequacy in maintaining the bank's ability to meet obligations and absorb losses.\\\"\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"chunk\": 23,\n",
      "      \"result\": {\n",
      "        \"raw\": \"{\\n  \\\"evidence\\\": [\\n    \\\"Hedging is part of the issue\\\",\\n    \\\"Late quarter assets came on balance sheet\\\",\\n    \\\"Loans are also a factor\\\",\\n    \\\"Markets NII piece is involved\\\",\\n    \\\"Insufficient evidence\\\"\\n  ],\\n  \\\"pra_categories\\\": [\\n    {\\n      \\\"name\\\": \\\"Liquidity\\\",\\n      \\\"why\\\": \\\"Ability to meet obligations as they fall due; LCR/NSFR, funding profile, stress outflows.\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"Credit risk\\\",\\n      \\\"why\\\": \\\"Counterparty default/credit deterioration; provisioning, NPLs, concentration.\\\"\\n    }\\n  ],\\n  \\\"summary\\\": \\\"JPMorgan Chase reported a decline in net income for the second quarter of 2025 due to various factors. Hedging was identified as part of the issue, and late quarter assets coming on the balance sheet also contributed to the decline. Loans and Markets NII were also mentioned as factors. However, insufficient evidence was provided to support all of these claims.\\\"\\n}\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"combined_summary\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Runner.\n",
    "pra_categories = load_pra_categories(Path(PRA_NOTES_PATH)) # load pra category file\n",
    "embedder, category_embeddings = build_embedding_index(pra_categories) # embed pra categories\n",
    "\n",
    "transcript_text = Path(TRANSCRIPT_PATH).read_text() # load the transcript text\n",
    "transcript_chunks = chunk_text(transcript_text) # split the transcript into smaller chunks\n",
    "\n",
    "n_threads = max(4, (os.cpu_count() or 8) - 2)\n",
    "\n",
    "model = Llama(\n",
    "    model_path=str(MODEL_PATH), \n",
    "    n_ctx=4096, \n",
    "    n_gpu_layers=20, \n",
    "    chat_format='mistral-instruct', \n",
    "    n_threads=n_threads)\n",
    "\n",
    "results = []\n",
    "for i, chunk in enumerate(transcript_chunks, 1):\n",
    "    top_categories = find_rel_categories(chunk, pra_categories, embedder, category_embeddings)\n",
    "    summary_result = summarise_chunk(model, chunk, top_categories)\n",
    "    results.append({'chunk': i, 'result': summary_result})\n",
    "\n",
    "combined_summary = ' '.join(r['result'].get('summary', '') for r in results)\n",
    "final_output = {'chunks': results, 'combined_summary': combined_summary.strip()}\n",
    "\n",
    "Path(OUTPUT_PATH).write_text(json.dumps(final_output, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1aadc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd9d93f",
   "metadata": {},
   "source": [
    "# **6. Evasion Scoring**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
