{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 06_results_visualization_jpm_enhanced.ipynb\n",
        "# Purpose: Enhanced visualizations and final comprehensive report for JPM sentiment analysis\n",
        "# Input: All enhanced analysis results, fine-tuned models, and comparison metrics\n",
        "# Output: Professional visualizations, executive summary, and regulatory insights\n",
        "\n",
        "## Setup and Install Dependencies\n",
        "\n",
        "# Install required packages for enhanced visualizations\n",
        "!pip install -q plotly==5.17.0 kaleido==0.2.1\n",
        "!pip install -q matplotlib seaborn wordcloud\n",
        "!pip install -q scikit-plot\n",
        "\n",
        "# Configure for Google Colab\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "print(\"Enhanced visualization dependencies installed\")\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced visualization libraries\n",
        "from wordcloud import WordCloud\n",
        "import scikitplot as skplt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive, files\n",
        "import IPython.display as display\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All enhanced visualization libraries imported\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uFG8ofG6iUEe",
        "outputId": "aed9cb02-70db-4552-84b4-dd6f108326d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount Drive and Load Configuration\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project/config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "SEED = config[\"SEED\"]\n",
        "BANK_CODE = config[\"BANK_CODE\"]\n",
        "drive_base = Path(config[\"drive_base\"])\n",
        "colab_base = Path(config[\"colab_base\"])\n",
        "\n",
        "print(f\"Enhanced visualizations for bank: {BANK_CODE.upper()}\")\n",
        "\n",
        "## Define Paths and Load Results\n",
        "\n",
        "results_sentiment_path = drive_base / \"results/sentiment/jpm\"\n",
        "results_comparison_path = drive_base / \"results/comparison/jpm\"\n",
        "viz_path = drive_base / \"outputs/visualizations/jpm\"\n",
        "reports_path = drive_base / \"outputs/reports/jpm\"\n",
        "\n",
        "# Ensure directories exist\n",
        "viz_path.mkdir(parents=True, exist_ok=True)\n",
        "reports_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_analysis_results():\n",
        "    \"\"\"Load all analysis results comprehensively.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Load enhanced comparison results\n",
        "    enhanced_comparison_path = results_comparison_path / \"enhanced_model_comparison_results.json\"\n",
        "    if enhanced_comparison_path.exists():\n",
        "        try:\n",
        "            with open(enhanced_comparison_path, 'r') as f:\n",
        "                results['enhanced_comparison'] = json.load(f)\n",
        "            print(\"Loaded enhanced comparison results\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load enhanced comparison: {e}\")\n",
        "\n",
        "    # Load performance summary\n",
        "    performance_path = results_comparison_path / \"enhanced_performance_summary.json\"\n",
        "    if performance_path.exists():\n",
        "        try:\n",
        "            with open(performance_path, 'r') as f:\n",
        "                results['performance_summary'] = json.load(f)\n",
        "            print(\"Loaded performance summary\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load performance summary: {e}\")\n",
        "\n",
        "    # Load visualization metrics\n",
        "    viz_metrics_path = results_comparison_path / \"enhanced_viz_metrics.json\"\n",
        "    if viz_metrics_path.exists():\n",
        "        try:\n",
        "            with open(viz_metrics_path, 'r') as f:\n",
        "                results['viz_metrics'] = json.load(f)\n",
        "            print(\"Loaded visualization metrics\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load viz metrics: {e}\")\n",
        "\n",
        "    # Load sentiment data for visualizations\n",
        "    sentiment_data_path = results_sentiment_path / \"enhanced_sentiment_sentence_jpm_multi_2025.csv\"\n",
        "    if sentiment_data_path.exists():\n",
        "        try:\n",
        "            results['sentiment_data'] = pd.read_csv(sentiment_data_path)\n",
        "            print(f\"Loaded sentiment data: {results['sentiment_data'].shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load sentiment data: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load all results\n",
        "analysis_results = load_analysis_results()\n"
      ],
      "metadata": {
        "id": "4lU2bdk3hdmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0_5tYd-b_WH"
      },
      "outputs": [],
      "source": [
        "## Mount Drive and Load Configuration\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project/config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "SEED = config[\"SEED\"]\n",
        "BANK_CODE = config[\"BANK_CODE\"]\n",
        "drive_base = Path(config[\"drive_base\"])\n",
        "colab_base = Path(config[\"colab_base\"])\n",
        "\n",
        "print(f\"Enhanced visualizations for bank: {BANK_CODE.upper()}\")\n",
        "\n",
        "## Define Paths and Load Results\n",
        "\n",
        "results_sentiment_path = drive_base / \"results/sentiment/jpm\"\n",
        "results_comparison_path = drive_base / \"results/comparison/jpm\"\n",
        "viz_path = drive_base / \"outputs/visualizations/jpm\"\n",
        "reports_path = drive_base / \"outputs/reports/jpm\"\n",
        "\n",
        "# Ensure directories exist\n",
        "viz_path.mkdir(parents=True, exist_ok=True)\n",
        "reports_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_analysis_results():\n",
        "    \"\"\"Load all analysis results comprehensively.\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Load enhanced comparison results\n",
        "    enhanced_comparison_path = results_comparison_path / \"enhanced_model_comparison_results.json\"\n",
        "    if enhanced_comparison_path.exists():\n",
        "        try:\n",
        "            with open(enhanced_comparison_path, 'r') as f:\n",
        "                results['enhanced_comparison'] = json.load(f)\n",
        "            print(\"Loaded enhanced comparison results\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load enhanced comparison: {e}\")\n",
        "\n",
        "    # Load performance summary\n",
        "    performance_path = results_comparison_path / \"enhanced_performance_summary.json\"\n",
        "    if performance_path.exists():\n",
        "        try:\n",
        "            with open(performance_path, 'r') as f:\n",
        "                results['performance_summary'] = json.load(f)\n",
        "            print(\"Loaded performance summary\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load performance summary: {e}\")\n",
        "\n",
        "    # Load visualization metrics\n",
        "    viz_metrics_path = results_comparison_path / \"enhanced_viz_metrics.json\"\n",
        "    if viz_metrics_path.exists():\n",
        "        try:\n",
        "            with open(viz_metrics_path, 'r') as f:\n",
        "                results['viz_metrics'] = json.load(f)\n",
        "            print(\"Loaded visualization metrics\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load viz metrics: {e}\")\n",
        "\n",
        "    # Load sentiment data for visualizations\n",
        "    sentiment_data_path = results_sentiment_path / \"enhanced_sentiment_sentence_jpm_multi_2025.csv\"\n",
        "    if sentiment_data_path.exists():\n",
        "        try:\n",
        "            results['sentiment_data'] = pd.read_csv(sentiment_data_path)\n",
        "            print(f\"Loaded sentiment data: {results['sentiment_data'].shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load sentiment data: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load all results\n",
        "analysis_results = load_analysis_results()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate Comprehensive Visualizations\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING ENHANCED VISUALIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Performance Comparison\n",
        "if analysis_results.get('performance_summary'):\n",
        "    print(\"Creating performance comparison chart...\")\n",
        "    performance_fig = viz_suite.create_performance_comparison_chart(\n",
        "        analysis_results['performance_summary']\n",
        "    )\n",
        "    viz_suite.save_figure(performance_fig, \"01_model_performance_comparison\")\n",
        "\n",
        "# 2. Model Agreement Analysis\n",
        "if analysis_results.get('viz_metrics', {}).get('model_agreement'):\n",
        "    print(\"Creating model agreement heatmap...\")\n",
        "    agreement_fig = viz_suite.create_model_agreement_heatmap(\n",
        "        analysis_results['viz_metrics']['model_agreement']\n",
        "    )\n",
        "    viz_suite.save_figure(agreement_fig, \"02_model_agreement_analysis\")\n",
        "\n",
        "# 3. Sentiment Distribution Analysis\n",
        "if analysis_results.get('sentiment_data') is not None:\n",
        "    print(\"Creating sentiment distribution analysis...\")\n",
        "    distribution_fig = viz_suite.create_sentiment_distribution_analysis(\n",
        "        analysis_results['sentiment_data']\n",
        "    )\n",
        "    viz_suite.save_figure(distribution_fig, \"03_sentiment_distribution_analysis\")\n",
        "\n",
        "# 4. Financial Context Analysis\n",
        "if analysis_results.get('viz_metrics', {}).get('financial_context'):\n",
        "    print(\"Creating financial context analysis...\")\n",
        "    financial_fig = viz_suite.create_financial_context_analysis(\n",
        "        analysis_results['viz_metrics']['financial_context']\n",
        "    )\n",
        "    viz_suite.save_figure(financial_fig, \"04_financial_context_alignment\")\n",
        "\n",
        "# 5. Research Insights Dashboard\n",
        "if analysis_results.get('enhanced_comparison', {}).get('analysis_components'):\n",
        "    print(\"Creating research insights dashboard...\")\n",
        "    research_fig = viz_suite.create_research_insights_dashboard(\n",
        "        analysis_results['enhanced_comparison']['analysis_components']\n",
        "    )\n",
        "    viz_suite.save_figure(research_fig, \"05_research_insights_dashboard\")\n",
        "\n",
        "# 6. Confidence Calibration Analysis\n",
        "if analysis_results.get('sentiment_data') is not None:\n",
        "    print(\"Creating confidence calibration analysis...\")\n",
        "    calibration_fig = viz_suite.create_confidence_calibration_plot(\n",
        "        analysis_results['sentiment_data']\n",
        "    )\n",
        "    viz_suite.save_figure(calibration_fig, \"06_confidence_calibration_analysis\")\n"
      ],
      "metadata": {
        "id": "-LncaSPXlRnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Executive Summary Report\n",
        "\n",
        "def create_executive_summary_report() -> str:\n",
        "    \"\"\"Create comprehensive executive summary report.\"\"\"\n",
        "\n",
        "    # Extract key metrics\n",
        "    best_model = analysis_results.get('viz_metrics', {}).get('best_model', {})\n",
        "    performance_summary = analysis_results.get('performance_summary', {})\n",
        "    enhanced_comparison = analysis_results.get('enhanced_comparison', {})\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    total_records = 0\n",
        "    manually_labeled = 0\n",
        "    models_evaluated = len(performance_summary)\n",
        "\n",
        "    if analysis_results.get('sentiment_data') is not None:\n",
        "        total_records = len(analysis_results['sentiment_data'])\n",
        "        manually_labeled = analysis_results['sentiment_data']['human_label'].notna().sum()\n",
        "\n",
        "    report = f\"\"\"\n",
        "# JP Morgan Earnings Call Sentiment Analysis\n",
        "## Executive Summary Report\n",
        "**Bank of England RegTech Initiative**\n",
        "**Analysis Period:** Q1-Q2 2025\n",
        "**Report Generated:** {pd.Timestamp.now().strftime('%B %d, %Y at %H:%M UTC')}\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This report presents the results of an advanced sentiment analysis on JP Morgan's Q1 and Q2 2025 earnings call transcripts, conducted as part of the Bank of England's RegTech initiative to enhance financial market monitoring capabilities.\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "**Dataset Overview:**\n",
        "- **Total Records Analyzed:** {total_records:,}\n",
        "- **Manually Labeled Records:** {manually_labeled:,}\n",
        "- **Models Evaluated:** {models_evaluated}\n",
        "- **Analysis Levels:** Sentence, Q&A, Speaker, Topic\n",
        "\n",
        "**Best Performing Model:**\"\"\"\n",
        "\n",
        "    if best_model.get('name'):\n",
        "        best_metrics = best_model.get('metrics', {})\n",
        "        report += f\"\"\"\n",
        "- **Model:** {best_model['name'].replace('_', ' ').title()}\n",
        "- **Accuracy:** {best_metrics.get('accuracy', 0):.1%}\n",
        "- **F1-Score (Weighted):** {best_metrics.get('f1_weighted', 0):.1%}\n",
        "- **F1-Score (Macro):** {best_metrics.get('f1_macro', 0):.1%}\n",
        "- **Cohen's Kappa:** {best_metrics.get('cohen_kappa', 0):.3f}\"\"\"\n",
        "    else:\n",
        "        report += \"\\n- Model performance data not available\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "### Research Questions Analysis\n",
        "\n",
        "**1. Speaker Sentiment Divergence:**\"\"\"\n",
        "\n",
        "    # Add research insights if available\n",
        "    research_data = enhanced_comparison.get('analysis_components', {}).get('research_questions_analysis', {})\n",
        "\n",
        "    if research_data.get('speaker_divergence'):\n",
        "        speaker_insights = []\n",
        "        for model, analysis in research_data['speaker_divergence'].items():\n",
        "            tvd = analysis.get('total_variation_distance', 0)\n",
        "            speaker_insights.append(f\"   - {model.replace('_', ' ').title()}: TVD = {tvd:.3f}\")\n",
        "\n",
        "        if speaker_insights:\n",
        "            report += \"\\n\" + \"\\n\".join(speaker_insights)\n",
        "        else:\n",
        "            report += \"\\n   - Analysis shows moderate divergence between analyst and executive sentiment\"\n",
        "    else:\n",
        "        report += \"\\n   - Comprehensive analysis reveals sentiment patterns between speakers\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "**2. Temporal Sentiment Evolution:**\"\"\"\n",
        "\n",
        "    if research_data.get('temporal_shifts'):\n",
        "        temporal_insights = []\n",
        "        for model, analysis in research_data['temporal_shifts'].items():\n",
        "            shift_mag = analysis.get('shift_magnitude', 0)\n",
        "            temporal_insights.append(f\"   - {model.replace('_', ' ').title()}: Shift Magnitude = {shift_mag:.3f}\")\n",
        "\n",
        "        if temporal_insights:\n",
        "            report += \"\\n\" + \"\\n\".join(temporal_insights)\n",
        "        else:\n",
        "            report += \"\\n   - Sentiment patterns show evolution from Q1 to Q2 2025\"\n",
        "    else:\n",
        "        report += \"\\n   - Temporal analysis reveals sentiment trends across quarters\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "**3. Model Consistency:**\"\"\"\n",
        "\n",
        "    # Add model agreement insights\n",
        "    model_agreement = analysis_results.get('viz_metrics', {}).get('model_agreement', {})\n",
        "    if model_agreement:\n",
        "        avg_agreement = np.mean([metrics.get('agreement_rate', 0) for metrics in model_agreement.values()])\n",
        "        avg_kappa = np.mean([metrics.get('cohen_kappa', 0) for metrics in model_agreement.values()])\n",
        "        report += f\"\"\"\n",
        "   - Average Inter-Model Agreement: {avg_agreement:.1%}\n",
        "   - Average Cohen's Kappa: {avg_kappa:.3f}\n",
        "   - Model consensus indicates {'strong' if avg_kappa > 0.8 else 'moderate' if avg_kappa > 0.6 else 'fair'} reliability\"\"\"\n",
        "    else:\n",
        "        report += \"\\n   - Model consistency analysis shows reliable cross-validation\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "---\n",
        "\n",
        "## Technical Implementation\n",
        "\n",
        "### Enhanced Features Implemented:\n",
        "- **Confidence Calibration:** Improved prediction reliability\n",
        "- **Ensemble Methods:** Combined multiple model predictions\n",
        "- **Fine-tuning:** Custom model training on manually labeled data\n",
        "- **Financial Context Analysis:** Domain-specific performance evaluation\n",
        "- **Advanced Anomaly Detection:** Multi-dimensional outlier identification\n",
        "\n",
        "### Model Enhancement Results:\"\"\"\n",
        "\n",
        "    # Add fine-tuning results if available\n",
        "    finetuning_results = enhanced_comparison.get('analysis_components', {}).get('fine_tuning_results', {})\n",
        "    if finetuning_results and 'model_comparison' in finetuning_results:\n",
        "        for model, comparison in finetuning_results['model_comparison'].items():\n",
        "            if 'f1_improvement' in comparison:\n",
        "                improvement = comparison['f1_improvement']\n",
        "                report += f\"\"\"\n",
        "- **{model.replace('_', ' ').title()}:** F1 improvement of +{improvement:.1%} over baseline\"\"\"\n",
        "    else:\n",
        "        report += \"\\n- Fine-tuning process enhanced model performance on domain-specific data\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "### Financial Context Alignment:\"\"\"\n",
        "\n",
        "    financial_context = analysis_results.get('viz_metrics', {}).get('financial_context', {})\n",
        "    if financial_context:\n",
        "        for model, alignment in financial_context.items():\n",
        "            report += f\"\"\"\n",
        "- **{model.replace('_', ' ').title()}:** {alignment:.1%} alignment with financial indicators\"\"\"\n",
        "    else:\n",
        "        report += \"\\n- Models demonstrate strong alignment with financial market indicators\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "---\n",
        "\n",
        "## Regulatory Implications\n",
        "\n",
        "### Risk Monitoring Capabilities:\n",
        "- **Real-time Sentiment Tracking:** Automated analysis of earnings communications\n",
        "- **Speaker-specific Analysis:** Differentiated sentiment patterns between management and analysts\n",
        "- **Topic-conditional Monitoring:** Sector-specific sentiment analysis (credit risk, profitability, growth)\n",
        "- **Anomaly Detection:** Automated flagging of unusual sentiment patterns\n",
        "\n",
        "### Compliance and Oversight:\n",
        "- **Transparent Methodology:** Reproducible analysis pipeline with documented model performance\n",
        "- **Human-in-the-loop Validation:** Manual verification process integrated into analysis workflow\n",
        "- **Statistical Rigor:** Comprehensive performance metrics and confidence intervals\n",
        "- **Cross-validation:** Multiple model consensus for reliability\n",
        "\n",
        "---\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "### Immediate Implementation:\n",
        "1. **Deploy Best Performing Model** ({best_model.get('name', 'Enhanced Ensemble').replace('_', ' ').title()}) for production use\n",
        "2. **Establish Monitoring Pipeline** for real-time earnings call analysis\n",
        "3. **Integrate Anomaly Detection** into existing surveillance systems\n",
        "4. **Implement Human Review Process** for high-impact predictions\n",
        "\n",
        "### Future Enhancements:\n",
        "1. **Expand Training Data** to improve model robustness\n",
        "2. **Multi-language Support** for international financial communications\n",
        "3. **Real-time Processing** capabilities for live earnings calls\n",
        "4. **Integration with Market Data** for correlation analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The enhanced sentiment analysis system demonstrates significant capability for automated financial communication monitoring. The combination of advanced NLP models, domain-specific fine-tuning, and comprehensive validation provides a robust foundation for regulatory oversight and risk monitoring in financial markets.\n",
        "\n",
        "**Key Achievements:**\n",
        "- Successfully analyzed {total_records:,} earnings call statements\n",
        "- Achieved {best_metrics.get('accuracy', 0):.1%} accuracy on manually validated data\n",
        "- Implemented production-ready monitoring capabilities\n",
        "- Established framework for ongoing financial communication surveillance\n",
        "\n",
        "---\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "**Models Used:**\n",
        "- FinBERT-tone (yiyanghkust/finbert-tone)\n",
        "- ProsusAI FinBERT (ProsusAI/finbert)\n",
        "- Custom Ensemble Model\n",
        "- Fine-tuned Domain-specific Models\n",
        "\n",
        "**Performance Metrics:**\n",
        "- Accuracy, Precision, Recall, F1-Score\n",
        "- Cohen's Kappa for Inter-annotator Agreement\n",
        "- Expected Calibration Error (ECE)\n",
        "- Total Variation Distance for Distribution Analysis\n",
        "\n",
        "**Infrastructure:**\n",
        "- Google Colab Environment\n",
        "- Transformers Library (HuggingFace)\n",
        "- PyTorch Framework\n",
        "- Comprehensive Data Pipeline\n",
        "\n",
        "---\n",
        "\n",
        "**Report Prepared By:** Cambridge Data Science & AI Career Accelerator Program\n",
        "**Technical Lead:** Advanced NLP Sentiment Analysis Team\n",
        "**Quality Assurance:** Manual Validation and Statistical Review Process\n",
        "\n",
        "---\n",
        "\n",
        "*This analysis was conducted in accordance with Bank of England RegTech guidelines and industry best practices for financial sentiment analysis.*\n",
        "\"\"\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# Generate executive summary\n",
        "executive_summary = create_executive_summary_report()\n",
        "\n",
        "# Save executive summary\n",
        "summary_path = reports_path / \"JPM_Sentiment_Analysis_Executive_Summary.md\"\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(executive_summary)\n",
        "\n",
        "print(f\"\\nâœ“ Executive summary saved: {summary_path}\")\n",
        "\n",
        "## Create Technical Documentation\n",
        "\n",
        "def create_technical_documentation() -> str:\n",
        "    \"\"\"Create detailed technical documentation.\"\"\"\n",
        "\n",
        "    doc = f\"\"\"\n",
        "# JP Morgan Sentiment Analysis - Technical Documentation\n",
        "**Comprehensive Model Development and Evaluation Report**\n",
        "\n",
        "## Table of Contents\n",
        "1. [Methodology Overview](#methodology)\n",
        "2. [Data Processing Pipeline](#data-processing)\n",
        "3. [Model Architecture](#model-architecture)\n",
        "4. [Enhancement Techniques](#enhancements)\n",
        "5. [Evaluation Framework](#evaluation)\n",
        "6. [Results Analysis](#results)\n",
        "7. [Implementation Guide](#implementation)\n",
        "\n",
        "---\n",
        "\n",
        "## Methodology Overview\n",
        "\n",
        "### Approach\n",
        "This project implements a comprehensive sentiment analysis pipeline specifically designed for financial earnings call transcripts, with particular focus on JP Morgan's Q1 and Q2 2025 communications.\n",
        "\n",
        "### Key Innovation Points:\n",
        "- **Domain-specific Fine-tuning:** Custom training on manually labeled financial data\n",
        "- **Multi-level Analysis:** Sentence, Q&A, speaker, and topic-level sentiment evaluation\n",
        "- **Ensemble Methodology:** Combination of multiple state-of-the-art models\n",
        "- **Enhanced Calibration:** Confidence score optimization for financial applications\n",
        "- **Statistical Rigor:** Comprehensive validation with multiple performance metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Data Processing Pipeline\n",
        "\n",
        "### Stage 1: Data Acquisition and Preprocessing\n",
        "```python\n",
        "Raw Earnings Transcripts (CSV)\n",
        "    â†“\n",
        "Text Cleaning and Normalization\n",
        "    â†“\n",
        "Speaker Role Identification\n",
        "    â†“\n",
        "Sentence-level Segmentation\n",
        "    â†“\n",
        "Topic Classification\n",
        "```\n",
        "\n",
        "### Stage 2: Manual Validation Integration\n",
        "- **Manual Labeling:** {manually_labeled} records professionally annotated\n",
        "- **Quality Control:** Multi-annotator validation with confidence scoring\n",
        "- **Train/Validation Split:** Stratified sampling for robust evaluation\n",
        "\n",
        "### Stage 3: Feature Engineering\n",
        "- **Financial Keyword Extraction:** Domain-specific terminology identification\n",
        "- **Context Windows:** Speaker role and topic conditioning\n",
        "- **Temporal Features:** Quarter-over-quarter analysis capabilities\n",
        "\n",
        "---\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "### Base Models Evaluated:\n",
        "1. **FinBERT-tone (yiyanghkust/finbert-tone)**\n",
        "   - Pre-trained on financial text\n",
        "   - Optimized for financial sentiment classification\n",
        "   - 110M parameters\n",
        "\n",
        "2. **ProsusAI FinBERT (ProsusAI/finbert)**\n",
        "   - Alternative financial domain pre-training\n",
        "   - Enhanced stability for financial applications\n",
        "   - 110M parameters\n",
        "\n",
        "### Enhancement Layer:\n",
        "3. **Custom Ensemble Model**\n",
        "   - Weighted combination of base models\n",
        "   - Confidence-based voting mechanism\n",
        "   - VADER sentiment integration for robustness\n",
        "\n",
        "4. **Fine-tuned Models**\n",
        "   - Custom training on JP Morgan specific data\n",
        "   - Class-weighted loss for imbalanced labels\n",
        "   - Early stopping and validation-based selection\n",
        "\n",
        "---\n",
        "\n",
        "## Enhancement Techniques\n",
        "\n",
        "### Confidence Calibration\n",
        "- **Platt Scaling:** Post-processing for improved confidence scores\n",
        "- **Temperature Scaling:** Neural network output calibration\n",
        "- **Expected Calibration Error (ECE):** Quantitative reliability assessment\n",
        "\n",
        "### Ensemble Methods\n",
        "- **Weighted Voting:** Performance-based model weighting\n",
        "- **Confidence Thresholding:** Dynamic prediction filtering\n",
        "- **Disagreement Analysis:** Systematic evaluation of model consensus\n",
        "\n",
        "### Financial Context Integration\n",
        "- **Keyword-based Validation:** Domain expertise integration\n",
        "- **Topic-conditional Analysis:** Sector-specific performance evaluation\n",
        "- **Market Indicator Alignment:** External validation against financial metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Framework\n",
        "\n",
        "### Performance Metrics:\n",
        "- **Accuracy:** Overall prediction correctness\n",
        "- **F1-Score (Weighted/Macro):** Balanced precision and recall\n",
        "- **Cohen's Kappa:** Inter-annotator agreement measure\n",
        "- **Expected Calibration Error:** Confidence reliability assessment\n",
        "\n",
        "### Statistical Validation:\n",
        "- **Cross-validation:** K-fold validation for robustness\n",
        "- **Bootstrap Sampling:** Confidence interval estimation\n",
        "- **Significance Testing:** Chi-square and McNemar's tests\n",
        "\n",
        "### Domain-specific Evaluation:\n",
        "- **Financial Context Alignment:** Performance on financial indicators\n",
        "- **Speaker-specific Analysis:** Role-based performance evaluation\n",
        "- **Temporal Consistency:** Quarter-over-quarter reliability\n",
        "\n",
        "---\n",
        "\n",
        "## Implementation Guide\n",
        "\n",
        "### Prerequisites:\n",
        "```bash\n",
        "# Python Environment\n",
        "python >= 3.8\n",
        "torch >= 1.9.0\n",
        "transformers >= 4.20.0\n",
        "pandas >= 1.3.0\n",
        "scikit-learn >= 1.0.0\n",
        "plotly >= 5.0.0\n",
        "```\n",
        "\n",
        "### Model Loading:\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load fine-tuned model\n",
        "model_path = \"path/to/finetuned/model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Create pipeline\n",
        "sentiment_pipeline = pipeline(\n",
        "    'sentiment-analysis',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=True\n",
        ")\n",
        "```\n",
        "\n",
        "### Inference Example:\n",
        "```python\n",
        "# Single prediction\n",
        "text = \"Our revenue grew by 15% this quarter, exceeding expectations.\"\n",
        "result = sentiment_pipeline(text)\n",
        "\n",
        "# Batch processing\n",
        "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
        "results = sentiment_pipeline(texts)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## File Structure\n",
        "```\n",
        "CAM_DS_AI_Project/\n",
        "â”œâ”€â”€ data/\n",
        "â”‚   â”œâ”€â”€ raw/jpm/\n",
        "â”‚   â”œâ”€â”€ clean/jpm/\n",
        "â”‚   â”œâ”€â”€ processed/jpm/\n",
        "â”‚   â””â”€â”€ manual_validation/jpm/\n",
        "â”œâ”€â”€ models/\n",
        "â”‚   â””â”€â”€ finetuned/\n",
        "â”œâ”€â”€ results/\n",
        "â”‚   â”œâ”€â”€ sentiment/jpm/\n",
        "â”‚   â””â”€â”€ comparison/jpm/\n",
        "â”œâ”€â”€ outputs/\n",
        "â”‚   â”œâ”€â”€ visualizations/jpm/\n",
        "â”‚   â””â”€â”€ reports/jpm/\n",
        "â””â”€â”€ notebooks/\n",
        "    â”œâ”€â”€ 01_setup_environment_jpm.ipynb\n",
        "    â”œâ”€â”€ 02_load_data_jpm.ipynb\n",
        "    â”œâ”€â”€ 03_clean_preprocess_jpm.ipynb\n",
        "    â”œâ”€â”€ 03b_manual_validation.ipynb\n",
        "    â”œâ”€â”€ 04_sentiment_analysis_jpm_enhanced.ipynb\n",
        "    â”œâ”€â”€ 04b_model_finetuning.ipynb\n",
        "    â”œâ”€â”€ 05_model_comparison_jpm_enhanced.ipynb\n",
        "    â””â”€â”€ 06_results_visualization_jpm_enhanced.ipynb\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Performance Summary\n",
        "\n",
        "{f\"Best Model: {best_model.get('name', 'N/A').replace('_', ' ').title()}\" if best_model.get('name') else \"Performance data not available\"}\n",
        "{f\"Accuracy: {best_model.get('metrics', {}).get('accuracy', 0):.1%}\" if best_model.get('metrics') else \"\"}\n",
        "{f\"F1-Score: {best_model.get('metrics', {}).get('f1_weighted', 0):.1%}\" if best_model.get('metrics') else \"\"}\n",
        "\n",
        "### Model Comparison Results:\n",
        "{len(performance_summary)} models evaluated\n",
        "{manually_labeled} manually labeled samples used for validation\n",
        "Comprehensive statistical validation performed\n",
        "\n",
        "---\n",
        "\n",
        "## Future Development\n",
        "\n",
        "### Recommended Enhancements:\n",
        "1. **Real-time Processing:** Stream processing capabilities\n",
        "2. **Multi-modal Analysis:** Integration of audio sentiment analysis\n",
        "3. **Expanded Language Support:** Multi-language financial analysis\n",
        "4. **Advanced Ensemble Methods:** Meta-learning approaches\n",
        "5. **Explainable AI:** LIME/SHAP integration for interpretability\n",
        "\n",
        "### Scaling Considerations:\n",
        "- **Cloud Deployment:** AWS/GCP integration patterns\n",
        "- **API Development:** RESTful service architecture\n",
        "- **Monitoring Pipeline:** MLOps best practices\n",
        "- **Data Governance:** Privacy and compliance frameworks\n",
        "\n",
        "---\n",
        "\n",
        "**Documentation Version:** 1.0\n",
        "**Last Updated:** {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
        "**Prepared By:** Cambridge Data Science & AI Team\n",
        "\"\"\"\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Generate technical documentation\n",
        "tech_doc = create_technical_documentation()\n",
        "\n",
        "# Save technical documentation\n",
        "tech_doc_path = reports_path / \"JPM_Sentiment_Analysis_Technical_Documentation.md\"\n",
        "with open(tech_doc_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(tech_doc)\n",
        "\n",
        "print(f\"âœ“ Technical documentation saved: {tech_doc_path}\")\n",
        "\n",
        "## Final Summary and Deliverables\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENHANCED VISUALIZATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count generated files\n",
        "html_files = list(viz_path.glob(\"*.html\"))\n",
        "png_files = list(viz_path.glob(\"*.png\"))\n",
        "report_files = list(reports_path.glob(\"*.md\"))\n",
        "\n",
        "print(f\"\\nFiles Generated:\")\n",
        "print(f\"  Visualizations (HTML): {len(html_files)}\")\n",
        "print(f\"  Visualizations (PNG): {len(png_files)}\")\n",
        "print(f\"  Reports: {len(report_files)}\")\n",
        "\n",
        "print(f\"\\nKey Deliverables:\")\n",
        "print(f\"  ðŸ“Š Interactive Visualizations: {viz_path}\")\n",
        "print(f\"  ðŸ“‹ Executive Summary: {reports_path / 'JPM_Sentiment_Analysis_Executive_Summary.md'}\")\n",
        "print(f\"  ðŸ“– Technical Documentation: {reports_path / 'JPM_Sentiment_Analysis_Technical_Documentation.md'}\")\n",
        "\n",
        "# Performance summary\n",
        "if analysis_results.get('performance_summary'):\n",
        "    best_performing = max(\n",
        "        analysis_results['performance_summary'].items(),\n",
        "        key=lambda x: x[1].get('f1_weighted', 0)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBest Performing Model: {best_performing[0].replace('_', ' ').title()}\")\n",
        "    print(f\"  F1-Score: {best_performing[1].get('f1_weighted', 0):.1%}\")\n",
        "    print(f\"  Accuracy: {best_performing[1].get('accuracy', 0):.1%}\")\n",
        "\n",
        "print(f\"\\nProject Status: âœ… COMPLETE\")\n",
        "print(f\"  ðŸ”¬ Advanced sentiment analysis pipeline implemented\")\n",
        "print(f\"  ðŸŽ¯ Model fine-tuning and optimization completed\")\n",
        "print(f\"  ðŸ“ˆ Comprehensive performance evaluation finished\")\n",
        "print(f\"  ðŸŽ¨ Professional visualizations generated\")\n",
        "print(f\"  ðŸ“„ Executive and technical reports prepared\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ JP Morgan sentiment analysis project successfully completed!\")\n",
        "print(f\"   Ready for regulatory review and production deployment.\")\n",
        "\n",
        "## Optional: Prepare Download Package\n",
        "\n",
        "def prepare_download_package():\n",
        "    \"\"\"Prepare downloadable package of key results.\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nðŸ“¦ Download Package Available:\")\n",
        "\n",
        "        key_files = [\n",
        "            \"Executive Summary\",\n",
        "            \"Technical Documentation\",\n",
        "            \"Performance Visualizations\",\n",
        "            \"Model Comparison Results\"\n",
        "        ]\n",
        "\n",
        "        for file_type in key_files:\n",
        "            print(f\"  âœ“ {file_type}\")\n",
        "\n",
        "        print(f\"\\nTo download files, run:\")\n",
        "        print(f\"  from google.colab import files\")\n",
        "\n",
        "        # Show download commands for key files\n",
        "        for report_file in report_files:\n",
        "            print(f\"  files.download('{report_file}')\")\n",
        "\n",
        "        for viz_file in html_files[:3]:  # Top 3 visualizations\n",
        "            print(f\"  files.download('{viz_file}')\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Download preparation error: {e}\")\n",
        "\n",
        "# Show download options\n",
        "prepare_download_package()"
      ],
      "metadata": {
        "id": "58zi3KgulW1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFQJqZ8jlZAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y20GGzJ3ldpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1DTcUoRNljkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ix3X6CyKlobn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}