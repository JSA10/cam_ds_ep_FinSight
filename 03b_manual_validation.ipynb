{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 03b_manual_validation.ipynb\n",
        "# Purpose: Load and validate manually labeled sentiment data\n",
        "# Input: sentiment_sentence_jpm_multi_2025.csv from Google Drive\n",
        "# Output: Validated manual labels for fine-tuning\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import io\n",
        "import csv\n",
        "import tempfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Location A: Google Drive (Primary drive)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project/config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "SEED = config[\"SEED\"]\n",
        "BANK_CODE = config[\"BANK_CODE\"]\n",
        "drive_base = Path(config[\"drive_base\"])\n",
        "colab_base = Path(config[\"colab_base\"])\n",
        "\n",
        "print(f\"Manual validation for bank: {BANK_CODE.upper()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xh7oAU51vd6",
        "outputId": "f0a1b439-75db-4f07-cddc-aa5a89504d8c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Manual validation for bank: JPM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Paths\n",
        "\n",
        "processed_data_path = drive_base / \"data/processed/jpm\"\n",
        "manual_validation_path = drive_base / \"data/manual_validation/jpm\"\n",
        "results_sentiment_path = drive_base / \"results/sentiment/jpm\"\n",
        "\n",
        "# Ensure directories exist\n",
        "manual_validation_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "## Download Manually Labeled Data\n",
        "\n",
        "def extract_file_id_from_drive_url(url):\n",
        "    \"\"\"Extract file ID from Google Drive sharing URL.\"\"\"\n",
        "    if \"drive.google.com\" in url and \"/file/d/\" in url:\n",
        "        return url.split(\"/file/d/\")[1].split(\"/\")[0]\n",
        "    return None\n",
        "\n",
        "def download_manual_labels_from_drive():\n",
        "    \"\"\"Download manually labeled dataset from Google Drive.\"\"\"\n",
        "    # URL provided by user\n",
        "    manual_labels_url = \"https://drive.google.com/file/d/1aiqBl0Xll6eFgjXKlixSmVfIslA39bnU/view?usp=drive_link\"\n",
        "\n",
        "\n",
        "    file_id = extract_file_id_from_drive_url(manual_labels_url)\n",
        "    if not file_id:\n",
        "        print(\"Error: Could not extract file ID from URL\")\n",
        "        return None\n",
        "\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    filename = \"sentiment_sentence_jpm_multi_2025.csv\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Downloading manually labeled data from Google Drive...\")\n",
        "        response = requests.get(download_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save to manual validation directory\n",
        "        manual_file_path = manual_validation_path / filename\n",
        "        with open(manual_file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Also save to results directory for consistency\n",
        "        results_file_path = results_sentiment_path / filename\n",
        "        with open(results_file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"Downloaded {filename}\")\n",
        "        print(f\"  Manual validation: {manual_file_path}\")\n",
        "        print(f\"  Results: {results_file_path}\")\n",
        "\n",
        "        return manual_file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading manual labels: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Download the manually labeled data\n",
        "manual_labels_path = download_manual_labels_from_drive()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kWMIf_D149S",
        "outputId": "92f02eb7-0439-44c2-f7b2-e55b2c9d45fe"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading manually labeled data from Google Drive...\n",
            "Downloaded sentiment_sentence_jpm_multi_2025.csv\n",
            "  Manual validation: /content/drive/MyDrive/CAM_DS_AI_Project/data/manual_validation/jpm/sentiment_sentence_jpm_multi_2025.csv\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/sentiment_sentence_jpm_multi_2025.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Load and Validate Manual Labels\n",
        "\n",
        "def load_manual_labels(file_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Load manually labeled data with robust error handling for malformed CSV.\"\"\"\n",
        "    if not file_path or not file_path.exists():\n",
        "        print(\"Error: Manual labels file not found\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Try multiple approaches to handle malformed CSV\n",
        "        print(\"Attempting to load CSV with robust parsing...\")\n",
        "\n",
        "        # Approach 1: Standard loading with error handling\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding='utf-8')\n",
        "            print(f\"Loaded manual labels (standard method): {df.shape}\")\n",
        "            return df\n",
        "        except pd.errors.ParserError as e:\n",
        "            print(f\"Standard parsing failed: {e}\")\n",
        "\n",
        "        # Approach 2: Skip bad lines\n",
        "        try:\n",
        "            print(\"Trying with error_bad_lines=False...\")\n",
        "            df = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip')\n",
        "            print(f\"Loaded manual labels (skipped bad lines): {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Skip bad lines failed: {e}\")\n",
        "\n",
        "        # Approach 3: Use different separator or quoting\n",
        "        try:\n",
        "            print(\"Trying with flexible quoting...\")\n",
        "            df = pd.read_csv(file_path, encoding='utf-8', quoting=csv.QUOTE_ALL, on_bad_lines='skip')\n",
        "            print(f\"Loaded manual labels (flexible quoting): {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Flexible quoting failed: {e}\")\n",
        "\n",
        "        # Approach 4: Read as text and clean manually\n",
        "        try:\n",
        "            print(\"Attempting manual cleanup...\")\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            print(f\"File has {len(lines)} lines\")\n",
        "\n",
        "            # Check problematic line 20\n",
        "            if len(lines) > 20:\n",
        "                print(f\"Line 20 preview: {lines[19][:200]}...\")\n",
        "\n",
        "            # Try to identify the expected number of columns from header\n",
        "            header_line = lines[0].strip()\n",
        "            expected_columns = len(header_line.split(','))\n",
        "            print(f\"Expected columns from header: {expected_columns}\")\n",
        "\n",
        "            # Clean lines - remove lines with too many commas\n",
        "            cleaned_lines = []\n",
        "            for i, line in enumerate(lines):\n",
        "                if i == 0:  # Keep header\n",
        "                    cleaned_lines.append(line)\n",
        "                else:\n",
        "                    # Count commas (rough field count estimation)\n",
        "                    field_count = line.count(',') + 1\n",
        "                    if field_count <= expected_columns * 1.5:  # Allow some tolerance\n",
        "                        cleaned_lines.append(line)\n",
        "                    else:\n",
        "                        print(f\"Skipping line {i+1} with {field_count} fields\")\n",
        "\n",
        "            # Write cleaned data to temporary file\n",
        "            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8') as tmp_file:\n",
        "                tmp_file.writelines(cleaned_lines)\n",
        "                tmp_path = tmp_file.name\n",
        "\n",
        "            # Load cleaned file\n",
        "            df = pd.read_csv(tmp_path, encoding='utf-8')\n",
        "\n",
        "            # Clean up temp file\n",
        "            os.unlink(tmp_path)\n",
        "\n",
        "            print(f\"Loaded manual labels (manual cleanup): {df.shape}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Manual cleanup failed: {e}\")\n",
        "\n",
        "        # Approach 5: Last resort - try different encodings\n",
        "        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "        for encoding in encodings:\n",
        "            try:\n",
        "                print(f\"Trying encoding: {encoding}\")\n",
        "                df = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')\n",
        "                print(f\"Loaded manual labels (encoding {encoding}): {df.shape}\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"Encoding {encoding} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(\"All loading approaches failed\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading manual labels: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def validate_manual_labels(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Validate the structure and quality of manual labels.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MANUAL LABELS VALIDATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    validation_report = {\n",
        "        'total_records': len(df),\n",
        "        'manually_labeled_count': 0,\n",
        "        'label_distribution': {},\n",
        "        'confidence_stats': {},\n",
        "        'annotator_info': {},\n",
        "        'data_quality': {}\n",
        "    }\n",
        "\n",
        "    # Check for required columns\n",
        "    required_columns = ['human_label', 'human_confidence', 'annotation_notes', 'annotator_id']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: Missing required columns: {missing_columns}\")\n",
        "        validation_report['missing_columns'] = missing_columns\n",
        "\n",
        "        # If no manual label columns at all, see what we have\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "\n",
        "        # Check for alternative column names\n",
        "        alternative_mappings = {\n",
        "            'label': 'human_label',\n",
        "            'sentiment': 'human_label',\n",
        "            'manual_label': 'human_label',\n",
        "            'confidence': 'human_confidence',\n",
        "            'manual_confidence': 'human_confidence',\n",
        "            'notes': 'annotation_notes',\n",
        "            'comments': 'annotation_notes',\n",
        "            'annotator': 'annotator_id',\n",
        "            'user_id': 'annotator_id'\n",
        "        }\n",
        "\n",
        "        found_alternatives = {}\n",
        "        for alt_name, standard_name in alternative_mappings.items():\n",
        "            if alt_name in df.columns and standard_name in missing_columns:\n",
        "                found_alternatives[standard_name] = alt_name\n",
        "                print(f\"Found alternative column: '{alt_name}' for '{standard_name}'\")\n",
        "\n",
        "        # Rename columns if alternatives found\n",
        "        if found_alternatives:\n",
        "            df = df.rename(columns={v: k for k, v in found_alternatives.items()})\n",
        "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "            print(f\"After renaming, missing columns: {missing_columns}\")\n",
        "\n",
        "        if missing_columns:\n",
        "            return validation_report\n",
        "\n",
        "    # Count manually labeled records\n",
        "    manually_labeled_mask = df['human_label'].notna() & (df['human_label'] != '')\n",
        "    manually_labeled_df = df[manually_labeled_mask].copy()\n",
        "    validation_report['manually_labeled_count'] = len(manually_labeled_df)\n",
        "\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"Manually labeled: {len(manually_labeled_df):,} ({len(manually_labeled_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "    if len(manually_labeled_df) == 0:\n",
        "        print(\"Warning: No manually labeled records found\")\n",
        "        return validation_report\n",
        "\n",
        "    # Analyze label distribution\n",
        "    label_dist = manually_labeled_df['human_label'].value_counts()\n",
        "    validation_report['label_distribution'] = label_dist.to_dict()\n",
        "\n",
        "    print(f\"\\nLabel Distribution:\")\n",
        "    for label, count in label_dist.items():\n",
        "        pct = (count / len(manually_labeled_df)) * 100\n",
        "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    # Analyze confidence scores\n",
        "    if 'human_confidence' in manually_labeled_df.columns:\n",
        "        confidence_scores = manually_labeled_df['human_confidence'].dropna()\n",
        "        if len(confidence_scores) > 0:\n",
        "            validation_report['confidence_stats'] = {\n",
        "                'mean': confidence_scores.mean(),\n",
        "                'std': confidence_scores.std(),\n",
        "                'min': confidence_scores.min(),\n",
        "                'max': confidence_scores.max(),\n",
        "                'median': confidence_scores.median()\n",
        "            }\n",
        "\n",
        "            print(f\"\\nConfidence Scores:\")\n",
        "            print(f\"  Mean: {confidence_scores.mean():.2f}\")\n",
        "            print(f\"  Std: {confidence_scores.std():.2f}\")\n",
        "            print(f\"  Range: {confidence_scores.min():.2f} - {confidence_scores.max():.2f}\")\n",
        "\n",
        "    # Analyze annotator information\n",
        "    if 'annotator_id' in manually_labeled_df.columns:\n",
        "        annotator_dist = manually_labeled_df['annotator_id'].value_counts()\n",
        "        validation_report['annotator_info'] = {\n",
        "            'unique_annotators': len(annotator_dist),\n",
        "            'annotations_per_annotator': annotator_dist.to_dict()\n",
        "        }\n",
        "\n",
        "        print(f\"\\nAnnotator Information:\")\n",
        "        print(f\"  Unique annotators: {len(annotator_dist)}\")\n",
        "        for annotator, count in annotator_dist.items():\n",
        "            print(f\"  {annotator}: {count} annotations\")\n",
        "\n",
        "    # Data quality checks\n",
        "    quality_issues = []\n",
        "\n",
        "    # Check for missing confidence scores\n",
        "    missing_confidence = manually_labeled_df['human_confidence'].isna().sum()\n",
        "    if missing_confidence > 0:\n",
        "        quality_issues.append(f\"Missing confidence scores: {missing_confidence}\")\n",
        "\n",
        "    # Check for very low confidence annotations\n",
        "    if 'human_confidence' in manually_labeled_df.columns:\n",
        "        low_confidence = (manually_labeled_df['human_confidence'] < 0.5).sum()\n",
        "        if low_confidence > 0:\n",
        "            quality_issues.append(f\"Low confidence annotations (<0.5): {low_confidence}\")\n",
        "\n",
        "    # Check for missing annotation notes\n",
        "    missing_notes = manually_labeled_df['annotation_notes'].isna().sum()\n",
        "    if missing_notes > 0:\n",
        "        quality_issues.append(f\"Missing annotation notes: {missing_notes}\")\n",
        "\n",
        "    validation_report['data_quality']['issues'] = quality_issues\n",
        "\n",
        "    if quality_issues:\n",
        "        print(f\"\\nData Quality Issues:\")\n",
        "        for issue in quality_issues:\n",
        "            print(f\"  Warning: {issue}\")\n",
        "    else:\n",
        "        print(f\"\\nNo data quality issues detected\")\n",
        "\n",
        "    return validation_report\n",
        "\n",
        "# Load and validate manual labels\n",
        "manual_labels_df = load_manual_labels(manual_labels_path)\n",
        "validation_report = validate_manual_labels(manual_labels_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBxvgt_22AVD",
        "outputId": "8b8fc6f4-a2a2-4deb-b560-2a6bc6d6e4e1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load CSV with robust parsing...\n",
            "Loaded manual labels (standard method): (1110, 41)\n",
            "\n",
            "==================================================\n",
            "MANUAL LABELS VALIDATION\n",
            "==================================================\n",
            "Total records: 1,110\n",
            "Manually labeled: 200 (18.0%)\n",
            "\n",
            "Label Distribution:\n",
            "  neutral: 126 (63.0%)\n",
            "  positive: 44 (22.0%)\n",
            "  negative: 29 (14.5%)\n",
            "  poistive: 1 (0.5%)\n",
            "\n",
            "Confidence Scores:\n",
            "  Mean: 3.69\n",
            "  Std: 0.66\n",
            "  Range: 1.00 - 5.00\n",
            "\n",
            "Annotator Information:\n",
            "  Unique annotators: 1\n",
            "  Annotator01: 200 annotations\n",
            "\n",
            "Data Quality Issues:\n",
            "  Warning: Missing annotation notes: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Compare Manual Labels with Model Predictions\n",
        "\n",
        "def compare_manual_vs_model_predictions(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Compare manual labels with existing model predictions.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MANUAL VS MODEL COMPARISON\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Filter to manually labeled records\n",
        "    manually_labeled_mask = df['human_label'].notna() & (df['human_label'] != '')\n",
        "    manual_df = df[manually_labeled_mask].copy()\n",
        "\n",
        "    if len(manual_df) == 0:\n",
        "        print(\"No manually labeled records for comparison\")\n",
        "        return {}\n",
        "\n",
        "    comparison_results = {}\n",
        "\n",
        "    # Compare with FinBERT-tone predictions\n",
        "    if 'finbert_tone_label' in manual_df.columns:\n",
        "        finbert_comparison = analyze_model_vs_manual(\n",
        "            manual_df, 'finbert_tone_label', 'human_label', 'FinBERT-tone'\n",
        "        )\n",
        "        comparison_results['finbert_tone'] = finbert_comparison\n",
        "\n",
        "    # Compare with ProsusAI predictions\n",
        "    if 'prosus_label' in manual_df.columns:\n",
        "        prosus_comparison = analyze_model_vs_manual(\n",
        "            manual_df, 'prosus_label', 'human_label', 'ProsusAI'\n",
        "        )\n",
        "        comparison_results['prosus'] = prosus_comparison\n",
        "\n",
        "    return comparison_results\n",
        "\n",
        "def analyze_model_vs_manual(df: pd.DataFrame, model_col: str, manual_col: str, model_name: str) -> Dict:\n",
        "    \"\"\"Analyze agreement between model predictions and manual labels.\"\"\"\n",
        "\n",
        "    # Filter valid comparisons\n",
        "    valid_mask = df[model_col].notna() & df[manual_col].notna()\n",
        "    comparison_df = df[valid_mask].copy()\n",
        "\n",
        "    if len(comparison_df) == 0:\n",
        "        return {'error': f'No valid comparisons for {model_name}'}\n",
        "\n",
        "    print(f\"\\n{model_name} vs Manual Labels ({len(comparison_df)} comparisons)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Calculate agreement\n",
        "    agreement = (comparison_df[model_col] == comparison_df[manual_col]).mean()\n",
        "    print(f\"Overall Agreement: {agreement:.3f}\")\n",
        "\n",
        "    # Classification report\n",
        "    try:\n",
        "        report = classification_report(\n",
        "            comparison_df[manual_col],\n",
        "            comparison_df[model_col],\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(f\"Precision: {report['weighted avg']['precision']:.3f}\")\n",
        "        print(f\"Recall: {report['weighted avg']['recall']:.3f}\")\n",
        "        print(f\"F1-Score: {report['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate classification report: {e}\")\n",
        "        report = {}\n",
        "\n",
        "    # Confusion matrix\n",
        "    try:\n",
        "        labels = sorted(list(set(comparison_df[manual_col].unique()) | set(comparison_df[model_col].unique())))\n",
        "        cm = confusion_matrix(comparison_df[manual_col], comparison_df[model_col], labels=labels)\n",
        "\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(f\"Manual\\\\Model: {' '.join([f'{l:>8}' for l in labels])}\")\n",
        "        for i, manual_label in enumerate(labels):\n",
        "            row_str = f\"{manual_label:>12}: {' '.join([f'{cm[i,j]:>8}' for j in range(len(labels))])}\"\n",
        "            print(row_str)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate confusion matrix: {e}\")\n",
        "        cm = None\n",
        "\n",
        "    # Agreement by confidence level\n",
        "    confidence_analysis = {}\n",
        "    if 'human_confidence' in comparison_df.columns:\n",
        "        # High confidence vs low confidence agreement\n",
        "        high_conf_mask = comparison_df['human_confidence'] >= 0.8\n",
        "        if high_conf_mask.sum() > 0:\n",
        "            high_conf_agreement = (\n",
        "                comparison_df.loc[high_conf_mask, model_col] ==\n",
        "                comparison_df.loc[high_conf_mask, manual_col]\n",
        "            ).mean()\n",
        "            confidence_analysis['high_confidence_agreement'] = high_conf_agreement\n",
        "\n",
        "        low_conf_mask = comparison_df['human_confidence'] < 0.6\n",
        "        if low_conf_mask.sum() > 0:\n",
        "            low_conf_agreement = (\n",
        "                comparison_df.loc[low_conf_mask, model_col] ==\n",
        "                comparison_df.loc[low_conf_mask, manual_col]\n",
        "            ).mean()\n",
        "            confidence_analysis['low_confidence_agreement'] = low_conf_agreement\n",
        "\n",
        "        print(f\"\\nAgreement by Confidence:\")\n",
        "        if 'high_confidence_agreement' in confidence_analysis:\n",
        "            print(f\"  High confidence (≥0.8): {confidence_analysis['high_confidence_agreement']:.3f}\")\n",
        "        if 'low_confidence_agreement' in confidence_analysis:\n",
        "            print(f\"  Low confidence (<0.6): {confidence_analysis['low_confidence_agreement']:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'total_comparisons': len(comparison_df),\n",
        "        'agreement_rate': agreement,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': cm.tolist() if cm is not None else None,\n",
        "        'confusion_matrix_labels': labels if cm is not None else None,\n",
        "        'confidence_analysis': confidence_analysis\n",
        "    }\n",
        "\n",
        "# Compare manual labels with model predictions\n",
        "if manual_labels_df is not None:\n",
        "    model_comparison_results = compare_manual_vs_model_predictions(manual_labels_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu0Tuqk873e_",
        "outputId": "d6c908f7-d83f-46ad-fc55-ed3b1d04cd98"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MANUAL VS MODEL COMPARISON\n",
            "==================================================\n",
            "\n",
            "FinBERT-tone vs Manual Labels (200 comparisons)\n",
            "----------------------------------------\n",
            "Overall Agreement: 0.895\n",
            "Precision: 0.897\n",
            "Recall: 0.895\n",
            "F1-Score: 0.894\n",
            "\n",
            "Confusion Matrix:\n",
            "Manual\\Model: negative  neutral poistive positive\n",
            "    negative:       28        1        0        0\n",
            "     neutral:        9      113        0        4\n",
            "    poistive:        0        1        0        0\n",
            "    positive:        0        6        0       38\n",
            "\n",
            "Agreement by Confidence:\n",
            "  High confidence (≥0.8): 0.895\n",
            "\n",
            "ProsusAI vs Manual Labels (200 comparisons)\n",
            "----------------------------------------\n",
            "Overall Agreement: 0.855\n",
            "Precision: 0.855\n",
            "Recall: 0.855\n",
            "F1-Score: 0.853\n",
            "\n",
            "Confusion Matrix:\n",
            "Manual\\Model: negative  neutral poistive positive\n",
            "    negative:       20        8        0        1\n",
            "     neutral:        4      111        0       11\n",
            "    poistive:        0        0        0        1\n",
            "    positive:        0        4        0       40\n",
            "\n",
            "Agreement by Confidence:\n",
            "  High confidence (≥0.8): 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare Training/Validation Split\n",
        "\n",
        "def prepare_manual_data_for_training(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
        "    \"\"\"Prepare manually labeled data for model fine-tuning.\"\"\"\n",
        "    if df is None:\n",
        "        return None, None, {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PREPARING DATA FOR FINE-TUNING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Filter to manually labeled records\n",
        "    manually_labeled_mask = df['human_label'].notna() & (df['human_label'] != '')\n",
        "    manual_df = df[manually_labeled_mask].copy()\n",
        "\n",
        "    if len(manual_df) == 0:\n",
        "        print(\"No manually labeled data available for training\")\n",
        "        return None, None, {}\n",
        "\n",
        "    print(f\"Total manually labeled records: {len(manual_df)}\")\n",
        "\n",
        "    # Stratified split by label and confidence\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Create stratification groups based on label and confidence level\n",
        "    if 'human_confidence' in manual_df.columns:\n",
        "        manual_df['confidence_group'] = pd.cut(\n",
        "            manual_df['human_confidence'].fillna(0.5),\n",
        "            bins=[0, 0.6, 0.8, 1.0],\n",
        "            labels=['low', 'medium', 'high'],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "        manual_df['stratify_group'] = manual_df['human_label'].astype(str) + '_' + manual_df['confidence_group'].astype(str)\n",
        "        stratify_col = 'stratify_group'\n",
        "    else:\n",
        "        stratify_col = 'human_label'\n",
        "\n",
        "    # Split data (80/20 for training/validation)\n",
        "    try:\n",
        "        train_df, val_df = train_test_split(\n",
        "            manual_df,\n",
        "            test_size=0.2,\n",
        "            random_state=SEED,\n",
        "            stratify=manual_df[stratify_col]\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {len(train_df)} records\")\n",
        "        print(f\"Validation set: {len(val_df)} records\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        # If stratification fails (too few samples), do random split\n",
        "        print(f\"Stratification failed ({e}), using random split\")\n",
        "        train_df, val_df = train_test_split(\n",
        "            manual_df,\n",
        "            test_size=0.2,\n",
        "            random_state=SEED\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {len(train_df)} records\")\n",
        "        print(f\"Validation set: {len(val_df)} records\")\n",
        "\n",
        "    # Analyze splits\n",
        "    split_analysis = {\n",
        "        'total_manual_labels': len(manual_df),\n",
        "        'train_size': len(train_df),\n",
        "        'val_size': len(val_df),\n",
        "        'train_label_dist': train_df['human_label'].value_counts().to_dict(),\n",
        "        'val_label_dist': val_df['human_label'].value_counts().to_dict()\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTraining Label Distribution:\")\n",
        "    for label, count in split_analysis['train_label_dist'].items():\n",
        "        pct = (count / len(train_df)) * 100\n",
        "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nValidation Label Distribution:\")\n",
        "    for label, count in split_analysis['val_label_dist'].items():\n",
        "        pct = (count / len(val_df)) * 100\n",
        "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    return train_df, val_df, split_analysis\n",
        "\n",
        "# Prepare data for training\n",
        "if manual_labels_df is not None:\n",
        "    train_df, val_df, split_analysis = prepare_manual_data_for_training(manual_labels_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXTehWhB78oi",
        "outputId": "4021a618-2cc7-4075-edfc-81f6f0cc407a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PREPARING DATA FOR FINE-TUNING\n",
            "==================================================\n",
            "Total manually labeled records: 200\n",
            "Stratification failed (The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.), using random split\n",
            "Training set: 160 records\n",
            "Validation set: 40 records\n",
            "\n",
            "Training Label Distribution:\n",
            "  neutral: 105 (65.6%)\n",
            "  positive: 32 (20.0%)\n",
            "  negative: 22 (13.8%)\n",
            "  poistive: 1 (0.6%)\n",
            "\n",
            "Validation Label Distribution:\n",
            "  neutral: 21 (52.5%)\n",
            "  positive: 12 (30.0%)\n",
            "  negative: 7 (17.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Quality Assessment and Recommendations\n",
        "\n",
        "def assess_manual_data_quality(df: pd.DataFrame, validation_report: Dict, comparison_results: Dict) -> Dict:\n",
        "    \"\"\"Assess overall quality and provide recommendations.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MANUAL DATA QUALITY ASSESSMENT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    assessment = {\n",
        "        'overall_quality': 'unknown',\n",
        "        'recommendations': [],\n",
        "        'fine_tuning_readiness': False,\n",
        "        'key_metrics': {}\n",
        "    }\n",
        "\n",
        "    if df is None or not validation_report:\n",
        "        assessment['recommendations'].append(\"Manual labeling data not available\")\n",
        "        return assessment\n",
        "\n",
        "    # Extract key metrics\n",
        "    manually_labeled_count = validation_report.get('manually_labeled_count', 0)\n",
        "    total_records = validation_report.get('total_records', 0)\n",
        "\n",
        "    assessment['key_metrics'] = {\n",
        "        'manual_label_coverage': manually_labeled_count / total_records if total_records > 0 else 0,\n",
        "        'manual_label_count': manually_labeled_count,\n",
        "        'label_distribution': validation_report.get('label_distribution', {}),\n",
        "        'avg_confidence': validation_report.get('confidence_stats', {}).get('mean', 0)\n",
        "    }\n",
        "\n",
        "    # Assess data quantity\n",
        "    if manually_labeled_count < 50:\n",
        "        assessment['recommendations'].append(\"Very small dataset - consider active learning or data augmentation\")\n",
        "        quality_score = 1\n",
        "    elif manually_labeled_count < 100:\n",
        "        assessment['recommendations'].append(\"Small dataset - fine-tuning may have limited impact\")\n",
        "        quality_score = 2\n",
        "    elif manually_labeled_count < 500:\n",
        "        assessment['recommendations'].append(\"Moderate dataset size - suitable for fine-tuning\")\n",
        "        quality_score = 3\n",
        "    else:\n",
        "        assessment['recommendations'].append(\"Good dataset size for fine-tuning\")\n",
        "        quality_score = 4\n",
        "\n",
        "    # Assess label balance\n",
        "    label_dist = validation_report.get('label_distribution', {})\n",
        "    if label_dist:\n",
        "        label_counts = list(label_dist.values())\n",
        "        min_count = min(label_counts)\n",
        "        max_count = max(label_counts)\n",
        "        balance_ratio = min_count / max_count if max_count > 0 else 0\n",
        "\n",
        "        if balance_ratio < 0.1:\n",
        "            assessment['recommendations'].append(\"Highly imbalanced labels - consider resampling techniques\")\n",
        "            quality_score = min(quality_score, 2)\n",
        "        elif balance_ratio < 0.3:\n",
        "            assessment['recommendations'].append(\"Moderately imbalanced labels - use class weights in training\")\n",
        "            quality_score = min(quality_score, 3)\n",
        "        else:\n",
        "            assessment['recommendations'].append(\"Well-balanced label distribution\")\n",
        "\n",
        "    # Assess confidence scores\n",
        "    confidence_stats = validation_report.get('confidence_stats', {})\n",
        "    if confidence_stats:\n",
        "        avg_confidence = confidence_stats.get('mean', 0)\n",
        "        if avg_confidence < 0.6:\n",
        "            assessment['recommendations'].append(\"Low average confidence - review annotation quality\")\n",
        "            quality_score = min(quality_score, 2)\n",
        "        elif avg_confidence > 0.8:\n",
        "            assessment['recommendations'].append(\"High confidence annotations\")\n",
        "        else:\n",
        "            assessment['recommendations'].append(\"Moderate confidence annotations\")\n",
        "\n",
        "    # Assess model agreement\n",
        "    if comparison_results:\n",
        "        model_agreements = []\n",
        "        for model_name, results in comparison_results.items():\n",
        "            if isinstance(results, dict) and 'agreement_rate' in results:\n",
        "                agreement = results['agreement_rate']\n",
        "                model_agreements.append(agreement)\n",
        "\n",
        "                if agreement < 0.5:\n",
        "                    assessment['recommendations'].append(f\"Low agreement with {model_name} ({agreement:.2f}) - manual labels may be needed\")\n",
        "                elif agreement < 0.7:\n",
        "                    assessment['recommendations'].append(f\"Moderate agreement with {model_name} ({agreement:.2f}) - good for fine-tuning\")\n",
        "                else:\n",
        "                    assessment['recommendations'].append(f\"High agreement with {model_name} ({agreement:.2f}) - models already performing well\")\n",
        "\n",
        "        if model_agreements:\n",
        "            avg_agreement = np.mean(model_agreements)\n",
        "            assessment['key_metrics']['avg_model_agreement'] = avg_agreement\n",
        "\n",
        "    # Overall quality assessment\n",
        "    if quality_score >= 4:\n",
        "        assessment['overall_quality'] = 'excellent'\n",
        "        assessment['fine_tuning_readiness'] = True\n",
        "    elif quality_score >= 3:\n",
        "        assessment['overall_quality'] = 'good'\n",
        "        assessment['fine_tuning_readiness'] = True\n",
        "    elif quality_score >= 2:\n",
        "        assessment['overall_quality'] = 'fair'\n",
        "        assessment['fine_tuning_readiness'] = True\n",
        "        assessment['recommendations'].append(\"Proceed with caution - may need data augmentation\")\n",
        "    else:\n",
        "        assessment['overall_quality'] = 'poor'\n",
        "        assessment['fine_tuning_readiness'] = False\n",
        "        assessment['recommendations'].append(\"Not recommended for fine-tuning without more data\")\n",
        "\n",
        "    # Print assessment\n",
        "    print(f\"Overall Quality: {assessment['overall_quality'].upper()}\")\n",
        "    print(f\"Fine-tuning Ready: {'Yes' if assessment['fine_tuning_readiness'] else 'No'}\")\n",
        "    print(f\"\\nKey Metrics:\")\n",
        "    for metric, value in assessment['key_metrics'].items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {metric}: {value:.3f}\")\n",
        "        else:\n",
        "            print(f\"  {metric}: {value}\")\n",
        "\n",
        "    print(f\"\\nRecommendations:\")\n",
        "    for rec in assessment['recommendations']:\n",
        "        print(f\"  {rec}\")\n",
        "\n",
        "    return assessment\n",
        "\n",
        "# Assess overall quality\n",
        "if manual_labels_df is not None:\n",
        "    quality_assessment = assess_manual_data_quality(\n",
        "        manual_labels_df, validation_report, model_comparison_results\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0QOyp6I8DbX",
        "outputId": "89b9460c-4134-4ebe-aa8a-759ba808b8ac"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MANUAL DATA QUALITY ASSESSMENT\n",
            "============================================================\n",
            "Overall Quality: FAIR\n",
            "Fine-tuning Ready: Yes\n",
            "\n",
            "Key Metrics:\n",
            "  manual_label_coverage: 0.180\n",
            "  manual_label_count: 200\n",
            "  label_distribution: {'neutral': 126, 'positive': 44, 'negative': 29, 'poistive': 1}\n",
            "  avg_confidence: 3.695\n",
            "  avg_model_agreement: 0.875\n",
            "\n",
            "Recommendations:\n",
            "  Moderate dataset size - suitable for fine-tuning\n",
            "  Highly imbalanced labels - consider resampling techniques\n",
            "  High confidence annotations\n",
            "  High agreement with finbert_tone (0.90) - models already performing well\n",
            "  High agreement with prosus (0.85) - models already performing well\n",
            "  Proceed with caution - may need data augmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Validation Results\n",
        "\n",
        "def save_validation_results():\n",
        "    \"\"\"Save all validation results and prepared data.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAVING VALIDATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Save validation report\n",
        "    validation_report_path = manual_validation_path / \"manual_validation_report.json\"\n",
        "    with open(validation_report_path, 'w') as f:\n",
        "        json.dump(validation_report, f, indent=2, default=str)\n",
        "    print(f\"Validation report: {validation_report_path}\")\n",
        "\n",
        "    # Save model comparison results\n",
        "    if 'model_comparison_results' in locals():\n",
        "        comparison_path = manual_validation_path / \"manual_vs_model_comparison.json\"\n",
        "        with open(comparison_path, 'w') as f:\n",
        "            json.dump(model_comparison_results, f, indent=2, default=str)\n",
        "        print(f\"Model comparison: {comparison_path}\")\n",
        "\n",
        "    # Save quality assessment\n",
        "    if 'quality_assessment' in locals():\n",
        "        quality_path = manual_validation_path / \"data_quality_assessment.json\"\n",
        "        with open(quality_path, 'w') as f:\n",
        "            json.dump(quality_assessment, f, indent=2, default=str)\n",
        "        print(f\"Quality assessment: {quality_path}\")\n",
        "\n",
        "    # Save training/validation splits\n",
        "    if 'train_df' in locals() and train_df is not None:\n",
        "        train_path = manual_validation_path / \"train_manual_labels.csv\"\n",
        "        train_df.to_csv(train_path, index=False)\n",
        "        print(f\"Training data: {train_path} ({len(train_df)} records)\")\n",
        "\n",
        "    if 'val_df' in locals() and val_df is not None:\n",
        "        val_path = manual_validation_path / \"val_manual_labels.csv\"\n",
        "        val_df.to_csv(val_path, index=False)\n",
        "        print(f\"Validation data: {val_path} ({len(val_df)} records)\")\n",
        "\n",
        "    # Save full manual dataset to results directory for next notebooks\n",
        "    if manual_labels_df is not None:\n",
        "        full_manual_path = results_sentiment_path / \"sentiment_sentence_jpm_multi_2025_validated.csv\"\n",
        "        manual_labels_df.to_csv(full_manual_path, index=False)\n",
        "        print(f\"Full validated dataset: {full_manual_path}\")\n",
        "\n",
        "# Save all results\n",
        "save_validation_results()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z22wgZtN8IVM",
        "outputId": "631e23ae-81df-4562-f7c3-aa9d49ca798c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "SAVING VALIDATION RESULTS\n",
            "==================================================\n",
            "Validation report: /content/drive/MyDrive/CAM_DS_AI_Project/data/manual_validation/jpm/manual_validation_report.json\n",
            "Full validated dataset: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/sentiment_sentence_jpm_multi_2025_validated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Summary and Next Steps\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MANUAL VALIDATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if manual_labels_df is not None:\n",
        "    manually_labeled_count = validation_report.get('manually_labeled_count', 0)\n",
        "    total_records = validation_report.get('total_records', 0)\n",
        "    coverage = manually_labeled_count / total_records if total_records > 0 else 0\n",
        "\n",
        "    print(f\"Summary:\")\n",
        "    print(f\"  Total records: {total_records:,}\")\n",
        "    print(f\"  Manually labeled: {manually_labeled_count:,} ({coverage:.1%})\")\n",
        "\n",
        "    if 'quality_assessment' in locals():\n",
        "        print(f\"  Data quality: {quality_assessment['overall_quality']}\")\n",
        "        print(f\"  Fine-tuning ready: {'Yes' if quality_assessment['fine_tuning_readiness'] else 'No'}\")\n",
        "\n",
        "    if 'train_df' in locals() and train_df is not None:\n",
        "        print(f\"  Training samples: {len(train_df)}\")\n",
        "        print(f\"  Validation samples: {len(val_df)}\")\n",
        "\n",
        "    print(f\"\\nFiles created:\")\n",
        "    print(f\"  Manual validation data: {manual_validation_path}\")\n",
        "    print(f\"  Training/validation splits: Ready for fine-tuning\")\n",
        "\n",
        "    print(f\"\\nNext steps:\")\n",
        "    if quality_assessment.get('fine_tuning_readiness', False):\n",
        "        print(f\"  Proceed to 04b_model_finetuning.ipynb\")\n",
        "        print(f\"  Use prepared train/val splits for fine-tuning\")\n",
        "    else:\n",
        "        print(f\"  Consider collecting more manual labels\")\n",
        "        print(f\"  Review data quality issues before fine-tuning\")\n",
        "\n",
        "    print(f\"  Continue with enhanced sentiment analysis in 04_sentiment_analysis_jpm_enhanced.ipynb\")\n",
        "\n",
        "else:\n",
        "    print(\"Manual validation failed - no data available\")\n",
        "    print(\"   Check Google Drive URL and file permissions\")\n",
        "    print(\"   Ensure manual labeling is complete\")\n",
        "\n",
        "print(f\"\\nManual validation process complete!\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu0QAtITTrAK",
        "outputId": "c320e851-ceb6-4fdb-d63f-04f35b843c28"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MANUAL VALIDATION COMPLETE\n",
            "============================================================\n",
            "Summary:\n",
            "  Total records: 1,110\n",
            "  Manually labeled: 200 (18.0%)\n",
            "  Data quality: fair\n",
            "  Fine-tuning ready: Yes\n",
            "  Training samples: 160\n",
            "  Validation samples: 40\n",
            "\n",
            "Files created:\n",
            "  Manual validation data: /content/drive/MyDrive/CAM_DS_AI_Project/data/manual_validation/jpm\n",
            "  Training/validation splits: Ready for fine-tuning\n",
            "\n",
            "Next steps:\n",
            "  Proceed to 04b_model_finetuning.ipynb\n",
            "  Use prepared train/val splits for fine-tuning\n",
            "  Continue with enhanced sentiment analysis in 04_sentiment_analysis_jpm_enhanced.ipynb\n",
            "\n",
            "Manual validation process complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixk2HgcxTuh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}