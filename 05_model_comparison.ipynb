{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sGbUTbmV4Ck",
        "outputId": "c2a57b42-1dc1-4590-b136-c5c7396ce68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model comparison analysis for bank: JPM\n"
          ]
        }
      ],
      "source": [
        "# 05_model_comparison.ipynb\n",
        "# Purpose: Compare FinBERT models and analyze performance differences\n",
        "# Input: sentiment analysis results from 04_sentiment_analysis.ipynb\n",
        "# Output: model comparison metrics, agreement analysis, disagreement patterns\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Location A: Google Drive (Primary drive)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project/config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "SEED = config[\"SEED\"]\n",
        "BANK_CODE = config[\"BANK_CODE\"]\n",
        "drive_base = Path(config[\"drive_base\"])\n",
        "colab_base = Path(config[\"colab_base\"])\n",
        "\n",
        "print(f\"Model comparison analysis for bank: {BANK_CODE.upper()}\")\n",
        "\n",
        "## Define Paths\n",
        "\n",
        "results_sentiment_path = drive_base / \"results/sentiment/jpm\"\n",
        "results_comparison_path = drive_base / \"results/comparison/jpm\"\n",
        "viz_path = drive_base / \"outputs/visualizations/jpm\"\n",
        "\n",
        "# Ensure directories exist\n",
        "results_comparison_path.mkdir(parents=True, exist_ok=True)\n",
        "viz_path.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Load Sentiment Analysis Results\n",
        "\n",
        "def load_sentiment_results(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"Load sentiment analysis results with error handling.\"\"\"\n",
        "    file_path = results_sentiment_path / filename\n",
        "\n",
        "    if not file_path.exists():\n",
        "        print(f\"‚ùå File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"‚úì Loaded {filename}: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {filename}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"Loading sentiment analysis results...\")\n",
        "\n",
        "# Load sentence-level results\n",
        "sentiment_jpm_q1_2025_df = load_sentiment_results(\"sentiment_sentence_jpm_q1_2025.csv\")\n",
        "sentiment_jpm_q2_2025_df = load_sentiment_results(\"sentiment_sentence_jpm_q2_2025.csv\")\n",
        "sentiment_jpm_multi_2025_df = load_sentiment_results(\"sentiment_sentence_jpm_multi_2025.csv\")\n",
        "\n",
        "# Load aggregated results\n",
        "qa_level_jpm_multi_2025_df = load_sentiment_results(\"sentiment_qa_jpm_multi_2025.csv\")\n",
        "speaker_level_jpm_multi_2025_df = load_sentiment_results(\"sentiment_speaker_jpm_multi_2025.csv\")\n",
        "topic_level_jpm_multi_2025_df = load_sentiment_results(\"sentiment_topic_jpm_multi_2025.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PuqhjyeWbZv",
        "outputId": "22037323-5219-4b2b-a9c1-456c22c13157"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sentiment analysis results...\n",
            "‚úì Loaded sentiment_sentence_jpm_q1_2025.csv: (578, 37)\n",
            "‚úì Loaded sentiment_sentence_jpm_q2_2025.csv: (532, 37)\n",
            "‚úì Loaded sentiment_sentence_jpm_multi_2025.csv: (1110, 37)\n",
            "‚úì Loaded sentiment_qa_jpm_multi_2025.csv: (218, 27)\n",
            "‚úì Loaded sentiment_speaker_jpm_multi_2025.csv: (6, 21)\n",
            "‚úì Loaded sentiment_topic_jpm_multi_2025.csv: (16, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model Agreement Analysis\n",
        "\n",
        "class ModelComparisonAnalyzer:\n",
        "    \"\"\"Analyzer for comparing FinBERT model performance and agreement.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.finbert_tone_col = 'finbert_tone_label'\n",
        "        self.prosus_col = 'prosus_label'\n",
        "        self.finbert_score_col = 'finbert_tone_score'\n",
        "        self.prosus_score_col = 'prosus_score'\n",
        "\n",
        "    def calculate_agreement_metrics(self) -> Dict:\n",
        "        \"\"\"Calculate inter-model agreement metrics.\"\"\"\n",
        "        if self.df is None:\n",
        "            return {}\n",
        "\n",
        "        # Filter out rows with missing predictions\n",
        "        valid_df = self.df.dropna(subset=[self.finbert_tone_col, self.prosus_col])\n",
        "\n",
        "        if len(valid_df) == 0:\n",
        "            print(\"‚ùå No valid predictions found for comparison\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"Calculating agreement metrics for {len(valid_df)} predictions...\")\n",
        "\n",
        "        # Basic agreement rate\n",
        "        agreement = valid_df[self.finbert_tone_col] == valid_df[self.prosus_col]\n",
        "        agreement_rate = agreement.mean()\n",
        "\n",
        "        # Cohen's Kappa (accounting for chance agreement)\n",
        "        kappa = cohen_kappa_score(\n",
        "            valid_df[self.finbert_tone_col],\n",
        "            valid_df[self.prosus_col]\n",
        "        )\n",
        "\n",
        "        # Correlation between confidence scores\n",
        "        score_correlation = None\n",
        "        if self.finbert_score_col in valid_df.columns and self.prosus_score_col in valid_df.columns:\n",
        "            finbert_scores = valid_df[self.finbert_score_col].fillna(0)\n",
        "            prosus_scores = valid_df[self.prosus_score_col].fillna(0)\n",
        "\n",
        "            pearson_r, pearson_p = pearsonr(finbert_scores, prosus_scores)\n",
        "            spearman_r, spearman_p = spearmanr(finbert_scores, prosus_scores)\n",
        "\n",
        "            score_correlation = {\n",
        "                'pearson': {'r': pearson_r, 'p_value': pearson_p},\n",
        "                'spearman': {'r': spearman_r, 'p_value': spearman_p}\n",
        "            }\n",
        "\n",
        "        # Agreement by sentiment class\n",
        "        agreement_by_class = {}\n",
        "        for sentiment in ['positive', 'neutral', 'negative']:\n",
        "            mask = valid_df[self.finbert_tone_col] == sentiment\n",
        "            if mask.sum() > 0:\n",
        "                class_agreement = (\n",
        "                    valid_df.loc[mask, self.finbert_tone_col] ==\n",
        "                    valid_df.loc[mask, self.prosus_col]\n",
        "                ).mean()\n",
        "                agreement_by_class[sentiment] = class_agreement\n",
        "\n",
        "        metrics = {\n",
        "            'total_predictions': len(valid_df),\n",
        "            'agreement_rate': agreement_rate,\n",
        "            'cohen_kappa': kappa,\n",
        "            'score_correlation': score_correlation,\n",
        "            'agreement_by_class': agreement_by_class\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def analyze_distribution_differences(self) -> Dict:\n",
        "        \"\"\"Analyze sentiment distribution differences between models.\"\"\"\n",
        "        if self.df is None:\n",
        "            return {}\n",
        "\n",
        "        valid_df = self.df.dropna(subset=[self.finbert_tone_col, self.prosus_col])\n",
        "\n",
        "        if len(valid_df) == 0:\n",
        "            return {}\n",
        "\n",
        "        print(\"Analyzing sentiment distribution differences...\")\n",
        "\n",
        "        # Distribution for each model\n",
        "        finbert_dist = valid_df[self.finbert_tone_col].value_counts(normalize=True).sort_index()\n",
        "        prosus_dist = valid_df[self.prosus_col].value_counts(normalize=True).sort_index()\n",
        "\n",
        "        # Ensure same index\n",
        "        all_sentiments = set(finbert_dist.index).union(set(prosus_dist.index))\n",
        "        finbert_dist = finbert_dist.reindex(all_sentiments, fill_value=0)\n",
        "        prosus_dist = prosus_dist.reindex(all_sentiments, fill_value=0)\n",
        "\n",
        "        # Chi-square test for distribution similarity\n",
        "        try:\n",
        "            chi2_stat, chi2_p = stats.chisquare(finbert_dist.values, prosus_dist.values)\n",
        "        except:\n",
        "            chi2_stat, chi2_p = None, None\n",
        "\n",
        "        # Distribution differences\n",
        "        dist_diff = (finbert_dist - prosus_dist).abs()\n",
        "\n",
        "        distribution_analysis = {\n",
        "            'finbert_tone_distribution': finbert_dist.to_dict(),\n",
        "            'prosus_distribution': prosus_dist.to_dict(),\n",
        "            'distribution_differences': dist_diff.to_dict(),\n",
        "            'chi_square_test': {\n",
        "                'statistic': chi2_stat,\n",
        "                'p_value': chi2_p\n",
        "            } if chi2_stat is not None else None,\n",
        "            'max_difference': dist_diff.max(),\n",
        "            'total_variation_distance': dist_diff.sum() / 2\n",
        "        }\n",
        "\n",
        "        return distribution_analysis\n",
        "\n",
        "    def analyze_confidence_patterns(self) -> Dict:\n",
        "        \"\"\"Analyze confidence score patterns.\"\"\"\n",
        "        if self.df is None:\n",
        "            return {}\n",
        "\n",
        "        valid_df = self.df.dropna(subset=[self.finbert_score_col, self.prosus_score_col])\n",
        "\n",
        "        if len(valid_df) == 0:\n",
        "            return {}\n",
        "\n",
        "        print(\"Analyzing confidence score patterns...\")\n",
        "\n",
        "        # Confidence statistics by model\n",
        "        finbert_confidence = {\n",
        "            'mean': valid_df[self.finbert_score_col].mean(),\n",
        "            'std': valid_df[self.finbert_score_col].std(),\n",
        "            'median': valid_df[self.finbert_score_col].median(),\n",
        "            'min': valid_df[self.finbert_score_col].min(),\n",
        "            'max': valid_df[self.finbert_score_col].max()\n",
        "        }\n",
        "\n",
        "        prosus_confidence = {\n",
        "            'mean': valid_df[self.prosus_score_col].mean(),\n",
        "            'std': valid_df[self.prosus_score_col].std(),\n",
        "            'median': valid_df[self.prosus_score_col].median(),\n",
        "            'min': valid_df[self.prosus_score_col].min(),\n",
        "            'max': valid_df[self.prosus_score_col].max()\n",
        "        }\n",
        "\n",
        "        # Confidence by sentiment class\n",
        "        finbert_conf_by_class = {}\n",
        "        prosus_conf_by_class = {}\n",
        "\n",
        "        for sentiment in ['positive', 'neutral', 'negative']:\n",
        "            finbert_mask = valid_df[self.finbert_tone_col] == sentiment\n",
        "            prosus_mask = valid_df[self.prosus_col] == sentiment\n",
        "\n",
        "            if finbert_mask.sum() > 0:\n",
        "                finbert_conf_by_class[sentiment] = valid_df.loc[finbert_mask, self.finbert_score_col].mean()\n",
        "\n",
        "            if prosus_mask.sum() > 0:\n",
        "                prosus_conf_by_class[sentiment] = valid_df.loc[prosus_mask, self.prosus_score_col].mean()\n",
        "\n",
        "        # Confidence difference analysis\n",
        "        conf_diff = valid_df[self.finbert_score_col] - valid_df[self.prosus_score_col]\n",
        "\n",
        "        confidence_analysis = {\n",
        "            'finbert_tone_confidence': finbert_confidence,\n",
        "            'prosus_confidence': prosus_confidence,\n",
        "            'finbert_conf_by_class': finbert_conf_by_class,\n",
        "            'prosus_conf_by_class': prosus_conf_by_class,\n",
        "            'confidence_difference': {\n",
        "                'mean': conf_diff.mean(),\n",
        "                'std': conf_diff.std(),\n",
        "                'median': conf_diff.median()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return confidence_analysis\n",
        "\n",
        "    def analyze_disagreement_patterns(self) -> Dict:\n",
        "        \"\"\"Analyze patterns in model disagreements.\"\"\"\n",
        "        if self.df is None:\n",
        "            return {}\n",
        "\n",
        "        valid_df = self.df.dropna(subset=[self.finbert_tone_col, self.prosus_col])\n",
        "\n",
        "        if len(valid_df) == 0:\n",
        "            return {}\n",
        "\n",
        "        print(\"Analyzing disagreement patterns...\")\n",
        "\n",
        "        # Find disagreements\n",
        "        disagreements = valid_df[valid_df[self.finbert_tone_col] != valid_df[self.prosus_col]].copy()\n",
        "\n",
        "        if len(disagreements) == 0:\n",
        "            return {'total_disagreements': 0}\n",
        "\n",
        "        # Disagreement patterns\n",
        "        disagreement_patterns = disagreements.groupby([self.finbert_tone_col, self.prosus_col]).size()\n",
        "\n",
        "        # Most common disagreements\n",
        "        top_disagreements = disagreement_patterns.nlargest(5).to_dict()\n",
        "\n",
        "        # Disagreement by text characteristics\n",
        "        disagreement_analysis = {\n",
        "            'total_disagreements': len(disagreements),\n",
        "            'disagreement_rate': len(disagreements) / len(valid_df),\n",
        "            'disagreement_patterns': disagreement_patterns.to_dict(),\n",
        "            'top_disagreement_pairs': top_disagreements\n",
        "        }\n",
        "\n",
        "        # Add text length analysis if available\n",
        "        if 'sentence_length' in disagreements.columns:\n",
        "            disagreement_analysis['avg_disagreement_text_length'] = disagreements['sentence_length'].mean()\n",
        "            agreement_df = valid_df[valid_df[self.finbert_tone_col] == valid_df[self.prosus_col]]\n",
        "            disagreement_analysis['avg_agreement_text_length'] = agreement_df['sentence_length'].mean()\n",
        "\n",
        "        # Add topic analysis if available\n",
        "        if 'primary_topic' in disagreements.columns:\n",
        "            topic_disagreements = disagreements['primary_topic'].value_counts(normalize=True)\n",
        "            disagreement_analysis['disagreement_by_topic'] = topic_disagreements.to_dict()\n",
        "\n",
        "        return disagreement_analysis\n",
        "\n",
        "# Run comprehensive model comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Analyze each dataset\n",
        "comparison_results = {}\n",
        "\n",
        "datasets_to_analyze = [\n",
        "    (\"Q1 2025\", sentiment_jpm_q1_2025_df),\n",
        "    (\"Q2 2025\", sentiment_jpm_q2_2025_df),\n",
        "    (\"Multi 2025\", sentiment_jpm_multi_2025_df)\n",
        "]\n",
        "\n",
        "for dataset_name, df in datasets_to_analyze:\n",
        "    if df is not None:\n",
        "        print(f\"\\nüîç ANALYZING {dataset_name}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        analyzer = ModelComparisonAnalyzer(df)\n",
        "\n",
        "        # Calculate all metrics\n",
        "        agreement_metrics = analyzer.calculate_agreement_metrics()\n",
        "        distribution_analysis = analyzer.analyze_distribution_differences()\n",
        "        confidence_analysis = analyzer.analyze_confidence_patterns()\n",
        "        disagreement_analysis = analyzer.analyze_disagreement_patterns()\n",
        "\n",
        "        comparison_results[dataset_name] = {\n",
        "            'agreement_metrics': agreement_metrics,\n",
        "            'distribution_analysis': distribution_analysis,\n",
        "            'confidence_analysis': confidence_analysis,\n",
        "            'disagreement_analysis': disagreement_analysis\n",
        "        }\n",
        "\n",
        "        # Print key results\n",
        "        if agreement_metrics:\n",
        "            print(f\"  Agreement rate: {agreement_metrics['agreement_rate']:.3f}\")\n",
        "            print(f\"  Cohen's Kappa: {agreement_metrics['cohen_kappa']:.3f}\")\n",
        "            if agreement_metrics['score_correlation']:\n",
        "                print(f\"  Score correlation (Pearson): {agreement_metrics['score_correlation']['pearson']['r']:.3f}\")\n",
        "\n",
        "        if disagreement_analysis:\n",
        "            print(f\"  Disagreement rate: {disagreement_analysis['disagreement_rate']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzbDLHyIWclp",
        "outputId": "8ef36614-bce3-4d1e-8e96-768dc32d0439"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL COMPARISON ANALYSIS\n",
            "============================================================\n",
            "\n",
            "üîç ANALYZING Q1 2025\n",
            "----------------------------------------\n",
            "Calculating agreement metrics for 578 predictions...\n",
            "Analyzing sentiment distribution differences...\n",
            "Analyzing confidence score patterns...\n",
            "Analyzing disagreement patterns...\n",
            "  Agreement rate: 0.747\n",
            "  Cohen's Kappa: 0.442\n",
            "  Score correlation (Pearson): 0.188\n",
            "  Disagreement rate: 0.253\n",
            "\n",
            "üîç ANALYZING Q2 2025\n",
            "----------------------------------------\n",
            "Calculating agreement metrics for 532 predictions...\n",
            "Analyzing sentiment distribution differences...\n",
            "Analyzing confidence score patterns...\n",
            "Analyzing disagreement patterns...\n",
            "  Agreement rate: 0.776\n",
            "  Cohen's Kappa: 0.419\n",
            "  Score correlation (Pearson): 0.205\n",
            "  Disagreement rate: 0.224\n",
            "\n",
            "üîç ANALYZING Multi 2025\n",
            "----------------------------------------\n",
            "Calculating agreement metrics for 1110 predictions...\n",
            "Analyzing sentiment distribution differences...\n",
            "Analyzing confidence score patterns...\n",
            "Analyzing disagreement patterns...\n",
            "  Agreement rate: 0.761\n",
            "  Cohen's Kappa: 0.433\n",
            "  Score correlation (Pearson): 0.196\n",
            "  Disagreement rate: 0.239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Financial Context Metrics\n",
        "\n",
        "def analyze_financial_context_performance(df: pd.DataFrame, dataset_name: str) -> Dict:\n",
        "    \"\"\"Analyze model performance in financial context.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(f\"Analyzing financial context performance for {dataset_name}...\")\n",
        "\n",
        "    # Financial keywords that should correlate with sentiment\n",
        "    positive_financial_keywords = [\n",
        "        'growth', 'profit', 'increase', 'strong', 'improved', 'positive',\n",
        "        'success', 'beat', 'outperform', 'exceed'\n",
        "    ]\n",
        "\n",
        "    negative_financial_keywords = [\n",
        "        'loss', 'decline', 'decrease', 'weak', 'poor', 'negative',\n",
        "        'miss', 'underperform', 'below', 'concern', 'risk'\n",
        "    ]\n",
        "\n",
        "    # Create keyword-based expected sentiment\n",
        "    df_analysis = df.copy()\n",
        "\n",
        "    # Count positive and negative keywords\n",
        "    text_col = 'text' if 'text' in df.columns else 'text_clean'\n",
        "    if text_col not in df.columns:\n",
        "        return {}\n",
        "\n",
        "    df_analysis['positive_keywords'] = df_analysis[text_col].str.lower().str.count(\n",
        "        '|'.join(positive_financial_keywords)\n",
        "    )\n",
        "    df_analysis['negative_keywords'] = df_analysis[text_col].str.lower().str.count(\n",
        "        '|'.join(negative_financial_keywords)\n",
        "    )\n",
        "\n",
        "    # Expected sentiment based on keyword balance\n",
        "    df_analysis['keyword_sentiment_score'] = (\n",
        "        df_analysis['positive_keywords'] - df_analysis['negative_keywords']\n",
        "    )\n",
        "\n",
        "    # Categorize expected sentiment\n",
        "    df_analysis['expected_sentiment'] = 'neutral'\n",
        "    df_analysis.loc[df_analysis['keyword_sentiment_score'] > 0, 'expected_sentiment'] = 'positive'\n",
        "    df_analysis.loc[df_analysis['keyword_sentiment_score'] < 0, 'expected_sentiment'] = 'negative'\n",
        "\n",
        "    # Calculate alignment with keyword-based expectations\n",
        "    finbert_alignment = (\n",
        "        df_analysis['finbert_tone_label'] == df_analysis['expected_sentiment']\n",
        "    ).mean()\n",
        "\n",
        "    prosus_alignment = (\n",
        "        df_analysis['prosus_label'] == df_analysis['expected_sentiment']\n",
        "    ).mean()\n",
        "\n",
        "    # Analyze performance by financial topic\n",
        "    topic_performance = {}\n",
        "    if 'primary_topic' in df_analysis.columns:\n",
        "        for topic in df_analysis['primary_topic'].unique():\n",
        "            topic_mask = df_analysis['primary_topic'] == topic\n",
        "            topic_df = df_analysis[topic_mask]\n",
        "\n",
        "            if len(topic_df) > 10:  # Only analyze topics with sufficient data\n",
        "                # Model agreement within topic\n",
        "                topic_agreement = (\n",
        "                    topic_df['finbert_tone_label'] == topic_df['prosus_label']\n",
        "                ).mean()\n",
        "\n",
        "                # Keyword alignment within topic\n",
        "                finbert_topic_alignment = (\n",
        "                    topic_df['finbert_tone_label'] == topic_df['expected_sentiment']\n",
        "                ).mean()\n",
        "\n",
        "                prosus_topic_alignment = (\n",
        "                    topic_df['prosus_label'] == topic_df['expected_sentiment']\n",
        "                ).mean()\n",
        "\n",
        "                topic_performance[topic] = {\n",
        "                    'sample_size': len(topic_df),\n",
        "                    'model_agreement': topic_agreement,\n",
        "                    'finbert_keyword_alignment': finbert_topic_alignment,\n",
        "                    'prosus_keyword_alignment': prosus_topic_alignment\n",
        "                }\n",
        "\n",
        "    financial_context_metrics = {\n",
        "        'keyword_based_alignment': {\n",
        "            'finbert_tone': finbert_alignment,\n",
        "            'prosus': prosus_alignment\n",
        "        },\n",
        "        'topic_performance': topic_performance,\n",
        "        'keyword_distribution': {\n",
        "            'texts_with_positive_keywords': (df_analysis['positive_keywords'] > 0).sum(),\n",
        "            'texts_with_negative_keywords': (df_analysis['negative_keywords'] > 0).sum(),\n",
        "            'texts_with_mixed_keywords': (\n",
        "                (df_analysis['positive_keywords'] > 0) &\n",
        "                (df_analysis['negative_keywords'] > 0)\n",
        "            ).sum()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return financial_context_metrics\n"
      ],
      "metadata": {
        "id": "Tdeer1W0WwKe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze financial context performance\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINANCIAL CONTEXT ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for dataset_name, df in datasets_to_analyze:\n",
        "    if df is not None:\n",
        "        financial_metrics = analyze_financial_context_performance(df, dataset_name)\n",
        "        comparison_results[dataset_name]['financial_context_metrics'] = financial_metrics\n",
        "\n",
        "        if financial_metrics and 'keyword_based_alignment' in financial_metrics:\n",
        "            finbert_align = financial_metrics['keyword_based_alignment']['finbert_tone']\n",
        "            prosus_align = financial_metrics['keyword_based_alignment']['prosus']\n",
        "            print(f\"  {dataset_name} - FinBERT keyword alignment: {finbert_align:.3f}\")\n",
        "            print(f\"  {dataset_name} - ProsusAI keyword alignment: {prosus_align:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1HclLd2W058",
        "outputId": "c48a5c72-439c-458d-951f-a9a1d9581f8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "FINANCIAL CONTEXT ANALYSIS\n",
            "==================================================\n",
            "Analyzing financial context performance for Q1 2025...\n",
            "  Q1 2025 - FinBERT keyword alignment: 0.687\n",
            "  Q1 2025 - ProsusAI keyword alignment: 0.735\n",
            "Analyzing financial context performance for Q2 2025...\n",
            "  Q2 2025 - FinBERT keyword alignment: 0.722\n",
            "  Q2 2025 - ProsusAI keyword alignment: 0.789\n",
            "Analyzing financial context performance for Multi 2025...\n",
            "  Multi 2025 - FinBERT keyword alignment: 0.704\n",
            "  Multi 2025 - ProsusAI keyword alignment: 0.761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Volatility Correlation Analysis (Placeholder)\n",
        "\n",
        "def analyze_volatility_correlation(df: pd.DataFrame, dataset_name: str) -> Dict:\n",
        "    \"\"\"Analyze correlation with market volatility (placeholder for future stock data).\"\"\"\n",
        "    print(f\"Placeholder: Volatility correlation analysis for {dataset_name}\")\n",
        "\n",
        "    # This would integrate with stock price data when available\n",
        "    # For now, we create a framework for future integration\n",
        "\n",
        "    volatility_metrics = {\n",
        "        'note': 'Placeholder for stock price correlation analysis',\n",
        "        'framework_ready': True,\n",
        "        'required_data': [\n",
        "            'Stock price data for JPM during Q1 and Q2 2025',\n",
        "            'Market volatility indices',\n",
        "            'Earnings announcement dates'\n",
        "        ],\n",
        "        'analysis_approach': [\n",
        "            'Calculate sentiment scores around earnings dates',\n",
        "            'Correlate with stock price movements',\n",
        "            'Analyze sentiment-volatility relationship',\n",
        "            'Compare model sensitivity to market-relevant sentiment'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return volatility_metrics\n",
        "\n",
        "# Add volatility analysis placeholder\n",
        "for dataset_name in comparison_results:\n",
        "    volatility_metrics = analyze_volatility_correlation(None, dataset_name)\n",
        "    comparison_results[dataset_name]['volatility_correlation'] = volatility_metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTlsPSl7W41f",
        "outputId": "eba00cbf-eb0a-4e1d-fc2c-5e405e3e00df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Placeholder: Volatility correlation analysis for Q1 2025\n",
            "Placeholder: Volatility correlation analysis for Q2 2025\n",
            "Placeholder: Volatility correlation analysis for Multi 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Qualitative Disagreement Analysis\n",
        "\n",
        "def perform_qualitative_disagreement_analysis(df: pd.DataFrame, dataset_name: str, sample_size: int = 20) -> Dict:\n",
        "    \"\"\"Perform qualitative analysis of model disagreements.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(f\"Performing qualitative disagreement analysis for {dataset_name}...\")\n",
        "\n",
        "    # Find disagreements\n",
        "    disagreements = df[df['finbert_tone_label'] != df['prosus_label']].copy()\n",
        "\n",
        "    if len(disagreements) == 0:\n",
        "        return {'note': 'No disagreements found'}\n",
        "\n",
        "    # Sample for qualitative analysis\n",
        "    sample_disagreements = disagreements.sample(\n",
        "        min(sample_size, len(disagreements)),\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Categorize disagreement types\n",
        "    disagreement_types = sample_disagreements.groupby([\n",
        "        'finbert_tone_label', 'prosus_label'\n",
        "    ]).size().to_dict()\n",
        "\n",
        "    # Extract text examples for each disagreement type\n",
        "    examples = {}\n",
        "    for (finbert_pred, prosus_pred), count in disagreement_types.items():\n",
        "        mask = (\n",
        "            (sample_disagreements['finbert_tone_label'] == finbert_pred) &\n",
        "            (sample_disagreements['prosus_label'] == prosus_pred)\n",
        "        )\n",
        "\n",
        "        examples_for_type = sample_disagreements[mask]['text'].head(3).tolist()\n",
        "\n",
        "        examples[f\"{finbert_pred}_vs_{prosus_pred}\"] = {\n",
        "            'count': count,\n",
        "            'examples': examples_for_type\n",
        "        }\n",
        "\n",
        "    # Analyze text characteristics of disagreements\n",
        "    text_characteristics = {}\n",
        "    if 'sentence_length' in disagreements.columns:\n",
        "        text_characteristics['avg_length'] = disagreements['sentence_length'].mean()\n",
        "        text_characteristics['length_std'] = disagreements['sentence_length'].std()\n",
        "\n",
        "    if 'sentence_word_count' in disagreements.columns:\n",
        "        text_characteristics['avg_words'] = disagreements['sentence_word_count'].mean()\n",
        "        text_characteristics['word_std'] = disagreements['sentence_word_count'].std()\n",
        "\n",
        "    qualitative_analysis = {\n",
        "        'total_disagreements_analyzed': len(sample_disagreements),\n",
        "        'disagreement_types': disagreement_types,\n",
        "        'text_examples': examples,\n",
        "        'text_characteristics': text_characteristics\n",
        "    }\n",
        "\n",
        "    return qualitative_analysis\n",
        "\n",
        "# Perform qualitative analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"QUALITATIVE DISAGREEMENT ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for dataset_name, df in datasets_to_analyze:\n",
        "    if df is not None:\n",
        "        qualitative_analysis = perform_qualitative_disagreement_analysis(df, dataset_name)\n",
        "        comparison_results[dataset_name]['qualitative_analysis'] = qualitative_analysis\n",
        "\n",
        "        if 'disagreement_types' in qualitative_analysis:\n",
        "            print(f\"\\n{dataset_name} - Top disagreement patterns:\")\n",
        "            for pattern, count in list(qualitative_analysis['disagreement_types'].items())[:3]:\n",
        "                print(f\"  {pattern}: {count} cases\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LPESGY2W9h_",
        "outputId": "91190f08-1a56-44de-88f5-be32d90f89c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "QUALITATIVE DISAGREEMENT ANALYSIS\n",
            "==================================================\n",
            "Performing qualitative disagreement analysis for Q1 2025...\n",
            "\n",
            "Q1 2025 - Top disagreement patterns:\n",
            "  ('negative', 'neutral'): 9 cases\n",
            "  ('neutral', 'negative'): 1 cases\n",
            "  ('neutral', 'positive'): 4 cases\n",
            "Performing qualitative disagreement analysis for Q2 2025...\n",
            "\n",
            "Q2 2025 - Top disagreement patterns:\n",
            "  ('negative', 'neutral'): 3 cases\n",
            "  ('neutral', 'negative'): 1 cases\n",
            "  ('neutral', 'positive'): 5 cases\n",
            "Performing qualitative disagreement analysis for Multi 2025...\n",
            "\n",
            "Multi 2025 - Top disagreement patterns:\n",
            "  ('negative', 'neutral'): 7 cases\n",
            "  ('negative', 'positive'): 2 cases\n",
            "  ('neutral', 'positive'): 4 cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Research Questions Analysis\n",
        "\n",
        "def analyze_research_questions(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"Analyze key research questions using model comparison results.\"\"\"\n",
        "    if df is None:\n",
        "        return {}\n",
        "\n",
        "    print(\"Analyzing key research questions...\")\n",
        "\n",
        "    research_analysis = {}\n",
        "\n",
        "    # Question 1: Do bankers and analysts show diverging sentiment?\n",
        "    if 'speaker_role' in df.columns:\n",
        "        speaker_sentiment_analysis = {}\n",
        "\n",
        "        for model_col in ['finbert_tone_label', 'prosus_label']:\n",
        "            if model_col in df.columns:\n",
        "                model_name = 'finbert_tone' if 'finbert' in model_col else 'prosus'\n",
        "\n",
        "                # Calculate sentiment distribution by speaker role\n",
        "                speaker_dist = df.groupby('speaker_role')[model_col].value_counts(normalize=True).unstack(fill_value=0)\n",
        "\n",
        "                # Calculate divergence metrics\n",
        "                if 'analyst' in speaker_dist.index and 'executive' in speaker_dist.index:\n",
        "                    analyst_sentiment = speaker_dist.loc['analyst']\n",
        "                    exec_sentiment = speaker_dist.loc['executive']\n",
        "\n",
        "                    # Total variation distance\n",
        "                    divergence = 0.5 * (analyst_sentiment - exec_sentiment).abs().sum()\n",
        "\n",
        "                    # Positive sentiment difference\n",
        "                    pos_diff = analyst_sentiment.get('positive', 0) - exec_sentiment.get('positive', 0)\n",
        "\n",
        "                    speaker_sentiment_analysis[model_name] = {\n",
        "                        'analyst_distribution': analyst_sentiment.to_dict(),\n",
        "                        'executive_distribution': exec_sentiment.to_dict(),\n",
        "                        'total_variation_distance': divergence,\n",
        "                        'positive_sentiment_difference': pos_diff\n",
        "                    }\n",
        "\n",
        "        research_analysis['speaker_divergence'] = speaker_sentiment_analysis\n",
        "\n",
        "    # Question 2: Tone shift over time\n",
        "    if 'quarter' in df.columns:\n",
        "        temporal_analysis = {}\n",
        "\n",
        "        for model_col in ['finbert_tone_label', 'prosus_label']:\n",
        "            if model_col in df.columns:\n",
        "                model_name = 'finbert_tone' if 'finbert' in model_col else 'prosus'\n",
        "\n",
        "                # Calculate sentiment by quarter\n",
        "                quarter_sentiment = df.groupby('quarter')[model_col].value_counts(normalize=True).unstack(fill_value=0)\n",
        "\n",
        "                # Calculate temporal changes\n",
        "                if len(quarter_sentiment) >= 2:\n",
        "                    quarters = sorted(quarter_sentiment.index)\n",
        "                    q1_sentiment = quarter_sentiment.loc[quarters[0]]\n",
        "                    q2_sentiment = quarter_sentiment.loc[quarters[-1]]\n",
        "\n",
        "                    # Sentiment shift metrics\n",
        "                    sentiment_shift = q2_sentiment - q1_sentiment\n",
        "\n",
        "                    temporal_analysis[model_name] = {\n",
        "                        'q1_distribution': q1_sentiment.to_dict(),\n",
        "                        'q2_distribution': q2_sentiment.to_dict(),\n",
        "                        'sentiment_shift': sentiment_shift.to_dict(),\n",
        "                        'positive_change': sentiment_shift.get('positive', 0)\n",
        "                    }\n",
        "\n",
        "        research_analysis['temporal_shifts'] = temporal_analysis\n",
        "\n",
        "    # Question 3: Model consistency across contexts\n",
        "    model_consistency = {}\n",
        "\n",
        "    # Agreement rate by topic\n",
        "    if 'primary_topic' in df.columns:\n",
        "        topic_agreement = {}\n",
        "        for topic in df['primary_topic'].unique():\n",
        "            topic_mask = df['primary_topic'] == topic\n",
        "            topic_df = df[topic_mask]\n",
        "\n",
        "            if len(topic_df) > 5:\n",
        "                agreement_rate = (\n",
        "                    topic_df['finbert_tone_label'] == topic_df['prosus_label']\n",
        "                ).mean()\n",
        "                topic_agreement[topic] = agreement_rate\n",
        "\n",
        "        model_consistency['agreement_by_topic'] = topic_agreement\n",
        "\n",
        "    # Agreement rate by speaker\n",
        "    if 'speaker_role' in df.columns:\n",
        "        speaker_agreement = {}\n",
        "        for speaker in df['speaker_role'].unique():\n",
        "            speaker_mask = df['speaker_role'] == speaker\n",
        "            speaker_df = df[speaker_mask]\n",
        "\n",
        "            if len(speaker_df) > 5:\n",
        "                agreement_rate = (\n",
        "                    speaker_df['finbert_tone_label'] == speaker_df['prosus_label']\n",
        "                ).mean()\n",
        "                speaker_agreement[speaker] = agreement_rate\n",
        "\n",
        "        model_consistency['agreement_by_speaker'] = speaker_agreement\n",
        "\n",
        "    research_analysis['model_consistency'] = model_consistency\n",
        "\n",
        "    return research_analysis\n",
        "\n",
        "# Analyze research questions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESEARCH QUESTIONS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if sentiment_jpm_multi_2025_df is not None:\n",
        "    research_results = analyze_research_questions(sentiment_jpm_multi_2025_df)\n",
        "    comparison_results['research_questions'] = research_results\n",
        "\n",
        "    # Print key findings\n",
        "    if 'speaker_divergence' in research_results:\n",
        "        print(\"üë• Speaker Sentiment Divergence:\")\n",
        "        for model, analysis in research_results['speaker_divergence'].items():\n",
        "            pos_diff = analysis.get('positive_sentiment_difference', 0)\n",
        "            print(f\"  {model}: Analyst vs Executive positive sentiment difference = {pos_diff:.3f}\")\n",
        "\n",
        "    if 'temporal_shifts' in research_results:\n",
        "        print(\"\\nüìà Temporal Sentiment Shifts:\")\n",
        "        for model, analysis in research_results['temporal_shifts'].items():\n",
        "            pos_change = analysis.get('positive_change', 0)\n",
        "            print(f\"  {model}: Q1 to Q2 positive sentiment change = {pos_change:.3f}\")\n",
        "\n",
        "    if 'model_consistency' in research_results:\n",
        "        print(\"\\nü§ù Model Agreement by Context:\")\n",
        "        if 'agreement_by_topic' in research_results['model_consistency']:\n",
        "            topic_agreements = research_results['model_consistency']['agreement_by_topic']\n",
        "            for topic, agreement in sorted(topic_agreements.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "                print(f\"  {topic}: {agreement:.3f} agreement rate\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6M8ZxKMXCSQ",
        "outputId": "fbb2d237-63b2-4d90-fd8d-19dacdb95d3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RESEARCH QUESTIONS ANALYSIS\n",
            "============================================================\n",
            "Analyzing key research questions...\n",
            "üë• Speaker Sentiment Divergence:\n",
            "  finbert_tone: Analyst vs Executive positive sentiment difference = -0.025\n",
            "  prosus: Analyst vs Executive positive sentiment difference = 0.021\n",
            "\n",
            "üìà Temporal Sentiment Shifts:\n",
            "  finbert_tone: Q1 to Q2 positive sentiment change = 0.020\n",
            "  prosus: Q1 to Q2 positive sentiment change = -0.030\n",
            "\n",
            "ü§ù Model Agreement by Context:\n",
            "  capital: 0.825 agreement rate\n",
            "  general: 0.787 agreement rate\n",
            "  revenue: 0.786 agreement rate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Comparison Results\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def make_json_serializable(obj):\n",
        "    \"\"\"Recursively convert dict keys to JSON-safe strings, flattening tuples.\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        new_dict = {}\n",
        "        for k, v in obj.items():\n",
        "            if isinstance(k, tuple):\n",
        "                # Join tuple elements into a single string\n",
        "                key_str = \"_\".join(map(str, k))\n",
        "            else:\n",
        "                key_str = str(k)\n",
        "            new_dict[key_str] = make_json_serializable(v)\n",
        "        return new_dict\n",
        "    elif isinstance(obj, list):\n",
        "        return [make_json_serializable(i) for i in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "\n",
        "def save_comparison_results():\n",
        "    \"\"\"Save all comparison results to files.\"\"\"\n",
        "\n",
        "    # Convert results to JSON-safe format\n",
        "    safe_comparison_results = make_json_serializable(comparison_results)\n",
        "\n",
        "    # Save main comparison results\n",
        "    comparison_path = results_comparison_path / \"model_comparison_results.json\"\n",
        "    with open(comparison_path, \"w\") as f:\n",
        "        json.dump(safe_comparison_results, f, indent=2, default=str)\n",
        "    print(f\"‚úì Saved comparison results to: {comparison_path}\")\n",
        "\n",
        "    # Create summary report\n",
        "    summary_report = {\n",
        "        \"analysis_timestamp\": pd.Timestamp.now().isoformat(),\n",
        "        \"bank_code\": BANK_CODE,\n",
        "        \"models_compared\": [\"yiyanghkust/finbert-tone\", \"ProsusAI/finbert\"],\n",
        "        \"datasets_analyzed\": list(comparison_results.keys()),\n",
        "        \"key_metrics\": {}\n",
        "    }\n",
        "\n",
        "    # Extract key metrics for summary\n",
        "    for dataset_name, results in comparison_results.items():\n",
        "        if dataset_name == 'research_questions':\n",
        "            continue\n",
        "\n",
        "        if 'agreement_metrics' in results and results['agreement_metrics']:\n",
        "            summary_report[\"key_metrics\"][dataset_name] = {\n",
        "                \"agreement_rate\": results['agreement_metrics'].get('agreement_rate', 0),\n",
        "                \"cohen_kappa\": results['agreement_metrics'].get('cohen_kappa', 0),\n",
        "                \"total_predictions\": results['agreement_metrics'].get('total_predictions', 0)\n",
        "            }\n",
        "\n",
        "    # Save summary (also make safe)\n",
        "    summary_path = results_comparison_path / \"comparison_summary.json\"\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        json.dump(make_json_serializable(summary_report), f, indent=2, default=str)\n",
        "    print(f\"‚úì Saved summary report to: {summary_path}\")\n",
        "\n",
        "    return comparison_path, summary_path\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING COMPARISON RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_path, summary_path = save_comparison_results()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7kEMYAaXJgX",
        "outputId": "aac0abfd-98d0-4081-ab93-58144403eb9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING COMPARISON RESULTS\n",
            "============================================================\n",
            "‚úì Saved comparison results to: /content/drive/MyDrive/CAM_DS_AI_Project/results/comparison/jpm/model_comparison_results.json\n",
            "‚úì Saved summary report to: /content/drive/MyDrive/CAM_DS_AI_Project/results/comparison/jpm/comparison_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Final Model Comparison Summary\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"Results saved to: {results_comparison_path}\")\n",
        "\n",
        "# Overall model performance summary\n",
        "total_predictions = 0\n",
        "total_agreements = 0\n",
        "avg_kappa = 0\n",
        "kappa_count = 0\n",
        "\n",
        "for dataset_name, results in comparison_results.items():\n",
        "    if dataset_name == 'research_questions':\n",
        "        continue\n",
        "\n",
        "    if 'agreement_metrics' in results and results['agreement_metrics']:\n",
        "        metrics = results['agreement_metrics']\n",
        "        predictions = metrics.get('total_predictions', 0)\n",
        "        agreement_rate = metrics.get('agreement_rate', 0)\n",
        "        kappa = metrics.get('cohen_kappa', 0)\n",
        "\n",
        "        total_predictions += predictions\n",
        "        total_agreements += predictions * agreement_rate\n",
        "\n",
        "        if kappa != 0:\n",
        "            avg_kappa += kappa\n",
        "            kappa_count += 1\n",
        "\n",
        "if total_predictions > 0:\n",
        "    overall_agreement = total_agreements / total_predictions\n",
        "    print(f\"\\nüìä OVERALL MODEL PERFORMANCE:\")\n",
        "    print(f\"  Total predictions analyzed: {total_predictions:,}\")\n",
        "    print(f\"  Overall agreement rate: {overall_agreement:.3f}\")\n",
        "\n",
        "    if kappa_count > 0:\n",
        "        avg_kappa = avg_kappa / kappa_count\n",
        "        print(f\"  Average Cohen's Kappa: {avg_kappa:.3f}\")\n",
        "\n",
        "# Interpretation guide\n",
        "print(f\"\\nüìã INTERPRETATION GUIDE:\")\n",
        "print(f\"  Agreement Rate: {overall_agreement:.1%} of predictions match between models\")\n",
        "print(f\"  Cohen's Kappa: {avg_kappa:.3f} \" +\n",
        "      (\"(Strong agreement)\" if avg_kappa > 0.8 else\n",
        "       \"(Moderate agreement)\" if avg_kappa > 0.6 else\n",
        "       \"(Fair agreement)\" if avg_kappa > 0.4 else \"(Poor agreement)\"))\n",
        "\n",
        "print(f\"\\nNext step: Run 06_results_visualization.ipynb to create visualizations and final report\")\n",
        "\n",
        "# Export key metrics for visualization\n",
        "key_metrics_for_viz = {\n",
        "    'overall_agreement_rate': overall_agreement if total_predictions > 0 else 0,\n",
        "    'overall_kappa': avg_kappa if kappa_count > 0 else 0,\n",
        "    'total_predictions': total_predictions,\n",
        "    'datasets_analyzed': [name for name in comparison_results.keys() if name != 'research_questions']\n",
        "}\n",
        "\n",
        "viz_metrics_path = results_comparison_path / \"key_metrics_for_visualization.json\"\n",
        "with open(viz_metrics_path, \"w\") as f:\n",
        "    json.dump(key_metrics_for_viz, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Key metrics for visualization saved to: {viz_metrics_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMWJEAZYXOov",
        "outputId": "a1e28554-1fe4-4555-803e-d8b48770eec5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL COMPARISON ANALYSIS COMPLETE\n",
            "============================================================\n",
            "Results saved to: /content/drive/MyDrive/CAM_DS_AI_Project/results/comparison/jpm\n",
            "\n",
            "üìä OVERALL MODEL PERFORMANCE:\n",
            "  Total predictions analyzed: 2,220\n",
            "  Overall agreement rate: 0.761\n",
            "  Average Cohen's Kappa: 0.431\n",
            "\n",
            "üìã INTERPRETATION GUIDE:\n",
            "  Agreement Rate: 76.1% of predictions match between models\n",
            "  Cohen's Kappa: 0.431 (Fair agreement)\n",
            "\n",
            "Next step: Run 06_results_visualization.ipynb to create visualizations and final report\n",
            "‚úì Key metrics for visualization saved to: /content/drive/MyDrive/CAM_DS_AI_Project/results/comparison/jpm/key_metrics_for_visualization.json\n"
          ]
        }
      ]
    }
  ]
}