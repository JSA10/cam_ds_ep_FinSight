{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 03_clean_preprocess_enhanced.ipynb\n",
        "# Purpose: Enhanced cleaning and preprocessing for both JP Morgan and HSBC data\n",
        "# Banks: JP Morgan (JPM) and HSBC\n",
        "# Quarters: Q1 2025, Q2 2025\n",
        "# Models: 4 sentiment analysis models\n",
        "# Input: Raw datasets from both banks\n",
        "# Output: Clean and processed datasets ready for 4-model sentiment analysis\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced text processing\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import unicodedata\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "except:\n",
        "    print(\"NLTK downloads failed - continuing without\")\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load enhanced configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/configs/enhanced_config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    enhanced_config = json.load(f)\n",
        "\n",
        "# Load data registry\n",
        "registry_path = Path(enhanced_config[\"drive_base\"]) / \"configs\" / \"enhanced_data_registry.json\"\n",
        "with open(registry_path, \"r\") as f:\n",
        "    data_registry = json.load(f)\n",
        "\n",
        "SEED = enhanced_config[\"SEED\"]\n",
        "BANKS = enhanced_config[\"BANKS\"]\n",
        "QUARTERS = enhanced_config[\"QUARTERS\"]\n",
        "MODELS = enhanced_config[\"MODELS\"]\n",
        "drive_base = Path(enhanced_config[\"drive_base\"])\n",
        "colab_base = Path(enhanced_config[\"colab_base\"])\n",
        "\n",
        "print(f\"Enhanced cleaning and preprocessing for banks: {', '.join([bank.upper() for bank in BANKS])}\")\n",
        "print(f\"Target models: {len(MODELS)} ({', '.join(MODELS.keys())})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIyfsBE8APBp",
        "outputId": "6810f747-7479-4cf2-9f1c-6fec356b6737"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Enhanced cleaning and preprocessing for banks: JPM, HSBC\n",
            "Target models: 4 (finbert_yiyanghkust, finbert_prosusai, distilroberta, cardiffnlp_roberta)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Data Loading\n",
        "\n",
        "def load_raw_datasets_enhanced():\n",
        "    \"\"\"Load all raw datasets for processing.\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"LOADING RAW DATASETS\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    raw_datasets = {}\n",
        "\n",
        "    for bank in BANKS:\n",
        "        print(f\"\\nüìÇ Loading {bank.upper()} datasets...\")\n",
        "        raw_datasets[bank] = {}\n",
        "\n",
        "        # Load quarterly datasets\n",
        "        for quarter in QUARTERS:\n",
        "            filename = f\"raw_{bank}_{quarter}_earnings_call.csv\"\n",
        "            file_path = drive_base / f\"data/raw/{bank}\" / filename\n",
        "\n",
        "            if file_path.exists():\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    raw_datasets[bank][quarter] = df\n",
        "                    print(f\"  ‚úÖ {quarter}: {df.shape}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå {quarter}: Error loading - {str(e)}\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå {quarter}: File not found - {file_path}\")\n",
        "\n",
        "        # Load combined dataset\n",
        "        combined_filename = f\"raw_{bank}_multi_2025_earnings_call.csv\"\n",
        "        combined_path = drive_base / f\"data/raw/{bank}\" / combined_filename\n",
        "\n",
        "        if combined_path.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(combined_path)\n",
        "                raw_datasets[bank][\"combined\"] = df\n",
        "                print(f\"  ‚úÖ Combined: {df.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Combined: Error loading - {str(e)}\")\n",
        "\n",
        "        # Load manual labels if available\n",
        "        manual_filename = f\"raw_{bank}_manual_labels.csv\"\n",
        "        manual_path = drive_base / f\"data/raw/{bank}\" / manual_filename\n",
        "\n",
        "        if manual_path.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(manual_path)\n",
        "                raw_datasets[bank][\"manual_labels\"] = df\n",
        "                print(f\"  ‚úÖ Manual labels: {df.shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Manual labels: Error loading - {str(e)}\")\n",
        "\n",
        "    return raw_datasets\n",
        "\n",
        "# Load all raw datasets\n",
        "raw_datasets = load_raw_datasets_enhanced()\n"
      ],
      "metadata": {
        "id": "W9_ctTiCATm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc398a5d-cb01-41be-ab8b-2df45a6430a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "LOADING RAW DATASETS\n",
            "==================================================\n",
            "\n",
            "üìÇ Loading JPM datasets...\n",
            "  ‚úÖ q1_2025: (112, 10)\n",
            "  ‚úÖ q2_2025: (149, 10)\n",
            "  ‚úÖ Combined: (261, 11)\n",
            "  ‚úÖ Manual labels: (1121, 27)\n",
            "\n",
            "üìÇ Loading HSBC datasets...\n",
            "  ‚úÖ q1_2025: (49, 11)\n",
            "  ‚úÖ q2_2025: (30, 11)\n",
            "  ‚úÖ Combined: (79, 12)\n",
            "  ‚úÖ Manual labels: (858, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Cleaning Functions\n",
        "\n",
        "def standardize_column_names_enhanced(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced column name standardization with financial domain mapping.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert to snake_case\n",
        "    new_columns = {}\n",
        "    for col in df.columns:\n",
        "        # Remove special characters and convert to lowercase\n",
        "        clean_col = re.sub(r'[^\\w\\s]', '', str(col).lower())\n",
        "        # Replace spaces with underscores\n",
        "        clean_col = re.sub(r'\\s+', '_', clean_col.strip())\n",
        "        # Remove multiple underscores\n",
        "        clean_col = re.sub(r'_+', '_', clean_col)\n",
        "        new_columns[col] = clean_col\n",
        "\n",
        "    df = df.rename(columns=new_columns)\n",
        "\n",
        "    # Enhanced financial domain mapping\n",
        "    enhanced_column_mapping = {\n",
        "        # Text content mappings\n",
        "        'content': 'text',\n",
        "        'transcript': 'text',\n",
        "        'question': 'text',\n",
        "        'answer': 'text',\n",
        "        'response': 'text',\n",
        "        'statement': 'text',\n",
        "\n",
        "        # Speaker mappings\n",
        "        'speaker_name': 'speaker',\n",
        "        'participant': 'speaker',\n",
        "        'person': 'speaker',\n",
        "        'individual': 'speaker',\n",
        "\n",
        "        # Role mappings\n",
        "        'role': 'speaker_role',\n",
        "        'position': 'speaker_role',\n",
        "        'title': 'speaker_role',\n",
        "        'function': 'speaker_role',\n",
        "\n",
        "        # Timing mappings\n",
        "        'time': 'timestamp',\n",
        "        'datetime': 'timestamp',\n",
        "        'sequence': 'order_id',\n",
        "        'order': 'order_id',\n",
        "\n",
        "        # Classification mappings\n",
        "        'type': 'qa_type',\n",
        "        'category': 'topic',\n",
        "        'subject': 'topic',\n",
        "        'theme': 'topic'\n",
        "    }\n",
        "\n",
        "    # Apply enhanced mapping\n",
        "    mapping_applied = []\n",
        "    for old_col, new_col in enhanced_column_mapping.items():\n",
        "        if old_col in df.columns and new_col not in df.columns:\n",
        "            df = df.rename(columns={old_col: new_col})\n",
        "            mapping_applied.append(f\"{old_col} ‚Üí {new_col}\")\n",
        "\n",
        "    if mapping_applied:\n",
        "        print(f\"  Column mappings applied: {', '.join(mapping_applied)}\")\n",
        "\n",
        "    print(f\"  Standardized columns: {list(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "def clean_speaker_names_enhanced(df: pd.DataFrame, speaker_col: str = 'speaker') -> pd.DataFrame:\n",
        "    \"\"\"Enhanced speaker name cleaning with financial role detection.\"\"\"\n",
        "    if speaker_col not in df.columns:\n",
        "        print(f\"  Warning: Speaker column '{speaker_col}' not found\")\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Enhanced financial speaker role mapping\n",
        "    enhanced_speaker_mapping = {\n",
        "        # Executive roles\n",
        "        r'.*\\b(ceo|chief executive)\\b.*': 'ceo',\n",
        "        r'.*\\b(cfo|chief financial)\\b.*': 'cfo',\n",
        "        r'.*\\b(coo|chief operating)\\b.*': 'coo',\n",
        "        r'.*\\b(cro|chief risk)\\b.*': 'cro',\n",
        "        r'.*\\b(president)\\b.*': 'president',\n",
        "        r'.*\\b(chairman|chair)\\b.*': 'chairman',\n",
        "        r'.*\\b(director)\\b.*': 'director',\n",
        "        r'.*\\b(head of|head)\\b.*': 'head',\n",
        "\n",
        "        # Analyst roles\n",
        "        r'.*\\b(analyst|research)\\b.*': 'analyst',\n",
        "        r'.*\\b(equity research)\\b.*': 'analyst',\n",
        "        r'.*\\b(investment banking)\\b.*': 'analyst',\n",
        "        r'.*\\b(portfolio manager)\\b.*': 'analyst',\n",
        "\n",
        "        # Operational roles\n",
        "        r'.*\\b(operator|moderator)\\b.*': 'operator',\n",
        "        r'.*\\b(facilitator|host)\\b.*': 'operator',\n",
        "        r'.*\\b(coordinator)\\b.*': 'operator',\n",
        "\n",
        "        # Generic roles\n",
        "        r'.*\\b(management|executive)\\b.*': 'executive',\n",
        "        r'.*\\b(representative|rep)\\b.*': 'representative',\n",
        "        r'.*\\b(investor relations|ir)\\b.*': 'investor_relations'\n",
        "    }\n",
        "\n",
        "    # Clean speaker names\n",
        "    df[speaker_col] = df[speaker_col].astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Apply enhanced role detection\n",
        "    df['speaker_role'] = 'other'  # Default\n",
        "\n",
        "    for pattern, role in enhanced_speaker_mapping.items():\n",
        "        mask = df[speaker_col].str.contains(pattern, regex=True, na=False)\n",
        "        df.loc[mask, 'speaker_role'] = role\n",
        "\n",
        "    # Also check original_role column if it exists\n",
        "    if 'original_role' in df.columns:\n",
        "        df['original_role'] = df['original_role'].astype(str).str.lower().str.strip()\n",
        "        for pattern, role in enhanced_speaker_mapping.items():\n",
        "            mask = df['original_role'].str.contains(pattern, regex=True, na=False)\n",
        "            df.loc[mask, 'speaker_role'] = role\n",
        "\n",
        "    # Role distribution\n",
        "    role_counts = df['speaker_role'].value_counts()\n",
        "    print(f\"  Speaker roles detected:\")\n",
        "    for role, count in role_counts.items():\n",
        "        print(f\"    {role}: {count}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_text_content_enhanced(df: pd.DataFrame, text_col: str = 'text') -> pd.DataFrame:\n",
        "    \"\"\"Enhanced text cleaning optimized for financial content and 4 sentiment models.\"\"\"\n",
        "    if text_col not in df.columns:\n",
        "        print(f\"  Warning: Text column '{text_col}' not found\")\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    print(f\"  Cleaning text in column: {text_col}\")\n",
        "\n",
        "    # Convert to string and handle missing values\n",
        "    df[text_col] = df[text_col].astype(str).fillna('')\n",
        "\n",
        "    # Enhanced financial text cleaning\n",
        "    def clean_financial_text(text):\n",
        "        \"\"\"Enhanced financial text cleaning function.\"\"\"\n",
        "        if pd.isna(text) or text == '' or text == 'nan':\n",
        "            return ''\n",
        "\n",
        "        text = str(text)\n",
        "\n",
        "        # Remove common artifacts from earnings calls\n",
        "        text = re.sub(r'\\[.*?\\]', '', text)  # [brackets]\n",
        "        text = re.sub(r'\\(.*?\\)', '', text)  # (parentheses)\n",
        "        text = re.sub(r'--+', ' ', text)     # Multiple dashes\n",
        "        text = re.sub(r'\\*+', '', text)      # Asterisks\n",
        "        text = re.sub(r'#+', '', text)       # Hash symbols\n",
        "\n",
        "        # Clean financial notation while preserving meaning\n",
        "        text = re.sub(r'\\$\\s*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(billion|million|thousand|bn|mn|k)\\b',\n",
        "                     r'$\\1 \\2', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        # Preserve financial terminology\n",
        "        # Remove very short or very long texts that might be artifacts\n",
        "        if len(text) < 10 or len(text) > 10000:\n",
        "            return ''\n",
        "\n",
        "        return text\n",
        "\n",
        "    # Apply enhanced cleaning\n",
        "    df[f'{text_col}_clean'] = df[text_col].apply(clean_financial_text)\n",
        "\n",
        "    # Calculate enhanced text statistics\n",
        "    df[f'{text_col}_length'] = df[f'{text_col}_clean'].str.len()\n",
        "    df[f'{text_col}_word_count'] = df[f'{text_col}_clean'].str.split().str.len()\n",
        "    df[f'{text_col}_sentence_count'] = df[f'{text_col}_clean'].str.count(r'[.!?]+') + 1\n",
        "\n",
        "    # Enhanced readability metrics for model analysis\n",
        "    try:\n",
        "        import textstat\n",
        "        df[f'{text_col}_readability'] = df[f'{text_col}_clean'].apply(\n",
        "            lambda x: textstat.flesch_reading_ease(x) if x and len(x) > 0 else 0\n",
        "        )\n",
        "    except ImportError:\n",
        "        df[f'{text_col}_readability'] = 50  # Default moderate readability\n",
        "\n",
        "    # Remove very short texts\n",
        "    min_length = 15\n",
        "    short_text_mask = df[f'{text_col}_length'] < min_length\n",
        "    df.loc[short_text_mask, f'{text_col}_clean'] = ''\n",
        "\n",
        "    # Calculate cleaning effectiveness\n",
        "    original_lengths = df[text_col].str.len()\n",
        "    clean_lengths = df[f'{text_col}_length'].fillna(0)\n",
        "    avg_reduction = (original_lengths - clean_lengths).mean()\n",
        "\n",
        "    valid_text_count = (df[f'{text_col}_clean'] != '').sum()\n",
        "\n",
        "    print(f\"    Average length reduction: {avg_reduction:.1f} characters\")\n",
        "    print(f\"    Valid texts after cleaning: {valid_text_count}/{len(df)} ({valid_text_count/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Sample cleaned text\n",
        "    valid_texts = df[df[f'{text_col}_clean'] != ''][f'{text_col}_clean']\n",
        "    if len(valid_texts) > 0:\n",
        "        sample_text = valid_texts.iloc[0]\n",
        "        print(f\"    Sample: '{sample_text[:150]}...'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_enhanced_metadata(df: pd.DataFrame, bank_code: str) -> pd.DataFrame:\n",
        "    \"\"\"Add enhanced metadata optimized for 4-model sentiment analysis.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Basic metadata\n",
        "    df['qa_id'] = df.index\n",
        "    df['bank_code'] = bank_code\n",
        "    df['processing_timestamp'] = pd.Timestamp.now()\n",
        "\n",
        "    # Enhanced validation flags for different models\n",
        "    df['is_valid_for_analysis'] = True\n",
        "\n",
        "    # Model-specific readiness flags\n",
        "    for model_key in MODELS.keys():\n",
        "        df[f'ready_for_{model_key}'] = True\n",
        "\n",
        "    # Text validation\n",
        "    if 'text_clean' in df.columns:\n",
        "        # Basic text validation\n",
        "        invalid_text_mask = (\n",
        "            (df['text_clean'] == '') |\n",
        "            (df['text_clean'].isna()) |\n",
        "            (df['text_length'] < 15) |\n",
        "            (df['text_word_count'] < 3)\n",
        "        )\n",
        "\n",
        "        df.loc[invalid_text_mask, 'is_valid_for_analysis'] = False\n",
        "\n",
        "        # Model-specific validation\n",
        "        # FinBERT models prefer longer, more structured text\n",
        "        finbert_invalid = (df['text_length'] < 20) | (df['text_word_count'] < 4)\n",
        "        df.loc[finbert_invalid, 'ready_for_finbert_yiyanghkust'] = False\n",
        "        df.loc[finbert_invalid, 'ready_for_finbert_prosusai'] = False\n",
        "\n",
        "        # RoBERTa models are more flexible\n",
        "        roberta_invalid = (df['text_length'] < 10) | (df['text_word_count'] < 2)\n",
        "        df.loc[roberta_invalid, 'ready_for_distilroberta'] = False\n",
        "        df.loc[roberta_invalid, 'ready_for_cardiffnlp_roberta'] = False\n",
        "\n",
        "    # Speaker role validation\n",
        "    if 'speaker_role' in df.columns:\n",
        "        # Flag records with unidentified speakers for special handling\n",
        "        df['speaker_identified'] = df['speaker_role'] != 'other'\n",
        "\n",
        "    # Enhanced financial context flags\n",
        "    if 'text_clean' in df.columns:\n",
        "        # Financial sentiment indicators\n",
        "        financial_keywords = {\n",
        "            'earnings_context': r'\\b(earnings|revenue|profit|loss|income|ebitda)\\b',\n",
        "            'risk_context': r'\\b(risk|exposure|provision|default|credit)\\b',\n",
        "            'growth_context': r'\\b(growth|expansion|increase|improvement|strong)\\b',\n",
        "            'performance_context': r'\\b(performance|results|metrics|kpi|target)\\b'\n",
        "        }\n",
        "\n",
        "        for context_type, pattern in financial_keywords.items():\n",
        "            df[f'has_{context_type}'] = df['text_clean'].str.contains(\n",
        "                pattern, case=False, regex=True, na=False\n",
        "            )\n",
        "\n",
        "    # Summary statistics\n",
        "    total_records = len(df)\n",
        "    valid_records = df['is_valid_for_analysis'].sum()\n",
        "\n",
        "    print(f\"  Enhanced metadata added:\")\n",
        "    print(f\"    Total records: {total_records}\")\n",
        "    print(f\"    Valid for analysis: {valid_records} ({valid_records/total_records*100:.1f}%)\")\n",
        "\n",
        "    # Model readiness summary\n",
        "    for model_key in MODELS.keys():\n",
        "        ready_count = df[f'ready_for_{model_key}'].sum()\n",
        "        print(f\"    Ready for {model_key}: {ready_count} ({ready_count/total_records*100:.1f}%)\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "EGBs9P7AAcBS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Dataset Processing\n",
        "\n",
        "def process_dataset_enhanced(df: pd.DataFrame, dataset_name: str, bank_code: str) -> pd.DataFrame:\n",
        "    \"\"\"Apply all enhanced cleaning steps to a dataset.\"\"\"\n",
        "    if df is None:\n",
        "        print(f\"‚ùå Cannot process {dataset_name} for {bank_code.upper()} - dataset is None\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüßπ [{bank_code.upper()}] PROCESSING {dataset_name.upper()}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Input shape: {df.shape}\")\n",
        "    print(f\"Input columns: {list(df.columns)}\")\n",
        "\n",
        "    # Step 1: Standardize column names\n",
        "    df_clean = standardize_column_names_enhanced(df)\n",
        "\n",
        "    # Step 2: Clean speaker names and detect roles\n",
        "    df_clean = clean_speaker_names_enhanced(df_clean)\n",
        "\n",
        "    # Step 3: Enhanced text cleaning\n",
        "    df_clean = clean_text_content_enhanced(df_clean)\n",
        "\n",
        "    # Step 4: Remove duplicates with enhanced detection\n",
        "    original_count = len(df_clean)\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "\n",
        "    # Enhanced duplicate detection on cleaned text\n",
        "    if 'text_clean' in df_clean.columns:\n",
        "        df_clean = df_clean.drop_duplicates(subset=['text_clean'])\n",
        "\n",
        "    removed_count = original_count - len(df_clean)\n",
        "    print(f\"  Removed {removed_count} duplicates ({removed_count/original_count*100:.1f}%)\")\n",
        "\n",
        "    # Step 5: Add enhanced metadata\n",
        "    df_clean = add_enhanced_metadata(df_clean, bank_code)\n",
        "\n",
        "    # Step 6: Filter for valid records but keep some invalid for debugging\n",
        "    valid_df = df_clean[df_clean['is_valid_for_analysis']].copy()\n",
        "    invalid_count = len(df_clean) - len(valid_df)\n",
        "\n",
        "    print(f\"Final shape: {valid_df.shape} (removed {invalid_count} invalid entries)\")\n",
        "\n",
        "    return valid_df\n",
        "\n",
        "# Process all datasets for both banks\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ENHANCED DATASET PROCESSING\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "processed_datasets = {}\n",
        "\n",
        "for bank in BANKS:\n",
        "    processed_datasets[bank] = {}\n",
        "\n",
        "    if bank in raw_datasets:\n",
        "        print(f\"\\nüìä Processing {bank.upper()} datasets...\")\n",
        "\n",
        "        # Process quarterly datasets\n",
        "        for quarter in QUARTERS:\n",
        "            if quarter in raw_datasets[bank]:\n",
        "                processed_datasets[bank][quarter] = process_dataset_enhanced(\n",
        "                    raw_datasets[bank][quarter], quarter, bank\n",
        "                )\n",
        "\n",
        "        # Process combined dataset\n",
        "        if \"combined\" in raw_datasets[bank]:\n",
        "            processed_datasets[bank][\"combined\"] = process_dataset_enhanced(\n",
        "                raw_datasets[bank][\"combined\"], \"combined\", bank\n",
        "            )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESVCw31IAhMK",
        "outputId": "a6317013-6d9f-4cd8-afea-008a189e58da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED DATASET PROCESSING\n",
            "============================================================\n",
            "\n",
            "üìä Processing JPM datasets...\n",
            "\n",
            "üßπ [JPM] PROCESSING Q1_2025\n",
            "--------------------------------------------------\n",
            "Input shape: (112, 10)\n",
            "Input columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry']\n",
            "  Column mappings applied: content ‚Üí text, speaker_name ‚Üí speaker, role ‚Üí speaker_role\n",
            "  Standardized columns: ['section', 'question_number', 'answer_number', 'speaker', 'speaker_role', 'company', 'text', 'year', 'quarter', 'is_pleasantry']\n",
            "  Speaker roles detected:\n",
            "    other: 112\n",
            "  Cleaning text in column: text\n",
            "    Average length reduction: 1.1 characters\n",
            "    Valid texts after cleaning: 107/112 (95.5%)\n",
            "    Sample: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 billion, EPS of $5.07 on revenue of $46 billion, with ...'\n",
            "  Removed 5 duplicates (4.5%)\n",
            "  Enhanced metadata added:\n",
            "    Total records: 107\n",
            "    Valid for analysis: 106 (99.1%)\n",
            "    Ready for finbert_yiyanghkust: 103 (96.3%)\n",
            "    Ready for finbert_prosusai: 103 (96.3%)\n",
            "    Ready for distilroberta: 106 (99.1%)\n",
            "    Ready for cardiffnlp_roberta: 106 (99.1%)\n",
            "Final shape: (106, 28) (removed 1 invalid entries)\n",
            "\n",
            "üßπ [JPM] PROCESSING Q2_2025\n",
            "--------------------------------------------------\n",
            "Input shape: (149, 10)\n",
            "Input columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry']\n",
            "  Column mappings applied: content ‚Üí text, speaker_name ‚Üí speaker, role ‚Üí speaker_role\n",
            "  Standardized columns: ['section', 'question_number', 'answer_number', 'speaker', 'speaker_role', 'company', 'text', 'year', 'quarter', 'is_pleasantry']\n",
            "  Speaker roles detected:\n",
            "    other: 147\n",
            "    cfo: 2\n",
            "  Cleaning text in column: text\n",
            "    Average length reduction: 0.8 characters\n",
            "    Valid texts after cleaning: 140/149 (94.0%)\n",
            "    Sample: 'names, partially offset by the scenario probability adjustment I mentioned upfront. Turning to Asset & Wealth Management to complete our lines of busi...'\n",
            "  Removed 8 duplicates (5.4%)\n",
            "  Enhanced metadata added:\n",
            "    Total records: 141\n",
            "    Valid for analysis: 140 (99.3%)\n",
            "    Ready for finbert_yiyanghkust: 134 (95.0%)\n",
            "    Ready for finbert_prosusai: 134 (95.0%)\n",
            "    Ready for distilroberta: 141 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 141 (100.0%)\n",
            "Final shape: (140, 28) (removed 1 invalid entries)\n",
            "\n",
            "üßπ [JPM] PROCESSING COMBINED\n",
            "--------------------------------------------------\n",
            "Input shape: (261, 11)\n",
            "Input columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry', 'bank_code']\n",
            "  Column mappings applied: content ‚Üí text, speaker_name ‚Üí speaker, role ‚Üí speaker_role\n",
            "  Standardized columns: ['section', 'question_number', 'answer_number', 'speaker', 'speaker_role', 'company', 'text', 'year', 'quarter', 'is_pleasantry', 'bank_code']\n",
            "  Speaker roles detected:\n",
            "    other: 259\n",
            "    cfo: 2\n",
            "  Cleaning text in column: text\n",
            "    Average length reduction: 0.9 characters\n",
            "    Valid texts after cleaning: 247/261 (94.6%)\n",
            "    Sample: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 billion, EPS of $5.07 on revenue of $46 billion, with ...'\n",
            "  Removed 15 duplicates (5.7%)\n",
            "  Enhanced metadata added:\n",
            "    Total records: 246\n",
            "    Valid for analysis: 245 (99.6%)\n",
            "    Ready for finbert_yiyanghkust: 237 (96.3%)\n",
            "    Ready for finbert_prosusai: 237 (96.3%)\n",
            "    Ready for distilroberta: 245 (99.6%)\n",
            "    Ready for cardiffnlp_roberta: 245 (99.6%)\n",
            "Final shape: (245, 28) (removed 1 invalid entries)\n",
            "\n",
            "üìä Processing HSBC datasets...\n",
            "\n",
            "üßπ [HSBC] PROCESSING Q1_2025\n",
            "--------------------------------------------------\n",
            "Input shape: (49, 11)\n",
            "Input columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry', 'source_pdf']\n",
            "  Column mappings applied: content ‚Üí text, speaker_name ‚Üí speaker, role ‚Üí speaker_role\n",
            "  Standardized columns: ['section', 'question_number', 'answer_number', 'speaker', 'speaker_role', 'company', 'text', 'year', 'quarter', 'is_pleasantry', 'source_pdf']\n",
            "  Speaker roles detected:\n",
            "    other: 49\n",
            "  Cleaning text in column: text\n",
            "    Average length reduction: 7.8 characters\n",
            "    Valid texts after cleaning: 49/49 (100.0%)\n",
            "    Sample: 'Welcome, all, to today ‚Äôs call . I‚Äôm joined here in London by Pam . Before Pam takes you through the numbers, I would like to begin with some opening ...'\n",
            "  Removed 0 duplicates (0.0%)\n",
            "  Enhanced metadata added:\n",
            "    Total records: 49\n",
            "    Valid for analysis: 49 (100.0%)\n",
            "    Ready for finbert_yiyanghkust: 49 (100.0%)\n",
            "    Ready for finbert_prosusai: 49 (100.0%)\n",
            "    Ready for distilroberta: 49 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 49 (100.0%)\n",
            "Final shape: (49, 29) (removed 0 invalid entries)\n",
            "\n",
            "üßπ [HSBC] PROCESSING Q2_2025\n",
            "--------------------------------------------------\n",
            "Input shape: (30, 11)\n",
            "Input columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry', 'source_pdf']\n",
            "  Column mappings applied: content ‚Üí text, speaker_name ‚Üí speaker, role ‚Üí speaker_role\n",
            "  Standardized columns: ['section', 'question_number', 'answer_number', 'speaker', 'speaker_role', 'company', 'text', 'year', 'quarter', 'is_pleasantry', 'source_pdf']\n",
            "  Speaker roles detected:\n",
            "    other: 30\n",
            "  Cleaning text in column: text\n",
            "    Average length reduction: 0.0 characters\n",
            "    Valid texts after cleaning: 30/30 (100.0%)\n",
            "    Sample: 'Welcome, all, to today‚Äôs call. I‚Äôm joined by Pam. Before Pam takes you through the second quarter numbers, I will cover three items: our first half pe...'\n",
            "  Removed 0 duplicates (0.0%)\n",
            "  Enhanced metadata added:\n",
            "    Total records: 30\n",
            "    Valid for analysis: 30 (100.0%)\n",
            "    Ready for finbert_yiyanghkust: 30 (100.0%)\n",
            "    Ready for finbert_prosusai: 30 (100.0%)\n",
            "    Ready for distilroberta: 30 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 30 (100.0%)\n",
            "Final shape: (30, 29) (removed 0 invalid entries)\n",
            "\n",
            "üßπ [HSBC] PROCESSING COMBINED\n",
            "--------------------------------------------------\n",
            "Input shape: (79, 12)\n",
            "Input columns: ['section', 'question_number', 'answer_number', 'speaker_name', 'role', 'company', 'content', 'year', 'quarter', 'is_pleasantry', 'source_pdf', 'bank_code']\n",
            "  Column mappings applied: content ‚Üí text, speaker_name ‚Üí speaker, role ‚Üí speaker_role\n",
            "  Standardized columns: ['section', 'question_number', 'answer_number', 'speaker', 'speaker_role', 'company', 'text', 'year', 'quarter', 'is_pleasantry', 'source_pdf', 'bank_code']\n",
            "  Speaker roles detected:\n",
            "    other: 79\n",
            "  Cleaning text in column: text\n",
            "    Average length reduction: 4.8 characters\n",
            "    Valid texts after cleaning: 79/79 (100.0%)\n",
            "    Sample: 'Welcome, all, to today ‚Äôs call . I‚Äôm joined here in London by Pam . Before Pam takes you through the numbers, I would like to begin with some opening ...'\n",
            "  Removed 0 duplicates (0.0%)\n",
            "  Enhanced metadata added:\n",
            "    Total records: 79\n",
            "    Valid for analysis: 79 (100.0%)\n",
            "    Ready for finbert_yiyanghkust: 79 (100.0%)\n",
            "    Ready for finbert_prosusai: 79 (100.0%)\n",
            "    Ready for distilroberta: 79 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 79 (100.0%)\n",
            "Final shape: (79, 29) (removed 0 invalid entries)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Sentence-level Preprocessing for 4 Models\n",
        "\n",
        "def preprocess_for_sentiment_models_enhanced(df: pd.DataFrame, bank_code: str) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced preprocessing specifically optimized for 4 sentiment models.\"\"\"\n",
        "    if df is None:\n",
        "        print(f\"‚ùå Cannot preprocess for {bank_code.upper()} - dataset is None\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüî¨ [{bank_code.upper()}] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Input shape: {df.shape}\")\n",
        "\n",
        "    if 'text_clean' not in df.columns:\n",
        "        print(\"‚ùå 'text_clean' column not found\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    sentences = []\n",
        "    processed_count = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        text_content = row.get('text_clean', '')\n",
        "\n",
        "        # Skip invalid text\n",
        "        if pd.isna(text_content) or str(text_content).strip() == '' or str(text_content) == 'nan':\n",
        "            continue\n",
        "\n",
        "        text_content = str(text_content).strip()\n",
        "\n",
        "        if len(text_content) < 15:  # Minimum length for sentiment analysis\n",
        "            continue\n",
        "\n",
        "        # Enhanced sentence splitting for financial text\n",
        "        # Split on multiple sentence terminators and financial patterns\n",
        "        sentence_patterns = [\n",
        "            r'[.!?]+\\s+',  # Standard sentence endings\n",
        "            r'[.!?]+$'\n",
        "    ,    # End of text\n",
        "            r';\\s+',       # Semicolon separations\n",
        "            r'\\.\\s+[A-Z]', # Period followed by capital letter\n",
        "        ]\n",
        "\n",
        "        # Use the first pattern that produces meaningful splits\n",
        "        text_sentences = [text_content]  # Default to whole text\n",
        "\n",
        "        for pattern in sentence_patterns:\n",
        "            splits = re.split(pattern, text_content)\n",
        "            if len(splits) > 1 and all(len(s.strip()) > 10 for s in splits if s.strip()):\n",
        "                text_sentences = splits\n",
        "                break\n",
        "\n",
        "        sentence_count = 0\n",
        "        for sent_idx, sentence in enumerate(text_sentences):\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            # Enhanced sentence validation for different models\n",
        "            if len(sentence) < 15:  # Too short\n",
        "                continue\n",
        "\n",
        "            if len(sentence) > 5000:  # Too long - truncate\n",
        "                sentence = sentence[:5000]\n",
        "\n",
        "            # Model-specific preprocessing flags\n",
        "            model_ready = {}\n",
        "\n",
        "            # FinBERT models prefer structured financial language\n",
        "            finbert_ready = (\n",
        "                len(sentence) >= 20 and\n",
        "                len(sentence.split()) >= 4 and\n",
        "                any(word in sentence.lower() for word in [\n",
        "                    'quarter', 'revenue', 'profit', 'growth', 'performance',\n",
        "                    'market', 'business', 'financial', 'earnings', 'results'\n",
        "                ])\n",
        "            )\n",
        "            model_ready['finbert_yiyanghkust'] = finbert_ready\n",
        "            model_ready['finbert_prosusai'] = finbert_ready\n",
        "\n",
        "            # RoBERTa models are more flexible\n",
        "            roberta_ready = len(sentence) >= 10 and len(sentence.split()) >= 2\n",
        "            model_ready['distilroberta'] = roberta_ready\n",
        "            model_ready['cardiffnlp_roberta'] = roberta_ready\n",
        "\n",
        "            # Enhanced financial context detection\n",
        "            financial_indicators = {\n",
        "                'has_financial_numbers': bool(re.search(r'\\$\\d+|\\d+%|\\d+\\.\\d+', sentence)),\n",
        "                'has_temporal_ref': bool(re.search(r'\\b(quarter|q[1-4]|year|month|2025)\\b', sentence, re.I)),\n",
        "                'has_performance_terms': bool(re.search(r'\\b(increase|decrease|growth|decline|strong|weak)\\b', sentence, re.I)),\n",
        "                'has_financial_terms': bool(re.search(r'\\b(revenue|profit|earnings|ebitda|margin|roi)\\b', sentence, re.I))\n",
        "            }\n",
        "\n",
        "            sentence_record = {\n",
        "                'original_qa_id': row.get('qa_id', idx),\n",
        "                'sentence_id': f\"{bank_code}_{row.get('qa_id', idx)}_{sent_idx}\",\n",
        "                'text': sentence,\n",
        "                'speaker': row.get('speaker', ''),\n",
        "                'speaker_role': row.get('speaker_role', ''),\n",
        "                'quarter': row.get('quarter', ''),\n",
        "                'bank_code': bank_code,\n",
        "                'sentence_length': len(sentence),\n",
        "                'sentence_word_count': len(sentence.split()),\n",
        "                'sentence_order': sent_idx,\n",
        "                'readability_score': row.get('text_readability', 50),\n",
        "\n",
        "                # Model readiness flags\n",
        "                **{f'ready_for_{model}': ready for model, ready in model_ready.items()},\n",
        "\n",
        "                # Financial context flags\n",
        "                **financial_indicators,\n",
        "\n",
        "                # Processing metadata\n",
        "                'processing_timestamp': pd.Timestamp.now().isoformat(),\n",
        "                'preprocessing_version': 'enhanced_v1'\n",
        "            }\n",
        "\n",
        "            sentences.append(sentence_record)\n",
        "            sentence_count += 1\n",
        "\n",
        "        if sentence_count > 0:\n",
        "            processed_count += 1\n",
        "\n",
        "    sentences_df = pd.DataFrame(sentences)\n",
        "\n",
        "    print(f\"  Processed {processed_count}/{len(df)} Q&A pairs\")\n",
        "    print(f\"  Created sentence-level dataset: {len(sentences_df)} sentences\")\n",
        "\n",
        "    if len(sentences_df) > 0:\n",
        "        # Model readiness summary\n",
        "        for model_key in MODELS.keys():\n",
        "            ready_count = sentences_df[f'ready_for_{model_key}'].sum()\n",
        "            print(f\"    Ready for {model_key}: {ready_count} ({ready_count/len(sentences_df)*100:.1f}%)\")\n",
        "\n",
        "        # Financial context summary\n",
        "        financial_stats = {\n",
        "            'with_numbers': sentences_df['has_financial_numbers'].sum(),\n",
        "            'with_temporal': sentences_df['has_temporal_ref'].sum(),\n",
        "            'with_performance': sentences_df['has_performance_terms'].sum(),\n",
        "            'with_financial_terms': sentences_df['has_financial_terms'].sum()\n",
        "        }\n",
        "\n",
        "        print(f\"  Financial context analysis:\")\n",
        "        for context, count in financial_stats.items():\n",
        "            print(f\"    {context}: {count} ({count/len(sentences_df)*100:.1f}%)\")\n",
        "\n",
        "        # Sample sentences for different models\n",
        "        print(f\"  Sample sentences:\")\n",
        "        for model_key in list(MODELS.keys())[:2]:  # Show first 2 models\n",
        "            model_ready_sentences = sentences_df[sentences_df[f'ready_for_{model_key}']]\n",
        "            if len(model_ready_sentences) > 0:\n",
        "                sample = model_ready_sentences.iloc[0]['text']\n",
        "                print(f\"    {model_key}: '{sample[:100]}...'\")\n",
        "\n",
        "    return sentences_df\n",
        "\n",
        "# Create enhanced sentence-level datasets for 4 models\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "sentence_level_datasets = {}\n",
        "\n",
        "for bank in BANKS:\n",
        "    sentence_level_datasets[bank] = {}\n",
        "\n",
        "    if bank in processed_datasets:\n",
        "        print(f\"\\nüî¨ Creating sentence-level datasets for {bank.upper()}...\")\n",
        "\n",
        "        # Process quarterly datasets\n",
        "        for quarter in QUARTERS:\n",
        "            if quarter in processed_datasets[bank] and processed_datasets[bank][quarter] is not None:\n",
        "                sentence_level_datasets[bank][quarter] = preprocess_for_sentiment_models_enhanced(\n",
        "                    processed_datasets[bank][quarter], bank\n",
        "                )\n",
        "\n",
        "        # Process combined dataset\n",
        "        if \"combined\" in processed_datasets[bank] and processed_datasets[bank][\"combined\"] is not None:\n",
        "            sentence_level_datasets[bank][\"combined\"] = preprocess_for_sentiment_models_enhanced(\n",
        "                processed_datasets[bank][\"combined\"], bank\n",
        "            )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRDu5g_kArEj",
        "outputId": "8481b44f-1257-4c85-f6ae-947d72d487da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "============================================================\n",
            "\n",
            "üî¨ Creating sentence-level datasets for JPM...\n",
            "\n",
            "üî¨ [JPM] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "------------------------------------------------------------\n",
            "Input shape: (106, 28)\n",
            "  Processed 105/106 Q&A pairs\n",
            "  Created sentence-level dataset: 313 sentences\n",
            "    Ready for finbert_yiyanghkust: 56 (17.9%)\n",
            "    Ready for finbert_prosusai: 56 (17.9%)\n",
            "    Ready for distilroberta: 313 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 313 (100.0%)\n",
            "  Financial context analysis:\n",
            "    with_numbers: 16 (5.1%)\n",
            "    with_temporal: 30 (9.6%)\n",
            "    with_performance: 16 (5.1%)\n",
            "    with_financial_terms: 13 (4.2%)\n",
            "  Sample sentences:\n",
            "    finbert_yiyanghkust: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 bill...'\n",
            "    finbert_prosusai: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 bill...'\n",
            "\n",
            "üî¨ [JPM] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "------------------------------------------------------------\n",
            "Input shape: (140, 28)\n",
            "  Processed 140/140 Q&A pairs\n",
            "  Created sentence-level dataset: 440 sentences\n",
            "    Ready for finbert_yiyanghkust: 85 (19.3%)\n",
            "    Ready for finbert_prosusai: 85 (19.3%)\n",
            "    Ready for distilroberta: 440 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 440 (100.0%)\n",
            "  Financial context analysis:\n",
            "    with_numbers: 27 (6.1%)\n",
            "    with_temporal: 34 (7.7%)\n",
            "    with_performance: 27 (6.1%)\n",
            "    with_financial_terms: 16 (3.6%)\n",
            "  Sample sentences:\n",
            "    finbert_yiyanghkust: 'Turning to Asset & Wealth Management to complete our lines of business...'\n",
            "    finbert_prosusai: 'Turning to Asset & Wealth Management to complete our lines of business...'\n",
            "\n",
            "üî¨ [JPM] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "------------------------------------------------------------\n",
            "Input shape: (245, 28)\n",
            "  Processed 244/245 Q&A pairs\n",
            "  Created sentence-level dataset: 752 sentences\n",
            "    Ready for finbert_yiyanghkust: 141 (18.8%)\n",
            "    Ready for finbert_prosusai: 141 (18.8%)\n",
            "    Ready for distilroberta: 752 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 752 (100.0%)\n",
            "  Financial context analysis:\n",
            "    with_numbers: 43 (5.7%)\n",
            "    with_temporal: 64 (8.5%)\n",
            "    with_performance: 43 (5.7%)\n",
            "    with_financial_terms: 29 (3.9%)\n",
            "  Sample sentences:\n",
            "    finbert_yiyanghkust: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 bill...'\n",
            "    finbert_prosusai: 'Thank you and good morning, everyone. Starting on page 1, the Firm reported net income of $14.6 bill...'\n",
            "\n",
            "üî¨ Creating sentence-level datasets for HSBC...\n",
            "\n",
            "üî¨ [HSBC] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "------------------------------------------------------------\n",
            "Input shape: (49, 29)\n",
            "  Processed 49/49 Q&A pairs\n",
            "  Created sentence-level dataset: 300 sentences\n",
            "    Ready for finbert_yiyanghkust: 121 (40.3%)\n",
            "    Ready for finbert_prosusai: 121 (40.3%)\n",
            "    Ready for distilroberta: 300 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 300 (100.0%)\n",
            "  Financial context analysis:\n",
            "    with_numbers: 51 (17.0%)\n",
            "    with_temporal: 78 (26.0%)\n",
            "    with_performance: 47 (15.7%)\n",
            "    with_financial_terms: 17 (5.7%)\n",
            "  Sample sentences:\n",
            "    finbert_yiyanghkust: 'Overall, there was a strong quarter, marked by three key drivers ‚Äì momentum in our earnings, discipl...'\n",
            "    finbert_prosusai: 'Overall, there was a strong quarter, marked by three key drivers ‚Äì momentum in our earnings, discipl...'\n",
            "\n",
            "üî¨ [HSBC] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "------------------------------------------------------------\n",
            "Input shape: (30, 29)\n",
            "  Processed 30/30 Q&A pairs\n",
            "  Created sentence-level dataset: 340 sentences\n",
            "    Ready for finbert_yiyanghkust: 116 (34.1%)\n",
            "    Ready for finbert_prosusai: 116 (34.1%)\n",
            "    Ready for distilroberta: 340 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 340 (100.0%)\n",
            "  Financial context analysis:\n",
            "    with_numbers: 92 (27.1%)\n",
            "    with_temporal: 85 (25.0%)\n",
            "    with_performance: 49 (14.4%)\n",
            "    with_financial_terms: 11 (3.2%)\n",
            "  Sample sentences:\n",
            "    finbert_yiyanghkust: 'Welcome, all, to today‚Äôs call. I‚Äôm joined by Pam. Before Pam takes you through the second quarter nu...'\n",
            "    finbert_prosusai: 'Welcome, all, to today‚Äôs call. I‚Äôm joined by Pam. Before Pam takes you through the second quarter nu...'\n",
            "\n",
            "üî¨ [HSBC] SENTENCE-LEVEL PREPROCESSING FOR 4 MODELS\n",
            "------------------------------------------------------------\n",
            "Input shape: (79, 29)\n",
            "  Processed 79/79 Q&A pairs\n",
            "  Created sentence-level dataset: 640 sentences\n",
            "    Ready for finbert_yiyanghkust: 237 (37.0%)\n",
            "    Ready for finbert_prosusai: 237 (37.0%)\n",
            "    Ready for distilroberta: 640 (100.0%)\n",
            "    Ready for cardiffnlp_roberta: 640 (100.0%)\n",
            "  Financial context analysis:\n",
            "    with_numbers: 143 (22.3%)\n",
            "    with_temporal: 163 (25.5%)\n",
            "    with_performance: 96 (15.0%)\n",
            "    with_financial_terms: 28 (4.4%)\n",
            "  Sample sentences:\n",
            "    finbert_yiyanghkust: 'Overall, there was a strong quarter, marked by three key drivers ‚Äì momentum in our earnings, discipl...'\n",
            "    finbert_prosusai: 'Overall, there was a strong quarter, marked by three key drivers ‚Äì momentum in our earnings, discipl...'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Data Quality Assessment\n",
        "\n",
        "def assess_enhanced_data_quality():\n",
        "    \"\"\"Comprehensive quality assessment for multi-bank, multi-model analysis.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ENHANCED DATA QUALITY ASSESSMENT\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    quality_report = {\n",
        "        \"processing_timestamp\": pd.Timestamp.now().isoformat(),\n",
        "        \"banks_processed\": {},\n",
        "        \"model_readiness\": {},\n",
        "        \"financial_context_analysis\": {},\n",
        "        \"overall_statistics\": {}\n",
        "    }\n",
        "\n",
        "    total_qa_pairs = 0\n",
        "    total_sentences = 0\n",
        "    total_memory = 0\n",
        "\n",
        "    for bank in BANKS:\n",
        "        bank_stats = {\n",
        "            \"qa_level\": {},\n",
        "            \"sentence_level\": {},\n",
        "            \"model_readiness\": {},\n",
        "            \"quality_issues\": []\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüìä [{bank.upper()}] Quality Assessment\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Assess Q&A level data\n",
        "        if bank in processed_datasets:\n",
        "            for dataset_type, df in processed_datasets[bank].items():\n",
        "                if df is not None:\n",
        "                    bank_stats[\"qa_level\"][dataset_type] = {\n",
        "                        \"shape\": df.shape,\n",
        "                        \"valid_records\": df.get('is_valid_for_analysis', pd.Series([True] * len(df))).sum(),\n",
        "                        \"memory_mb\": df.memory_usage(deep=True).sum() / 1024**2\n",
        "                    }\n",
        "                    total_qa_pairs += len(df)\n",
        "                    total_memory += df.memory_usage(deep=True).sum()\n",
        "\n",
        "        # Assess sentence-level data\n",
        "        if bank in sentence_level_datasets:\n",
        "            for dataset_type, df in sentence_level_datasets[bank].items():\n",
        "                if df is not None:\n",
        "                    bank_stats[\"sentence_level\"][dataset_type] = {\n",
        "                        \"shape\": df.shape,\n",
        "                        \"memory_mb\": df.memory_usage(deep=True).sum() / 1024**2\n",
        "                    }\n",
        "                    total_sentences += len(df)\n",
        "                    total_memory += df.memory_usage(deep=True).sum()\n",
        "\n",
        "                    # Model readiness assessment\n",
        "                    for model_key in MODELS.keys():\n",
        "                        ready_col = f'ready_for_{model_key}'\n",
        "                        if ready_col in df.columns:\n",
        "                            ready_count = df[ready_col].sum()\n",
        "                            ready_pct = ready_count / len(df) * 100\n",
        "\n",
        "                            if model_key not in bank_stats[\"model_readiness\"]:\n",
        "                                bank_stats[\"model_readiness\"][model_key] = {}\n",
        "\n",
        "                            bank_stats[\"model_readiness\"][model_key][dataset_type] = {\n",
        "                                \"ready_count\": ready_count,\n",
        "                                \"ready_percentage\": ready_pct\n",
        "                            }\n",
        "\n",
        "                            # Flag potential issues\n",
        "                            if ready_pct < 80:\n",
        "                                bank_stats[\"quality_issues\"].append(\n",
        "                                    f\"{dataset_type}: Low {model_key} readiness ({ready_pct:.1f}%)\"\n",
        "                                )\n",
        "\n",
        "        quality_report[\"banks_processed\"][bank] = bank_stats\n",
        "\n",
        "        # Print bank summary\n",
        "        qa_datasets = len(bank_stats[\"qa_level\"])\n",
        "        sentence_datasets = len(bank_stats[\"sentence_level\"])\n",
        "        issues = len(bank_stats[\"quality_issues\"])\n",
        "\n",
        "        print(f\"  Q&A level datasets: {qa_datasets}\")\n",
        "        print(f\"  Sentence level datasets: {sentence_datasets}\")\n",
        "        print(f\"  Quality issues: {issues}\")\n",
        "\n",
        "        if bank_stats[\"quality_issues\"]:\n",
        "            for issue in bank_stats[\"quality_issues\"]:\n",
        "                print(f\"    - {issue}\")\n",
        "        else:\n",
        "            print(f\"    No major quality issues detected\")\n",
        "\n",
        "    # Overall model readiness summary\n",
        "    print(f\"\\nüéØ Model Readiness Summary:\")\n",
        "    for model_key in MODELS.keys():\n",
        "        total_ready = 0\n",
        "        total_records = 0\n",
        "\n",
        "        for bank in BANKS:\n",
        "            if bank in quality_report[\"banks_processed\"]:\n",
        "                bank_readiness = quality_report[\"banks_processed\"][bank][\"model_readiness\"]\n",
        "                if model_key in bank_readiness:\n",
        "                    for dataset_stats in bank_readiness[model_key].values():\n",
        "                        total_ready += dataset_stats[\"ready_count\"]\n",
        "                        total_records += dataset_stats[\"ready_count\"] / dataset_stats[\"ready_percentage\"] * 100\n",
        "\n",
        "        if total_records > 0:\n",
        "            overall_readiness = total_ready / total_records * 100\n",
        "            quality_report[\"model_readiness\"][model_key] = {\n",
        "                \"overall_readiness_percentage\": overall_readiness,\n",
        "                \"total_ready_records\": total_ready\n",
        "            }\n",
        "            print(f\"  {model_key}: {total_ready:,} records ready ({overall_readiness:.1f}%)\")\n",
        "\n",
        "    # Overall statistics\n",
        "    quality_report[\"overall_statistics\"] = {\n",
        "        \"total_qa_pairs\": total_qa_pairs,\n",
        "        \"total_sentences\": total_sentences,\n",
        "        \"total_memory_mb\": total_memory / 1024**2,\n",
        "        \"banks_with_complete_data\": len([\n",
        "            bank for bank in BANKS\n",
        "            if bank in sentence_level_datasets and\n",
        "            len(sentence_level_datasets[bank]) > 0\n",
        "        ]),\n",
        "        \"models_with_high_readiness\": len([\n",
        "            model for model, stats in quality_report[\"model_readiness\"].items()\n",
        "            if stats.get(\"overall_readiness_percentage\", 0) >= 80\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìà Overall Statistics:\")\n",
        "    print(f\"  Total Q&A pairs: {total_qa_pairs:,}\")\n",
        "    print(f\"  Total sentences: {total_sentences:,}\")\n",
        "    print(f\"  Total memory: {total_memory / 1024**2:.2f} MB\")\n",
        "    print(f\"  Banks with complete data: {quality_report['overall_statistics']['banks_with_complete_data']}/{len(BANKS)}\")\n",
        "    print(f\"  Models with >80% readiness: {quality_report['overall_statistics']['models_with_high_readiness']}/{len(MODELS)}\")\n",
        "\n",
        "    return quality_report\n",
        "\n",
        "# Run enhanced quality assessment\n",
        "enhanced_quality_report = assess_enhanced_data_quality()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G11AhNIrAnVO",
        "outputId": "0ce37da5-e9e9-4b2f-da36-07e01f2e2623"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED DATA QUALITY ASSESSMENT\n",
            "============================================================\n",
            "\n",
            "üìä [JPM] Quality Assessment\n",
            "----------------------------------------\n",
            "  Q&A level datasets: 3\n",
            "  Sentence level datasets: 3\n",
            "  Quality issues: 6\n",
            "    - q1_2025: Low finbert_yiyanghkust readiness (17.9%)\n",
            "    - q1_2025: Low finbert_prosusai readiness (17.9%)\n",
            "    - q2_2025: Low finbert_yiyanghkust readiness (19.3%)\n",
            "    - q2_2025: Low finbert_prosusai readiness (19.3%)\n",
            "    - combined: Low finbert_yiyanghkust readiness (18.8%)\n",
            "    - combined: Low finbert_prosusai readiness (18.8%)\n",
            "\n",
            "üìä [HSBC] Quality Assessment\n",
            "----------------------------------------\n",
            "  Q&A level datasets: 3\n",
            "  Sentence level datasets: 3\n",
            "  Quality issues: 6\n",
            "    - q1_2025: Low finbert_yiyanghkust readiness (40.3%)\n",
            "    - q1_2025: Low finbert_prosusai readiness (40.3%)\n",
            "    - q2_2025: Low finbert_yiyanghkust readiness (34.1%)\n",
            "    - q2_2025: Low finbert_prosusai readiness (34.1%)\n",
            "    - combined: Low finbert_yiyanghkust readiness (37.0%)\n",
            "    - combined: Low finbert_prosusai readiness (37.0%)\n",
            "\n",
            "üéØ Model Readiness Summary:\n",
            "  finbert_yiyanghkust: 756 records ready (27.1%)\n",
            "  finbert_prosusai: 756 records ready (27.1%)\n",
            "  distilroberta: 2,785 records ready (100.0%)\n",
            "  cardiffnlp_roberta: 2,785 records ready (100.0%)\n",
            "\n",
            "üìà Overall Statistics:\n",
            "  Total Q&A pairs: 649\n",
            "  Total sentences: 2,785\n",
            "  Total memory: 4.39 MB\n",
            "  Banks with complete data: 2/2\n",
            "  Models with >80% readiness: 2/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Enhanced Clean and Processed Datasets\n",
        "\n",
        "def save_enhanced_datasets():\n",
        "    \"\"\"Save all cleaned and processed datasets with enhanced organization.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SAVING ENHANCED DATASETS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    saved_files = {}\n",
        "\n",
        "    for bank in BANKS:\n",
        "        saved_files[bank] = {}\n",
        "\n",
        "        print(f\"\\nüíæ Saving {bank.upper()} datasets...\")\n",
        "\n",
        "        # Save Q&A level cleaned datasets\n",
        "        if bank in processed_datasets:\n",
        "            for dataset_type, df in processed_datasets[bank].items():\n",
        "                if df is not None:\n",
        "                    filename = f\"clean_{bank}_{dataset_type}_qa_level.csv\"\n",
        "\n",
        "                    # Save to multiple locations\n",
        "                    drive_path = drive_base / f\"data/clean/{bank}\" / filename\n",
        "                    colab_path = colab_base / f\"data/clean/{bank}\" / filename\n",
        "\n",
        "                    drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    colab_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                    df.to_csv(drive_path, index=False)\n",
        "                    df.to_csv(colab_path, index=False)\n",
        "\n",
        "                    saved_files[bank][f\"qa_{dataset_type}\"] = {\n",
        "                        \"drive_path\": str(drive_path),\n",
        "                        \"colab_path\": str(colab_path),\n",
        "                        \"shape\": df.shape,\n",
        "                        \"type\": \"qa_level\"\n",
        "                    }\n",
        "\n",
        "                    print(f\"    Q&A {dataset_type}: {filename} ({df.shape})\")\n",
        "\n",
        "        # Save sentence level datasets for 4 models\n",
        "        if bank in sentence_level_datasets:\n",
        "            for dataset_type, df in sentence_level_datasets[bank].items():\n",
        "                if df is not None:\n",
        "                    filename = f\"processed_{bank}_{dataset_type}_sentence_level.csv\"\n",
        "\n",
        "                    # Save to processed directory\n",
        "                    drive_path = drive_base / f\"data/processed/{bank}\" / filename\n",
        "                    colab_path = colab_base / f\"data/processed/{bank}\" / filename\n",
        "\n",
        "                    drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    colab_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                    df.to_csv(drive_path, index=False)\n",
        "                    df.to_csv(colab_path, index=False)\n",
        "\n",
        "                    saved_files[bank][f\"sentence_{dataset_type}\"] = {\n",
        "                        \"drive_path\": str(drive_path),\n",
        "                        \"colab_path\": str(colab_path),\n",
        "                        \"shape\": df.shape,\n",
        "                        \"type\": \"sentence_level\"\n",
        "                    }\n",
        "\n",
        "                    print(f\"    Sentence {dataset_type}: {filename} ({df.shape})\")\n",
        "\n",
        "    return saved_files\n",
        "\n",
        "# Save all enhanced datasets\n",
        "saved_files = save_enhanced_datasets()\n"
      ],
      "metadata": {
        "id": "_Jzqqa_wAwek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8531e8-0428-4669-d3d4-a4f17670e3a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING ENHANCED DATASETS\n",
            "============================================================\n",
            "\n",
            "üíæ Saving JPM datasets...\n",
            "    Q&A q1_2025: clean_jpm_q1_2025_qa_level.csv ((106, 28))\n",
            "    Q&A q2_2025: clean_jpm_q2_2025_qa_level.csv ((140, 28))\n",
            "    Q&A combined: clean_jpm_combined_qa_level.csv ((245, 28))\n",
            "    Sentence q1_2025: processed_jpm_q1_2025_sentence_level.csv ((313, 21))\n",
            "    Sentence q2_2025: processed_jpm_q2_2025_sentence_level.csv ((440, 21))\n",
            "    Sentence combined: processed_jpm_combined_sentence_level.csv ((752, 21))\n",
            "\n",
            "üíæ Saving HSBC datasets...\n",
            "    Q&A q1_2025: clean_hsbc_q1_2025_qa_level.csv ((49, 29))\n",
            "    Q&A q2_2025: clean_hsbc_q2_2025_qa_level.csv ((30, 29))\n",
            "    Q&A combined: clean_hsbc_combined_qa_level.csv ((79, 29))\n",
            "    Sentence q1_2025: processed_hsbc_q1_2025_sentence_level.csv ((300, 21))\n",
            "    Sentence q2_2025: processed_hsbc_q2_2025_sentence_level.csv ((340, 21))\n",
            "    Sentence combined: processed_hsbc_combined_sentence_level.csv ((640, 21))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Enhanced Processing Report\n",
        "\n",
        "enhanced_processing_report = {\n",
        "    \"processing_timestamp\": pd.Timestamp.now().isoformat(),\n",
        "    \"banks_processed\": BANKS,\n",
        "    \"models_configured\": list(MODELS.keys()),\n",
        "    \"processing_stages\": [\n",
        "        \"column_standardization\",\n",
        "        \"speaker_role_detection\",\n",
        "        \"enhanced_text_cleaning\",\n",
        "        \"duplicate_removal\",\n",
        "        \"metadata_enhancement\",\n",
        "        \"sentence_level_preprocessing\",\n",
        "        \"model_readiness_assessment\"\n",
        "    ],\n",
        "    \"datasets_created\": saved_files,\n",
        "    \"quality_assessment\": enhanced_quality_report,\n",
        "    \"model_readiness_summary\": {\n",
        "        model_key: stats for model_key, stats in enhanced_quality_report[\"model_readiness\"].items()\n",
        "    },\n",
        "    \"processing_statistics\": {\n",
        "        \"total_files_saved\": sum(len(bank_files) for bank_files in saved_files.values()),\n",
        "        \"total_qa_pairs\": enhanced_quality_report[\"overall_statistics\"][\"total_qa_pairs\"],\n",
        "        \"total_sentences\": enhanced_quality_report[\"overall_statistics\"][\"total_sentences\"],\n",
        "        \"total_memory_mb\": enhanced_quality_report[\"overall_statistics\"][\"total_memory_mb\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save processing report\n",
        "report_path = drive_base / \"configs\" / \"enhanced_processing_report.json\"\n",
        "with open(report_path, \"w\") as f:\n",
        "    json.dump(enhanced_processing_report, f, indent=2, default=str)\n",
        "\n",
        "print(f\"Enhanced processing report saved: {report_path}\")\n",
        "\n",
        "## Final Summary\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ENHANCED CLEANING AND PREPROCESSING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Extract key statistics\n",
        "total_files = sum(len(bank_files) for bank_files in saved_files.values())\n",
        "total_qa_pairs = enhanced_quality_report[\"overall_statistics\"][\"total_qa_pairs\"]\n",
        "total_sentences = enhanced_quality_report[\"overall_statistics\"][\"total_sentences\"]\n",
        "total_memory = enhanced_quality_report[\"overall_statistics\"][\"total_memory_mb\"]\n",
        "high_readiness_models = enhanced_quality_report[\"overall_statistics\"][\"models_with_high_readiness\"]\n",
        "\n",
        "print(f\"üìä Processing Summary:\")\n",
        "print(f\"  Banks processed: {len(BANKS)} ({', '.join([b.upper() for b in BANKS])})\")\n",
        "print(f\"  Files created: {total_files}\")\n",
        "print(f\"  Q&A pairs processed: {total_qa_pairs:,}\")\n",
        "print(f\"  Sentences created: {total_sentences:,}\")\n",
        "print(f\"  Total memory: {total_memory:.2f} MB\")\n",
        "print(f\"  Models with high readiness: {high_readiness_models}/{len(MODELS)}\")\n",
        "\n",
        "print(f\"\\nüè¶ Bank-specific Summary:\")\n",
        "for bank in BANKS:\n",
        "    if bank in saved_files:\n",
        "        bank_files = len(saved_files[bank])\n",
        "        qa_files = len([f for f in saved_files[bank].values() if f[\"type\"] == \"qa_level\"])\n",
        "        sentence_files = len([f for f in saved_files[bank].values() if f[\"type\"] == \"sentence_level\"])\n",
        "\n",
        "        print(f\"  {bank.upper()}:\")\n",
        "        print(f\"    Total files: {bank_files}\")\n",
        "        print(f\"    Q&A level: {qa_files}\")\n",
        "        print(f\"    Sentence level: {sentence_files}\")\n",
        "\n",
        "print(f\"\\nüéØ Model Readiness:\")\n",
        "for model_key in MODELS.keys():\n",
        "    if model_key in enhanced_quality_report[\"model_readiness\"]:\n",
        "        readiness = enhanced_quality_report[\"model_readiness\"][model_key]\n",
        "        ready_records = readiness[\"total_ready_records\"]\n",
        "        ready_pct = readiness[\"overall_readiness_percentage\"]\n",
        "        status = \"Ready\" if ready_pct >= 80 else \"Needs attention\"\n",
        "        print(f\"  {model_key}: {ready_records:,} records ({ready_pct:.1f}%) - {status}\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(f\"  1. Run 03b_manual_validation.ipynb for manual label validation\")\n",
        "print(f\"  2. Run 04_sentiment_analysis.ipynb for 4-model sentiment analysis\")\n",
        "print(f\"  3. Enhanced datasets ready for all {len(MODELS)} models\")\n",
        "print(f\"  4. Financial context features integrated for domain-specific analysis\")\n",
        "\n",
        "print(f\"\\nEnhanced preprocessing complete for multi-bank, multi-model analysis!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQiLvJQzA1Vx",
        "outputId": "4dfac300-5540-4325-a362-240563b41d5b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced processing report saved: /content/drive/MyDrive/CAM_DS_AI_Project_Enhanced/configs/enhanced_processing_report.json\n",
            "\n",
            "============================================================\n",
            "ENHANCED CLEANING AND PREPROCESSING COMPLETE\n",
            "============================================================\n",
            "üìä Processing Summary:\n",
            "  Banks processed: 2 (JPM, HSBC)\n",
            "  Files created: 12\n",
            "  Q&A pairs processed: 649\n",
            "  Sentences created: 2,785\n",
            "  Total memory: 4.39 MB\n",
            "  Models with high readiness: 2/4\n",
            "\n",
            "üè¶ Bank-specific Summary:\n",
            "  JPM:\n",
            "    Total files: 6\n",
            "    Q&A level: 3\n",
            "    Sentence level: 3\n",
            "  HSBC:\n",
            "    Total files: 6\n",
            "    Q&A level: 3\n",
            "    Sentence level: 3\n",
            "\n",
            "üéØ Model Readiness:\n",
            "  finbert_yiyanghkust: 756 records (27.1%) - Needs attention\n",
            "  finbert_prosusai: 756 records (27.1%) - Needs attention\n",
            "  distilroberta: 2,785 records (100.0%) - Ready\n",
            "  cardiffnlp_roberta: 2,785 records (100.0%) - Ready\n",
            "\n",
            "üöÄ Next Steps:\n",
            "  1. Run 03b_manual_validation.ipynb for manual label validation\n",
            "  2. Run 04_sentiment_analysis.ipynb for 4-model sentiment analysis\n",
            "  3. Enhanced datasets ready for all 4 models\n",
            "  4. Financial context features integrated for domain-specific analysis\n",
            "\n",
            "Enhanced preprocessing complete for multi-bank, multi-model analysis!\n"
          ]
        }
      ]
    }
  ]
}