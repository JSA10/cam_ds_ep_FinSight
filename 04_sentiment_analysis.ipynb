{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vCXVGc21E4R5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8b88a6-1546-4a2e-c22b-7ce5db3eb765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enhanced sentiment analysis for bank: JPM\n",
            "Device: CPU\n"
          ]
        }
      ],
      "source": [
        "# 04_sentiment_analysis_jpm_enhanced.ipynb\n",
        "# Purpose: Enhanced sentiment analysis with performance optimization and result improvement techniques\n",
        "# Models: yiyanghkust/finbert-tone, ProsusAI/finbert + Enhanced techniques\n",
        "# Input: processed datasets + manual validation results\n",
        "# Output: Enhanced sentiment results with improved performance metrics\n",
        "\n",
        "## Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced ML libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Enhanced text processing\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "try:\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "except:\n",
        "    print(\"NLTK VADER not available\")\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Location A: Google Drive (Primary drive)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load configuration\n",
        "config_path = Path(\"/content/drive/MyDrive/CAM_DS_AI_Project/config.json\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "SEED = config[\"SEED\"]\n",
        "BANK_CODE = config[\"BANK_CODE\"]\n",
        "drive_base = Path(config[\"drive_base\"])\n",
        "colab_base = Path(config[\"colab_base\"])\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(f\"Enhanced sentiment analysis for bank: {BANK_CODE.upper()}\")\n",
        "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Paths\n",
        "\n",
        "processed_data_path = drive_base / \"data/processed/jpm\"\n",
        "manual_validation_path = drive_base / \"data/manual_validation/jpm\"\n",
        "results_sentiment_path = drive_base / \"results/sentiment/jpm\"\n",
        "models_path = drive_base / \"models\"\n",
        "\n",
        "# Ensure directories exist\n",
        "results_sentiment_path.mkdir(parents=True, exist_ok=True)\n",
        "models_path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "y93moUjoFEhK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Data and Manual Validation Results\n",
        "\n",
        "def load_processed_dataset(filename: str) -> pd.DataFrame:\n",
        "    \"\"\"Load processed dataset with error handling.\"\"\"\n",
        "    file_path = processed_data_path / filename\n",
        "    if not file_path.exists():\n",
        "        print(f\"Warning: File not found: {file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {filename}: {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def load_manual_validation_results() -> Tuple[pd.DataFrame, Dict]:\n",
        "    \"\"\"Load manual validation results.\"\"\"\n",
        "    # Load manual labels\n",
        "    manual_labels_path = results_sentiment_path / \"sentiment_sentence_jpm_multi_2025_validated.csv\"\n",
        "    manual_df = None\n",
        "    if manual_labels_path.exists():\n",
        "        try:\n",
        "            manual_df = pd.read_csv(manual_labels_path)\n",
        "            print(f\"Loaded manual validation data: {manual_df.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load manual validation data: {e}\")\n",
        "\n",
        "    # Load validation report\n",
        "    validation_report_path = manual_validation_path / \"manual_validation_report.json\"\n",
        "    validation_report = {}\n",
        "    if validation_report_path.exists():\n",
        "        try:\n",
        "            with open(validation_report_path, 'r') as f:\n",
        "                validation_report = json.load(f)\n",
        "            print(\"Loaded validation report\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load validation report: {e}\")\n",
        "\n",
        "    return manual_df, validation_report\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "processed_jpm_q1_2025_df = load_processed_dataset(\"processed_jpm_q1_2025.csv\")\n",
        "processed_jpm_q2_2025_df = load_processed_dataset(\"processed_jpm_q2_2025.csv\")\n",
        "processed_jpm_multi_2025_df = load_processed_dataset(\"processed_jpm_multi_2025.csv\")\n",
        "\n",
        "# Load manual validation results\n",
        "manual_labels_df, validation_report = load_manual_validation_results()"
      ],
      "metadata": {
        "id": "zs7KB940FMYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea13bdf-b5bc-46f4-8b95-24c8bb3064df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Loaded processed_jpm_q1_2025.csv: (578, 9)\n",
            "Loaded processed_jpm_q2_2025.csv: (532, 9)\n",
            "Loaded processed_jpm_multi_2025.csv: (1110, 9)\n",
            "Loaded manual validation data: (1110, 41)\n",
            "Loaded validation report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced FinBERT Model with Confidence Calibration\n",
        "\n",
        "class EnhancedFinBERTAnalyzer:\n",
        "    \"\"\"Enhanced FinBERT analyzer with confidence calibration and ensemble methods.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, device: str = None):\n",
        "        self.model_name = model_name\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "        self.confidence_calibrator = None\n",
        "\n",
        "        # Enhanced configurations\n",
        "        self.model_configs = {\n",
        "            'yiyanghkust/finbert-tone': {\n",
        "                'labels': ['positive', 'neutral', 'negative'],\n",
        "                'max_length': 512,\n",
        "                'batch_size': 16,\n",
        "                'confidence_threshold': 0.6\n",
        "            },\n",
        "            'ProsusAI/finbert': {\n",
        "                'labels': ['positive', 'neutral', 'negative'],\n",
        "                'max_length': 512,\n",
        "                'batch_size': 16,\n",
        "                'confidence_threshold': 0.6\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.config = self.model_configs.get(model_name, {})\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the FinBERT model with enhanced features.\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading enhanced model: {self.model_name}\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Move model to device\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            # Create pipeline\n",
        "            self.pipeline = pipeline(\n",
        "                'sentiment-analysis',\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if self.device == 'cuda' else -1,\n",
        "                return_all_scores=True\n",
        "            )\n",
        "\n",
        "            print(f\"Enhanced model loaded on {self.device}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def calibrate_confidence(self, texts: List[str], true_labels: List[str] = None):\n",
        "        \"\"\"Calibrate confidence scores using manual labels if available.\"\"\"\n",
        "        if true_labels is None or len(true_labels) == 0:\n",
        "            print(\"No manual labels available for confidence calibration\")\n",
        "            return\n",
        "\n",
        "        print(\"Calibrating confidence scores...\")\n",
        "\n",
        "        # Get model predictions for calibration data\n",
        "        predictions = self.predict_sentiment_enhanced(texts, batch_size=8)\n",
        "\n",
        "        # Extract confidence scores and predictions\n",
        "        pred_labels = [p['predicted_label'] for p in predictions]\n",
        "        confidences = [p['predicted_score'] for p in predictions]\n",
        "\n",
        "        # Calculate calibration metrics\n",
        "        correct_mask = np.array(pred_labels) == np.array(true_labels)\n",
        "\n",
        "        # Platt scaling for confidence calibration\n",
        "        from sklearn.calibration import CalibratedClassifierCV\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "        # Simple confidence recalibration\n",
        "        confidence_bins = np.linspace(0, 1, 11)\n",
        "        bin_accuracies = []\n",
        "\n",
        "        for i in range(len(confidence_bins) - 1):\n",
        "            bin_mask = (np.array(confidences) >= confidence_bins[i]) & (np.array(confidences) < confidence_bins[i+1])\n",
        "            if bin_mask.sum() > 0:\n",
        "                bin_accuracy = correct_mask[bin_mask].mean()\n",
        "                bin_accuracies.append(bin_accuracy)\n",
        "            else:\n",
        "                bin_accuracies.append(0.0)\n",
        "\n",
        "        self.confidence_calibrator = {\n",
        "            'bin_edges': confidence_bins,\n",
        "            'bin_accuracies': bin_accuracies\n",
        "        }\n",
        "\n",
        "        print(f\"Confidence calibration complete\")\n",
        "\n",
        "    def get_calibrated_confidence(self, raw_confidence: float) -> float:\n",
        "        \"\"\"Get calibrated confidence score.\"\"\"\n",
        "        if self.confidence_calibrator is None:\n",
        "            return raw_confidence\n",
        "\n",
        "        # Find the appropriate bin\n",
        "        bin_edges = self.confidence_calibrator['bin_edges']\n",
        "        bin_accuracies = self.confidence_calibrator['bin_accuracies']\n",
        "\n",
        "        for i in range(len(bin_edges) - 1):\n",
        "            if bin_edges[i] <= raw_confidence < bin_edges[i+1]:\n",
        "                return bin_accuracies[i]\n",
        "\n",
        "        return raw_confidence\n",
        "\n",
        "    def predict_sentiment_enhanced(self, texts: List[str], batch_size: int = None) -> List[Dict]:\n",
        "        \"\"\"Enhanced sentiment prediction with additional features.\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
        "\n",
        "        batch_size = batch_size or self.config.get('batch_size', 16)\n",
        "        results = []\n",
        "\n",
        "        print(f\"Processing {len(texts)} texts with enhanced features...\")\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Enhanced Sentiment Analysis\"):\n",
        "            batch = texts[i:i + batch_size]\n",
        "\n",
        "            try:\n",
        "                # Get base model predictions\n",
        "                batch_results = self.pipeline(batch)\n",
        "\n",
        "                # Process each result with enhancements\n",
        "                for text_idx, text_result in enumerate(batch_results):\n",
        "                    text = batch[text_idx]\n",
        "\n",
        "                    # Convert to standard format\n",
        "                    scores_dict = {item['label'].lower(): item['score'] for item in text_result}\n",
        "\n",
        "                    # Get predicted label and score\n",
        "                    predicted_label = max(scores_dict, key=scores_dict.get)\n",
        "                    predicted_score = scores_dict[predicted_label]\n",
        "\n",
        "                    # Apply confidence calibration if available\n",
        "                    calibrated_score = self.get_calibrated_confidence(predicted_score)\n",
        "\n",
        "                    # Calculate additional metrics\n",
        "                    entropy = -sum(score * np.log(score + 1e-8) for score in scores_dict.values())\n",
        "                    max_prob_diff = predicted_score - sorted(scores_dict.values(), reverse=True)[1]\n",
        "\n",
        "                    # Text-based features\n",
        "                    text_features = self.extract_text_features(text)\n",
        "\n",
        "                    result = {\n",
        "                        'text': text,\n",
        "                        'predicted_label': predicted_label,\n",
        "                        'predicted_score': predicted_score,\n",
        "                        'calibrated_score': calibrated_score,\n",
        "                        'positive_score': scores_dict.get('positive', 0.0),\n",
        "                        'neutral_score': scores_dict.get('neutral', 0.0),\n",
        "                        'negative_score': scores_dict.get('negative', 0.0),\n",
        "                        'entropy': entropy,\n",
        "                        'max_prob_diff': max_prob_diff,\n",
        "                        'model_name': self.model_name,\n",
        "                        **text_features\n",
        "                    }\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch {i//batch_size + 1}: {str(e)}\")\n",
        "                # Add placeholder results for failed batch\n",
        "                for j in range(len(batch)):\n",
        "                    results.append({\n",
        "                        'text': batch[j],\n",
        "                        'predicted_label': 'neutral',\n",
        "                        'predicted_score': 0.33,\n",
        "                        'calibrated_score': 0.33,\n",
        "                        'positive_score': 0.33,\n",
        "                        'neutral_score': 0.34,\n",
        "                        'negative_score': 0.33,\n",
        "                        'entropy': 1.0,\n",
        "                        'max_prob_diff': 0.0,\n",
        "                        'model_name': self.model_name,\n",
        "                        'error': True\n",
        "                    })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_text_features(self, text: str) -> Dict:\n",
        "        \"\"\"Extract additional text-based features.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Basic text statistics\n",
        "        features['text_length'] = len(text)\n",
        "        features['word_count'] = len(text.split())\n",
        "        features['sentence_count'] = len(re.split(r'[.!?]+', text))\n",
        "\n",
        "        # Financial keyword indicators\n",
        "        positive_keywords = ['growth', 'profit', 'increase', 'strong', 'improved', 'positive', 'beat', 'exceed']\n",
        "        negative_keywords = ['loss', 'decline', 'decrease', 'weak', 'poor', 'negative', 'miss', 'below']\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        features['positive_keyword_count'] = sum(1 for kw in positive_keywords if kw in text_lower)\n",
        "        features['negative_keyword_count'] = sum(1 for kw in negative_keywords if kw in text_lower)\n",
        "        features['keyword_sentiment_score'] = features['positive_keyword_count'] - features['negative_keyword_count']\n",
        "\n",
        "        # Textual patterns\n",
        "        features['has_numbers'] = bool(re.search(r'\\d', text))\n",
        "        features['has_percentages'] = bool(re.search(r'\\d+%', text))\n",
        "        features['has_currency'] = bool(re.search(r'[$Â£â‚¬]\\d', text))\n",
        "        features['question_marks'] = text.count('?')\n",
        "        features['exclamation_marks'] = text.count('!')\n",
        "\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "Oxm7EgplFQPl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Ensemble and Blended Scoring Methods\n",
        "\n",
        "class SentimentEnsemble:\n",
        "    \"\"\"Ensemble methods for combining multiple sentiment analysis approaches.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.weights = {}\n",
        "        self.vader_analyzer = None\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.ml_classifier = None\n",
        "\n",
        "        # Initialize VADER if available\n",
        "        try:\n",
        "            self.vader_analyzer = SentimentIntensityAnalyzer()\n",
        "        except:\n",
        "            print(\"VADER sentiment analyzer not available\")\n",
        "\n",
        "    def add_model(self, name: str, model: EnhancedFinBERTAnalyzer, weight: float = 1.0):\n",
        "        \"\"\"Add a model to the ensemble.\"\"\"\n",
        "        self.models[name] = model\n",
        "        self.weights[name] = weight\n",
        "        print(f\"Added {name} to ensemble with weight {weight}\")\n",
        "\n",
        "    def train_ml_classifier(self, texts: List[str], labels: List[str]):\n",
        "        \"\"\"Train a simple ML classifier as backup.\"\"\"\n",
        "        print(\"Training backup ML classifier...\")\n",
        "\n",
        "        # Create TF-IDF features\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "        X = self.tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "        # Train logistic regression\n",
        "        self.ml_classifier = LogisticRegression(random_state=SEED, max_iter=1000)\n",
        "        self.ml_classifier.fit(X, labels)\n",
        "\n",
        "        print(\"ML classifier trained\")\n",
        "\n",
        "    def predict_ensemble(self, texts: List[str]) -> List[Dict]:\n",
        "        \"\"\"Generate ensemble predictions.\"\"\"\n",
        "        print(f\"Generating ensemble predictions for {len(texts)} texts...\")\n",
        "\n",
        "        all_predictions = {}\n",
        "\n",
        "        # Get predictions from all models\n",
        "        for model_name, model in self.models.items():\n",
        "            try:\n",
        "                predictions = model.predict_sentiment_enhanced(texts)\n",
        "                all_predictions[model_name] = predictions\n",
        "                print(f\"Got predictions from {model_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error getting predictions from {model_name}: {e}\")\n",
        "\n",
        "        # Get VADER predictions if available\n",
        "        if self.vader_analyzer:\n",
        "            vader_predictions = []\n",
        "            for text in texts:\n",
        "                scores = self.vader_analyzer.polarity_scores(text)\n",
        "                compound = scores['compound']\n",
        "\n",
        "                if compound >= 0.05:\n",
        "                    label = 'positive'\n",
        "                elif compound <= -0.05:\n",
        "                    label = 'negative'\n",
        "                else:\n",
        "                    label = 'neutral'\n",
        "\n",
        "                vader_predictions.append({\n",
        "                    'predicted_label': label,\n",
        "                    'predicted_score': abs(compound),\n",
        "                    'compound_score': compound\n",
        "                })\n",
        "\n",
        "            all_predictions['vader'] = vader_predictions\n",
        "\n",
        "        # Get ML classifier predictions if available\n",
        "        if self.ml_classifier and self.tfidf_vectorizer:\n",
        "            try:\n",
        "                X = self.tfidf_vectorizer.transform(texts)\n",
        "                ml_labels = self.ml_classifier.predict(X)\n",
        "                ml_probas = self.ml_classifier.predict_proba(X)\n",
        "\n",
        "                ml_predictions = []\n",
        "                for i, label in enumerate(ml_labels):\n",
        "                    max_prob = ml_probas[i].max()\n",
        "                    ml_predictions.append({\n",
        "                        'predicted_label': label,\n",
        "                        'predicted_score': max_prob\n",
        "                    })\n",
        "\n",
        "                all_predictions['ml_classifier'] = ml_predictions\n",
        "            except Exception as e:\n",
        "                print(f\"Error getting ML classifier predictions: {e}\")\n",
        "\n",
        "        # Combine predictions\n",
        "        ensemble_results = []\n",
        "        for i in range(len(texts)):\n",
        "            # Collect predictions for this text\n",
        "            text_predictions = {}\n",
        "            text_scores = {}\n",
        "\n",
        "            for model_name, predictions in all_predictions.items():\n",
        "                if i < len(predictions):\n",
        "                    pred = predictions[i]\n",
        "                    text_predictions[model_name] = pred['predicted_label']\n",
        "                    text_scores[model_name] = pred['predicted_score']\n",
        "\n",
        "            # Weighted voting for final prediction\n",
        "            label_votes = {}\n",
        "            total_weight = 0\n",
        "\n",
        "            for model_name, label in text_predictions.items():\n",
        "                weight = self.weights.get(model_name, 1.0)\n",
        "                confidence = text_scores.get(model_name, 0.5)\n",
        "\n",
        "                # Weight by model weight and confidence\n",
        "                vote_weight = weight * confidence\n",
        "\n",
        "                if label not in label_votes:\n",
        "                    label_votes[label] = 0\n",
        "                label_votes[label] += vote_weight\n",
        "                total_weight += vote_weight\n",
        "\n",
        "            # Normalize votes\n",
        "            if total_weight > 0:\n",
        "                for label in label_votes:\n",
        "                    label_votes[label] /= total_weight\n",
        "\n",
        "            # Get final prediction\n",
        "            if label_votes:\n",
        "                final_label = max(label_votes, key=label_votes.get)\n",
        "                final_confidence = label_votes[final_label]\n",
        "            else:\n",
        "                final_label = 'neutral'\n",
        "                final_confidence = 0.33\n",
        "\n",
        "            ensemble_result = {\n",
        "                'text': texts[i],\n",
        "                'ensemble_label': final_label,\n",
        "                'ensemble_confidence': final_confidence,\n",
        "                'individual_predictions': text_predictions,\n",
        "                'individual_scores': text_scores,\n",
        "                'label_votes': label_votes\n",
        "            }\n",
        "\n",
        "            ensemble_results.append(ensemble_result)\n",
        "\n",
        "        return ensemble_results\n"
      ],
      "metadata": {
        "id": "pdFG3FlXFVlx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Model Performance Evaluation\n",
        "\n",
        "class EnhancedModelEvaluator:\n",
        "    \"\"\"Enhanced evaluation with comprehensive metrics.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.evaluation_results = {}\n",
        "\n",
        "    def evaluate_model_performance(self, predictions: List[str], true_labels: List[str],\n",
        "                                 confidences: List[float] = None, model_name: str = \"model\") -> Dict:\n",
        "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "\n",
        "        if len(predictions) != len(true_labels):\n",
        "            raise ValueError(\"Predictions and true labels must have same length\")\n",
        "\n",
        "        print(f\"Evaluating {model_name} performance...\")\n",
        "\n",
        "        # Basic metrics\n",
        "        accuracy = (np.array(predictions) == np.array(true_labels)).mean()\n",
        "\n",
        "        # Classification report\n",
        "        report = classification_report(true_labels, predictions, output_dict=True, zero_division=0)\n",
        "\n",
        "        # Per-class metrics\n",
        "        f1_scores = {}\n",
        "        precision_scores = {}\n",
        "        recall_scores = {}\n",
        "\n",
        "        for label in set(true_labels + predictions):\n",
        "            if label in report:\n",
        "                f1_scores[label] = report[label]['f1-score']\n",
        "                precision_scores[label] = report[label]['precision']\n",
        "                recall_scores[label] = report[label]['recall']\n",
        "\n",
        "        # Weighted averages\n",
        "        weighted_f1 = report['weighted avg']['f1-score']\n",
        "        weighted_precision = report['weighted avg']['precision']\n",
        "        weighted_recall = report['weighted avg']['recall']\n",
        "\n",
        "        # Macro averages\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "        macro_precision = report['macro avg']['precision']\n",
        "        macro_recall = report['macro avg']['recall']\n",
        "\n",
        "        # Confidence-based metrics\n",
        "        confidence_metrics = {}\n",
        "        if confidences is not None:\n",
        "            confidences = np.array(confidences)\n",
        "            correct_mask = np.array(predictions) == np.array(true_labels)\n",
        "\n",
        "            # Accuracy by confidence bins\n",
        "            confidence_bins = np.linspace(0, 1, 6)  # 5 bins\n",
        "            bin_accuracies = []\n",
        "            bin_counts = []\n",
        "\n",
        "            for i in range(len(confidence_bins) - 1):\n",
        "                bin_mask = (confidences >= confidence_bins[i]) & (confidences < confidence_bins[i+1])\n",
        "                if bin_mask.sum() > 0:\n",
        "                    bin_accuracy = correct_mask[bin_mask].mean()\n",
        "                    bin_accuracies.append(bin_accuracy)\n",
        "                    bin_counts.append(bin_mask.sum())\n",
        "                else:\n",
        "                    bin_accuracies.append(0.0)\n",
        "                    bin_counts.append(0)\n",
        "\n",
        "            # Expected Calibration Error (ECE)\n",
        "            ece = 0\n",
        "            total_samples = len(predictions)\n",
        "            for i in range(len(bin_accuracies)):\n",
        "                if bin_counts[i] > 0:\n",
        "                    bin_confidence = (confidence_bins[i] + confidence_bins[i+1]) / 2\n",
        "                    ece += (bin_counts[i] / total_samples) * abs(bin_accuracies[i] - bin_confidence)\n",
        "\n",
        "            confidence_metrics = {\n",
        "                'confidence_bins': confidence_bins.tolist(),\n",
        "                'bin_accuracies': bin_accuracies,\n",
        "                'bin_counts': bin_counts,\n",
        "                'expected_calibration_error': ece,\n",
        "                'avg_confidence': confidences.mean(),\n",
        "                'confidence_std': confidences.std()\n",
        "            }\n",
        "\n",
        "        evaluation_result = {\n",
        "            'model_name': model_name,\n",
        "            'sample_size': len(predictions),\n",
        "            'accuracy': accuracy,\n",
        "            'weighted_f1': weighted_f1,\n",
        "            'weighted_precision': weighted_precision,\n",
        "            'weighted_recall': weighted_recall,\n",
        "            'macro_f1': macro_f1,\n",
        "            'macro_precision': macro_precision,\n",
        "            'macro_recall': macro_recall,\n",
        "            'per_class_f1': f1_scores,\n",
        "            'per_class_precision': precision_scores,\n",
        "            'per_class_recall': recall_scores,\n",
        "            'classification_report': report,\n",
        "            'confidence_metrics': confidence_metrics\n",
        "        }\n",
        "\n",
        "        # Print key metrics\n",
        "        print(f\"  Accuracy: {accuracy:.3f}\")\n",
        "        print(f\"  Weighted F1: {weighted_f1:.3f}\")\n",
        "        print(f\"  Macro F1: {macro_f1:.3f}\")\n",
        "        if confidence_metrics:\n",
        "            print(f\"  Avg Confidence: {confidence_metrics['avg_confidence']:.3f}\")\n",
        "            print(f\"  Calibration Error: {confidence_metrics['expected_calibration_error']:.3f}\")\n",
        "\n",
        "        self.evaluation_results[model_name] = evaluation_result\n",
        "        return evaluation_result\n",
        "\n",
        "    def compare_models(self) -> Dict:\n",
        "        \"\"\"Compare all evaluated models.\"\"\"\n",
        "        if not self.evaluation_results:\n",
        "            return {}\n",
        "\n",
        "        print(\"\\nModel Comparison Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        comparison = {\n",
        "            'model_rankings': {},\n",
        "            'best_model': None,\n",
        "            'performance_summary': {}\n",
        "        }\n",
        "\n",
        "        # Rank models by weighted F1\n",
        "        f1_scores = {name: results['weighted_f1'] for name, results in self.evaluation_results.items()}\n",
        "        ranked_models = sorted(f1_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        comparison['model_rankings']['by_f1'] = ranked_models\n",
        "        comparison['best_model'] = ranked_models[0][0] if ranked_models else None\n",
        "\n",
        "        # Performance summary\n",
        "        for model_name, results in self.evaluation_results.items():\n",
        "            print(f\"{model_name}:\")\n",
        "            print(f\"  F1: {results['weighted_f1']:.3f}\")\n",
        "            print(f\"  Precision: {results['weighted_precision']:.3f}\")\n",
        "            print(f\"  Recall: {results['weighted_recall']:.3f}\")\n",
        "            print(f\"  Accuracy: {results['accuracy']:.3f}\")\n",
        "\n",
        "            comparison['performance_summary'][model_name] = {\n",
        "                'weighted_f1': results['weighted_f1'],\n",
        "                'weighted_precision': results['weighted_precision'],\n",
        "                'weighted_recall': results['weighted_recall'],\n",
        "                'accuracy': results['accuracy']\n",
        "            }\n",
        "\n",
        "        if comparison['best_model']:\n",
        "            print(f\"\\nBest performing model: {comparison['best_model']}\")\n",
        "\n",
        "        return comparison\n"
      ],
      "metadata": {
        "id": "52KF7-5DFZ-T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize Enhanced Models\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIALIZING ENHANCED MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize enhanced analyzers\n",
        "finbert_tone_enhanced = EnhancedFinBERTAnalyzer('yiyanghkust/finbert-tone')\n",
        "prosus_finbert_enhanced = EnhancedFinBERTAnalyzer('ProsusAI/finbert')\n",
        "\n",
        "# Load models\n",
        "print(\"Loading enhanced FinBERT models...\")\n",
        "finbert_tone_loaded = finbert_tone_enhanced.load_model()\n",
        "prosus_finbert_loaded = prosus_finbert_enhanced.load_model()\n",
        "\n",
        "# Initialize ensemble\n",
        "ensemble = SentimentEnsemble()\n",
        "if finbert_tone_loaded:\n",
        "    ensemble.add_model('finbert_tone', finbert_tone_enhanced, weight=1.0)\n",
        "if prosus_finbert_loaded:\n",
        "    ensemble.add_model('prosus_finbert', prosus_finbert_enhanced, weight=1.0)\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = EnhancedModelEvaluator()\n"
      ],
      "metadata": {
        "id": "Vr_285iNFgOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2aaa67-5948-42ab-9b6a-425bd04cf470"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INITIALIZING ENHANCED MODELS\n",
            "============================================================\n",
            "Loading enhanced FinBERT models...\n",
            "Loading enhanced model: yiyanghkust/finbert-tone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced model loaded on cpu\n",
            "Loading enhanced model: ProsusAI/finbert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced model loaded on cpu\n",
            "Added finbert_tone to ensemble with weight 1.0\n",
            "Added prosus_finbert to ensemble with weight 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Confidence Calibration with Manual Labels\n",
        "\n",
        "if manual_labels_df is not None:\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"CONFIDENCE CALIBRATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Extract manually labeled data for calibration\n",
        "    manual_mask = manual_labels_df['human_label'].notna() & (manual_labels_df['human_label'] != '')\n",
        "    manual_calibration_df = manual_labels_df[manual_mask].copy()\n",
        "\n",
        "    if len(manual_calibration_df) > 10:  # Need minimum samples for calibration\n",
        "        calibration_texts = manual_calibration_df['text'].tolist()\n",
        "        calibration_labels = manual_calibration_df['human_label'].tolist()\n",
        "\n",
        "        # Calibrate both models\n",
        "        if finbert_tone_loaded:\n",
        "            finbert_tone_enhanced.calibrate_confidence(calibration_texts, calibration_labels)\n",
        "\n",
        "        if prosus_finbert_loaded:\n",
        "            prosus_finbert_enhanced.calibrate_confidence(calibration_texts, calibration_labels)\n",
        "\n",
        "        # Train backup ML classifier\n",
        "        ensemble.train_ml_classifier(calibration_texts, calibration_labels)\n",
        "\n",
        "        print(\"Confidence calibration complete\")\n",
        "    else:\n",
        "        print(\"Insufficient manual labels for confidence calibration\")"
      ],
      "metadata": {
        "id": "UnGqT1YsFlG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f560a1eb-3257-4205-c09a-ab218f7d5eb0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "CONFIDENCE CALIBRATION\n",
            "==================================================\n",
            "Calibrating confidence scores...\n",
            "Processing 200 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:31<00:00,  1.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence calibration complete\n",
            "Calibrating confidence scores...\n",
            "Processing 200 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:30<00:00,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence calibration complete\n",
            "Training backup ML classifier...\n",
            "ML classifier trained\n",
            "Confidence calibration complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Sentiment Analysis on Datasets\n",
        "\n",
        "def run_enhanced_sentiment_analysis(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
        "    \"\"\"Run enhanced sentiment analysis with all improvements.\"\"\"\n",
        "    if df is None:\n",
        "        print(f\"Cannot process {dataset_name} - dataset is None\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nðŸ”¬ ENHANCED SENTIMENT ANALYSIS - {dataset_name}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Input shape: {df.shape}\")\n",
        "\n",
        "    # Prepare text data\n",
        "    texts = df['text'].fillna('').astype(str).tolist()\n",
        "    print(f\"Processing {len(texts)} text entries\")\n",
        "\n",
        "    results_df = df.copy()\n",
        "\n",
        "    # Run enhanced FinBERT-tone analysis\n",
        "    if finbert_tone_loaded:\n",
        "        print(\"\\nðŸ¤– Running enhanced FinBERT-tone analysis...\")\n",
        "        try:\n",
        "            finbert_results = finbert_tone_enhanced.predict_sentiment_enhanced(texts)\n",
        "\n",
        "            # Add enhanced results to dataframe\n",
        "            for i, result in enumerate(finbert_results):\n",
        "                results_df.loc[i, 'finbert_tone_label'] = result['predicted_label']\n",
        "                results_df.loc[i, 'finbert_tone_score'] = result['predicted_score']\n",
        "                results_df.loc[i, 'finbert_tone_calibrated'] = result['calibrated_score']\n",
        "                results_df.loc[i, 'finbert_tone_entropy'] = result['entropy']\n",
        "                results_df.loc[i, 'finbert_tone_max_diff'] = result['max_prob_diff']\n",
        "                results_df.loc[i, 'finbert_tone_positive'] = result['positive_score']\n",
        "                results_df.loc[i, 'finbert_tone_neutral'] = result['neutral_score']\n",
        "                results_df.loc[i, 'finbert_tone_negative'] = result['negative_score']\n",
        "\n",
        "                # Add text features\n",
        "                for feature_name, feature_value in result.items():\n",
        "                    if feature_name.endswith('_count') or feature_name.startswith('has_') or feature_name.endswith('_score'):\n",
        "                        if feature_name not in ['positive_score', 'neutral_score', 'negative_score', 'predicted_score', 'calibrated_score']:\n",
        "                            results_df.loc[i, f'finbert_{feature_name}'] = feature_value\n",
        "\n",
        "            print(\"Enhanced FinBERT-tone analysis complete\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in enhanced FinBERT-tone analysis: {str(e)}\")\n",
        "\n",
        "    # Run enhanced ProsusAI analysis\n",
        "    if prosus_finbert_loaded:\n",
        "        print(\"\\nðŸ¤– Running enhanced ProsusAI FinBERT analysis...\")\n",
        "        try:\n",
        "            prosus_results = prosus_finbert_enhanced.predict_sentiment_enhanced(texts)\n",
        "\n",
        "            # Add enhanced results to dataframe\n",
        "            for i, result in enumerate(prosus_results):\n",
        "                results_df.loc[i, 'prosus_label'] = result['predicted_label']\n",
        "                results_df.loc[i, 'prosus_score'] = result['predicted_score']\n",
        "                results_df.loc[i, 'prosus_calibrated'] = result['calibrated_score']\n",
        "                results_df.loc[i, 'prosus_entropy'] = result['entropy']\n",
        "                results_df.loc[i, 'prosus_max_diff'] = result['max_prob_diff']\n",
        "                results_df.loc[i, 'prosus_positive'] = result['positive_score']\n",
        "                results_df.loc[i, 'prosus_neutral'] = result['neutral_score']\n",
        "                results_df.loc[i, 'prosus_negative'] = result['negative_score']\n",
        "\n",
        "                # Add text features\n",
        "                for feature_name, feature_value in result.items():\n",
        "                    if feature_name.endswith('_count') or feature_name.startswith('has_') or feature_name.endswith('_score'):\n",
        "                        if feature_name not in ['positive_score', 'neutral_score', 'negative_score', 'predicted_score', 'calibrated_score']:\n",
        "                            results_df.loc[i, f'prosus_{feature_name}'] = feature_value\n",
        "\n",
        "            print(\"Enhanced ProsusAI FinBERT analysis complete\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in enhanced ProsusAI FinBERT analysis: {str(e)}\")\n",
        "\n",
        "    # Run ensemble analysis\n",
        "    print(\"\\nRunning ensemble analysis...\")\n",
        "    try:\n",
        "        ensemble_results = ensemble.predict_ensemble(texts)\n",
        "\n",
        "        # Add ensemble results\n",
        "        for i, result in enumerate(ensemble_results):\n",
        "            results_df.loc[i, 'ensemble_label'] = result['ensemble_label']\n",
        "            results_df.loc[i, 'ensemble_confidence'] = result['ensemble_confidence']\n",
        "\n",
        "            # Add individual model votes\n",
        "            for model_name, prediction in result['individual_predictions'].items():\n",
        "                results_df.loc[i, f'vote_{model_name}'] = prediction\n",
        "\n",
        "        print(\"Ensemble analysis complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in ensemble analysis: {str(e)}\")\n",
        "\n",
        "    print(f\"Final enhanced shape: {results_df.shape}\")\n",
        "    return results_df\n",
        "\n",
        "# Run enhanced sentiment analysis on all datasets\n",
        "enhanced_jpm_q1_2025_df = run_enhanced_sentiment_analysis(processed_jpm_q1_2025_df, \"Q1 2025\")\n",
        "enhanced_jpm_q2_2025_df = run_enhanced_sentiment_analysis(processed_jpm_q2_2025_df, \"Q2 2025\")\n",
        "enhanced_jpm_multi_2025_df = run_enhanced_sentiment_analysis(processed_jpm_multi_2025_df, \"Multi 2025\")\n"
      ],
      "metadata": {
        "id": "V5h7BjOdFp3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2522760f-916b-4551-8240-71c525de1f74"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¬ ENHANCED SENTIMENT ANALYSIS - Q1 2025\n",
            "--------------------------------------------------\n",
            "Input shape: (578, 9)\n",
            "Processing 578 text entries\n",
            "\n",
            "ðŸ¤– Running enhanced FinBERT-tone analysis...\n",
            "Processing 578 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [02:01<00:00,  3.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced FinBERT-tone analysis complete\n",
            "\n",
            "ðŸ¤– Running enhanced ProsusAI FinBERT analysis...\n",
            "Processing 578 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [01:38<00:00,  2.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced ProsusAI FinBERT analysis complete\n",
            "\n",
            "Running ensemble analysis...\n",
            "Generating ensemble predictions for 578 texts...\n",
            "Processing 578 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [01:29<00:00,  2.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got predictions from finbert_tone\n",
            "Processing 578 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [01:26<00:00,  2.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got predictions from prosus_finbert\n",
            "Ensemble analysis complete\n",
            "Final enhanced shape: (578, 47)\n",
            "\n",
            "ðŸ”¬ ENHANCED SENTIMENT ANALYSIS - Q2 2025\n",
            "--------------------------------------------------\n",
            "Input shape: (532, 9)\n",
            "Processing 532 text entries\n",
            "\n",
            "ðŸ¤– Running enhanced FinBERT-tone analysis...\n",
            "Processing 532 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [01:24<00:00,  2.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced FinBERT-tone analysis complete\n",
            "\n",
            "ðŸ¤– Running enhanced ProsusAI FinBERT analysis...\n",
            "Processing 532 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [01:21<00:00,  2.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced ProsusAI FinBERT analysis complete\n",
            "\n",
            "Running ensemble analysis...\n",
            "Generating ensemble predictions for 532 texts...\n",
            "Processing 532 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [01:23<00:00,  2.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got predictions from finbert_tone\n",
            "Processing 532 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [01:19<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got predictions from prosus_finbert\n",
            "Ensemble analysis complete\n",
            "Final enhanced shape: (532, 47)\n",
            "\n",
            "ðŸ”¬ ENHANCED SENTIMENT ANALYSIS - Multi 2025\n",
            "--------------------------------------------------\n",
            "Input shape: (1110, 9)\n",
            "Processing 1110 text entries\n",
            "\n",
            "ðŸ¤– Running enhanced FinBERT-tone analysis...\n",
            "Processing 1110 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [02:53<00:00,  2.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced FinBERT-tone analysis complete\n",
            "\n",
            "ðŸ¤– Running enhanced ProsusAI FinBERT analysis...\n",
            "Processing 1110 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [02:46<00:00,  2.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced ProsusAI FinBERT analysis complete\n",
            "\n",
            "Running ensemble analysis...\n",
            "Generating ensemble predictions for 1110 texts...\n",
            "Processing 1110 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [02:51<00:00,  2.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got predictions from finbert_tone\n",
            "Processing 1110 texts with enhanced features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Enhanced Sentiment Analysis: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [02:47<00:00,  2.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got predictions from prosus_finbert\n",
            "Ensemble analysis complete\n",
            "Final enhanced shape: (1110, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Performance Evaluation with Manual Labels\n",
        "\n",
        "def evaluate_models_with_manual_labels(df: pd.DataFrame, dataset_name: str):\n",
        "    \"\"\"Evaluate model performance using manual labels.\"\"\"\n",
        "    if df is None or manual_labels_df is None:\n",
        "        print(f\"Cannot evaluate {dataset_name} - missing data\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"\\nPerformance Evaluation - {dataset_name}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Merge with manual labels\n",
        "    manual_mask = manual_labels_df['human_label'].notna() & (manual_labels_df['human_label'] != '')\n",
        "    manual_eval_df = manual_labels_df[manual_mask].copy()\n",
        "\n",
        "    # Find overlapping records (by text or sentence_id)\n",
        "    if 'sentence_id' in df.columns and 'sentence_id' in manual_eval_df.columns:\n",
        "        eval_df = df.merge(manual_eval_df[['sentence_id', 'human_label', 'human_confidence']],\n",
        "                          on='sentence_id', how='inner')\n",
        "    else:\n",
        "        # Fallback to text matching\n",
        "        eval_df = df.merge(manual_eval_df[['text', 'human_label', 'human_confidence']],\n",
        "                          on='text', how='inner')\n",
        "\n",
        "    if len(eval_df) == 0:\n",
        "        print(\"No overlapping records found for evaluation\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Evaluating on {len(eval_df)} manually labeled records\")\n",
        "\n",
        "    evaluation_results = {}\n",
        "\n",
        "    # Evaluate FinBERT-tone\n",
        "    if 'finbert_tone_label' in eval_df.columns:\n",
        "        finbert_eval = evaluator.evaluate_model_performance(\n",
        "            predictions=eval_df['finbert_tone_label'].tolist(),\n",
        "            true_labels=eval_df['human_label'].tolist(),\n",
        "            confidences=eval_df['finbert_tone_calibrated'].tolist() if 'finbert_tone_calibrated' in eval_df.columns else None,\n",
        "            model_name='Enhanced FinBERT-tone'\n",
        "        )\n",
        "        evaluation_results['finbert_tone'] = finbert_eval\n",
        "\n",
        "    # Evaluate ProsusAI\n",
        "    if 'prosus_label' in eval_df.columns:\n",
        "        prosus_eval = evaluator.evaluate_model_performance(\n",
        "            predictions=eval_df['prosus_label'].tolist(),\n",
        "            true_labels=eval_df['human_label'].tolist(),\n",
        "            confidences=eval_df['prosus_calibrated'].tolist() if 'prosus_calibrated' in eval_df.columns else None,\n",
        "            model_name='Enhanced ProsusAI'\n",
        "        )\n",
        "        evaluation_results['prosus'] = prosus_eval\n",
        "\n",
        "    # Evaluate Ensemble\n",
        "    if 'ensemble_label' in eval_df.columns:\n",
        "        ensemble_eval = evaluator.evaluate_model_performance(\n",
        "            predictions=eval_df['ensemble_label'].tolist(),\n",
        "            true_labels=eval_df['human_label'].tolist(),\n",
        "            confidences=eval_df['ensemble_confidence'].tolist() if 'ensemble_confidence' in eval_df.columns else None,\n",
        "            model_name='Ensemble Model'\n",
        "        )\n",
        "        evaluation_results['ensemble'] = ensemble_eval\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "# Evaluate models if manual labels available\n",
        "performance_results = {}\n",
        "if manual_labels_df is not None:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"MODEL PERFORMANCE EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for dataset_name, df in [(\"Q1 2025\", enhanced_jpm_q1_2025_df),\n",
        "                             (\"Q2 2025\", enhanced_jpm_q2_2025_df),\n",
        "                             (\"Multi 2025\", enhanced_jpm_multi_2025_df)]:\n",
        "        if df is not None:\n",
        "            eval_results = evaluate_models_with_manual_labels(df, dataset_name)\n",
        "            performance_results[dataset_name] = eval_results\n",
        "\n",
        "    # Compare all models\n",
        "    model_comparison = evaluator.compare_models()"
      ],
      "metadata": {
        "id": "AlXZ6h-xFt35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8399f7-92ee-43b7-e3ab-8db8a6e5dd97"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL PERFORMANCE EVALUATION\n",
            "============================================================\n",
            "\n",
            "Performance Evaluation - Q1 2025\n",
            "----------------------------------------\n",
            "Evaluating on 200 manually labeled records\n",
            "Evaluating Enhanced FinBERT-tone performance...\n",
            "  Accuracy: 0.895\n",
            "  Weighted F1: 0.894\n",
            "  Macro F1: 0.662\n",
            "  Avg Confidence: 0.895\n",
            "  Calibration Error: 0.022\n",
            "Evaluating Enhanced ProsusAI performance...\n",
            "  Accuracy: 0.855\n",
            "  Weighted F1: 0.853\n",
            "  Macro F1: 0.618\n",
            "  Avg Confidence: 0.855\n",
            "  Calibration Error: 0.014\n",
            "Evaluating Ensemble Model performance...\n",
            "  Accuracy: 0.920\n",
            "  Weighted F1: 0.916\n",
            "  Macro F1: 0.676\n",
            "  Avg Confidence: 0.850\n",
            "  Calibration Error: 0.071\n",
            "\n",
            "Performance Evaluation - Q2 2025\n",
            "----------------------------------------\n",
            "Evaluating on 45 manually labeled records\n",
            "Evaluating Enhanced FinBERT-tone performance...\n",
            "  Accuracy: 0.578\n",
            "  Weighted F1: 0.589\n",
            "  Macro F1: 0.248\n",
            "  Avg Confidence: 0.895\n",
            "  Calibration Error: 0.327\n",
            "Evaluating Enhanced ProsusAI performance...\n",
            "  Accuracy: 0.689\n",
            "  Weighted F1: 0.696\n",
            "  Macro F1: 0.345\n",
            "  Avg Confidence: 0.867\n",
            "  Calibration Error: 0.189\n",
            "Evaluating Ensemble Model performance...\n",
            "  Accuracy: 0.644\n",
            "  Weighted F1: 0.650\n",
            "  Macro F1: 0.331\n",
            "  Avg Confidence: 0.860\n",
            "  Calibration Error: 0.096\n",
            "\n",
            "Performance Evaluation - Multi 2025\n",
            "----------------------------------------\n",
            "Evaluating on 200 manually labeled records\n",
            "Evaluating Enhanced FinBERT-tone performance...\n",
            "  Accuracy: 0.895\n",
            "  Weighted F1: 0.894\n",
            "  Macro F1: 0.662\n",
            "  Avg Confidence: 0.895\n",
            "  Calibration Error: 0.022\n",
            "Evaluating Enhanced ProsusAI performance...\n",
            "  Accuracy: 0.855\n",
            "  Weighted F1: 0.853\n",
            "  Macro F1: 0.618\n",
            "  Avg Confidence: 0.855\n",
            "  Calibration Error: 0.014\n",
            "Evaluating Ensemble Model performance...\n",
            "  Accuracy: 0.920\n",
            "  Weighted F1: 0.916\n",
            "  Macro F1: 0.676\n",
            "  Avg Confidence: 0.850\n",
            "  Calibration Error: 0.071\n",
            "\n",
            "Model Comparison Summary:\n",
            "==================================================\n",
            "Enhanced FinBERT-tone:\n",
            "  F1: 0.894\n",
            "  Precision: 0.897\n",
            "  Recall: 0.895\n",
            "  Accuracy: 0.895\n",
            "Enhanced ProsusAI:\n",
            "  F1: 0.853\n",
            "  Precision: 0.855\n",
            "  Recall: 0.855\n",
            "  Accuracy: 0.855\n",
            "Ensemble Model:\n",
            "  F1: 0.916\n",
            "  Precision: 0.918\n",
            "  Recall: 0.920\n",
            "  Accuracy: 0.920\n",
            "\n",
            "Best performing model: Ensemble Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Continue with Enhanced Multi-level Analysis\n",
        "\n",
        "def enhanced_aggregate_sentence_to_qa_level(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced aggregation with additional metrics.\"\"\"\n",
        "    if df is None or 'original_qa_id' not in df.columns:\n",
        "        print(\"Cannot aggregate - missing original_qa_id column\")\n",
        "        return None\n",
        "\n",
        "    print(\"Enhanced aggregation to Q&A level...\")\n",
        "\n",
        "    # Enhanced aggregation functions\n",
        "    agg_functions = {\n",
        "        # Text info\n",
        "        'text': lambda x: ' '.join(x),\n",
        "        'speaker': 'first',\n",
        "        'speaker_role': 'first',\n",
        "        'quarter': 'first',\n",
        "        'bank_code': 'first',\n",
        "\n",
        "        # Sentence counts and stats\n",
        "        'sentence_length': ['count', 'mean', 'sum', 'std'],\n",
        "        'sentence_word_count': ['mean', 'sum', 'std'],\n",
        "\n",
        "        # Enhanced FinBERT-tone metrics\n",
        "        'finbert_tone_score': ['mean', 'std', 'min', 'max'],\n",
        "        'finbert_tone_calibrated': ['mean', 'std'],\n",
        "        'finbert_tone_entropy': ['mean', 'std'],\n",
        "        'finbert_tone_positive': 'mean',\n",
        "        'finbert_tone_neutral': 'mean',\n",
        "        'finbert_tone_negative': 'mean',\n",
        "\n",
        "        # Enhanced ProsusAI metrics\n",
        "        'prosus_score': ['mean', 'std', 'min', 'max'],\n",
        "        'prosus_calibrated': ['mean', 'std'],\n",
        "        'prosus_entropy': ['mean', 'std'],\n",
        "        'prosus_positive': 'mean',\n",
        "        'prosus_neutral': 'mean',\n",
        "        'prosus_negative': 'mean',\n",
        "\n",
        "        # Ensemble metrics\n",
        "        'ensemble_confidence': ['mean', 'std']\n",
        "    }\n",
        "\n",
        "    # Apply aggregations\n",
        "    qa_level_df = df.groupby('original_qa_id').agg(agg_functions).reset_index()\n",
        "\n",
        "    # Flatten column names\n",
        "    qa_level_df.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col\n",
        "                           for col in qa_level_df.columns]\n",
        "\n",
        "    # Enhanced label determination using ensemble and confidence\n",
        "    for model_prefix in ['finbert_tone', 'prosus', 'ensemble']:\n",
        "        label_col = f'{model_prefix}_label'\n",
        "        if label_col in df.columns:\n",
        "            # Confidence-weighted majority vote\n",
        "            def confidence_weighted_vote(group):\n",
        "                if f'{model_prefix}_calibrated' in df.columns:\n",
        "                    # Weight votes by calibrated confidence\n",
        "                    labels = group[label_col]\n",
        "                    weights = group[f'{model_prefix}_calibrated'].fillna(0.5)\n",
        "\n",
        "                    # Calculate weighted votes\n",
        "                    weighted_votes = {}\n",
        "                    for label, weight in zip(labels, weights):\n",
        "                        if label not in weighted_votes:\n",
        "                            weighted_votes[label] = 0\n",
        "                        weighted_votes[label] += weight\n",
        "\n",
        "                    return max(weighted_votes, key=weighted_votes.get)\n",
        "                else:\n",
        "                    # Simple majority vote\n",
        "                    return group[label_col].mode().iloc[0] if len(group[label_col].mode()) > 0 else 'neutral'\n",
        "\n",
        "            qa_labels = df.groupby('original_qa_id').apply(confidence_weighted_vote).reset_index()\n",
        "            qa_labels.columns = ['original_qa_id', f'{model_prefix}_qa_label']\n",
        "            qa_level_df = qa_level_df.merge(qa_labels, on='original_qa_id')\n",
        "\n",
        "    print(f\"Enhanced Q&A level aggregation complete: {qa_level_df.shape}\")\n",
        "    return qa_level_df\n",
        "\n",
        "def enhanced_aggregate_by_speaker_role(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced speaker-level aggregation.\"\"\"\n",
        "    if df is None or 'speaker_role' not in df.columns:\n",
        "        print(\"Cannot aggregate by speaker - missing speaker_role column\")\n",
        "        return None\n",
        "\n",
        "    print(\"Enhanced speaker role aggregation...\")\n",
        "\n",
        "    # Enhanced speaker aggregation\n",
        "    speaker_agg = {\n",
        "        'text': 'count',\n",
        "        'sentence_length': ['mean', 'std'],\n",
        "        'sentence_word_count': ['mean', 'std'],\n",
        "\n",
        "        # Enhanced metrics for both models\n",
        "        'finbert_tone_score': ['mean', 'std'],\n",
        "        'finbert_tone_calibrated': ['mean', 'std'],\n",
        "        'finbert_tone_entropy': ['mean', 'std'],\n",
        "        'finbert_tone_positive': 'mean',\n",
        "        'finbert_tone_neutral': 'mean',\n",
        "        'finbert_tone_negative': 'mean',\n",
        "\n",
        "        'prosus_score': ['mean', 'std'],\n",
        "        'prosus_calibrated': ['mean', 'std'],\n",
        "        'prosus_entropy': ['mean', 'std'],\n",
        "        'prosus_positive': 'mean',\n",
        "        'prosus_neutral': 'mean',\n",
        "        'prosus_negative': 'mean',\n",
        "\n",
        "        'ensemble_confidence': ['mean', 'std']\n",
        "    }\n",
        "\n",
        "    # Group by quarter and speaker role if quarter available\n",
        "    if 'quarter' in df.columns:\n",
        "        speaker_df = df.groupby(['quarter', 'speaker_role']).agg(speaker_agg).reset_index()\n",
        "    else:\n",
        "        speaker_df = df.groupby('speaker_role').agg(speaker_agg).reset_index()\n",
        "\n",
        "    # Flatten column names\n",
        "    speaker_df.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col\n",
        "                          for col in speaker_df.columns]\n",
        "\n",
        "    # Add enhanced label distributions\n",
        "    label_columns = [col for col in df.columns if col.endswith('_label')]\n",
        "\n",
        "    for label_col in label_columns:\n",
        "        model_name = label_col.replace('_label', '')\n",
        "\n",
        "        if 'quarter' in df.columns:\n",
        "            label_dist = df.groupby(['quarter', 'speaker_role'])[label_col].value_counts(normalize=True).unstack(fill_value=0)\n",
        "        else:\n",
        "            label_dist = df.groupby('speaker_role')[label_col].value_counts(normalize=True).unstack(fill_value=0)\n",
        "\n",
        "        # Add distribution columns\n",
        "        label_dist.columns = [f'{model_name}_{col}_pct' for col in label_dist.columns]\n",
        "\n",
        "        # Merge distributions\n",
        "        if 'quarter' in df.columns:\n",
        "            speaker_df = speaker_df.set_index(['quarter', 'speaker_role']).join(label_dist).reset_index()\n",
        "        else:\n",
        "            speaker_df = speaker_df.set_index('speaker_role').join(label_dist).reset_index()\n",
        "\n",
        "    print(f\"Enhanced speaker-level aggregation complete: {speaker_df.shape}\")\n",
        "    return speaker_df\n",
        "\n",
        "# Create enhanced multi-level aggregations\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ENHANCED MULTI-LEVEL AGGREGATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Enhanced Q&A level aggregations\n",
        "enhanced_qa_level_q1_df = enhanced_aggregate_sentence_to_qa_level(enhanced_jpm_q1_2025_df)\n",
        "enhanced_qa_level_q2_df = enhanced_aggregate_sentence_to_qa_level(enhanced_jpm_q2_2025_df)\n",
        "enhanced_qa_level_multi_df = enhanced_aggregate_sentence_to_qa_level(enhanced_jpm_multi_2025_df)\n",
        "\n",
        "# Enhanced speaker level aggregations\n",
        "enhanced_speaker_level_q1_df = enhanced_aggregate_by_speaker_role(enhanced_jpm_q1_2025_df)\n",
        "enhanced_speaker_level_q2_df = enhanced_aggregate_by_speaker_role(enhanced_jpm_q2_2025_df)\n",
        "enhanced_speaker_level_multi_df = enhanced_aggregate_by_speaker_role(enhanced_jpm_multi_2025_df)\n"
      ],
      "metadata": {
        "id": "PXlgz10pFyYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d09889-f89b-4d3d-a52e-7cd3eafc23d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENHANCED MULTI-LEVEL AGGREGATIONS\n",
            "============================================================\n",
            "Enhanced aggregation to Q&A level...\n",
            "Enhanced Q&A level aggregation complete: (97, 40)\n",
            "Enhanced aggregation to Q&A level...\n",
            "Enhanced Q&A level aggregation complete: (121, 40)\n",
            "Enhanced aggregation to Q&A level...\n",
            "Enhanced Q&A level aggregation complete: (218, 40)\n",
            "Enhanced speaker role aggregation...\n",
            "Enhanced speaker-level aggregation complete: (3, 36)\n",
            "Enhanced speaker role aggregation...\n",
            "Enhanced speaker-level aggregation complete: (3, 36)\n",
            "Enhanced speaker role aggregation...\n",
            "Enhanced speaker-level aggregation complete: (6, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Topic-Conditional Sentiment Analysis\n",
        "\n",
        "def enhanced_extract_financial_topics(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced financial topic extraction with more sophisticated categorization.\"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    print(\"Enhanced financial topic extraction...\")\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Enhanced topic keywords with financial domain expertise\n",
        "    enhanced_topic_keywords = {\n",
        "        'revenue_growth': ['revenue growth', 'sales growth', 'income increase', 'earnings growth', 'top line growth'],\n",
        "        'profitability': ['profit margin', 'profitability', 'ebitda', 'roi', 'return on equity', 'net income'],\n",
        "        'credit_risk': ['credit risk', 'default rate', 'loan loss', 'provision', 'non-performing', 'charge-off'],\n",
        "        'operational_risk': ['operational risk', 'compliance', 'regulatory', 'operational efficiency'],\n",
        "        'market_risk': ['market risk', 'interest rate', 'trading', 'volatility', 'market conditions'],\n",
        "        'capital_management': ['capital ratio', 'tier 1', 'leverage', 'capital adequacy', 'basel'],\n",
        "        'digital_transformation': ['digital', 'technology', 'fintech', 'automation', 'innovation'],\n",
        "        'customer_experience': ['customer satisfaction', 'client experience', 'customer acquisition', 'retention'],\n",
        "        'regulatory_environment': ['regulation', 'regulatory change', 'compliance cost', 'regulatory capital'],\n",
        "        'economic_outlook': ['economic environment', 'macro environment', 'economic outlook', 'recession']\n",
        "    }\n",
        "\n",
        "    # Identify topics using enhanced matching\n",
        "    for topic, keywords in enhanced_topic_keywords.items():\n",
        "        pattern = '|'.join([re.escape(kw) for kw in keywords])\n",
        "        df[f'topic_{topic}'] = df['text'].str.lower().str.contains(pattern, regex=True, na=False)\n",
        "\n",
        "    # Determine primary topic with confidence scoring\n",
        "    topic_columns = [f'topic_{topic}' for topic in enhanced_topic_keywords.keys()]\n",
        "    topic_scores = df[topic_columns].sum(axis=1)\n",
        "\n",
        "    # Primary topic is the one with most keyword matches\n",
        "    df['primary_topic'] = df[topic_columns].idxmax(axis=1).str.replace('topic_', '')\n",
        "    df['topic_confidence'] = topic_scores / len(enhanced_topic_keywords)\n",
        "\n",
        "    # If no clear topic, mark as 'general'\n",
        "    no_topics_mask = topic_scores == 0\n",
        "    df.loc[no_topics_mask, 'primary_topic'] = 'general'\n",
        "    df.loc[no_topics_mask, 'topic_confidence'] = 0.0\n",
        "\n",
        "    print(f\"Enhanced topic extraction complete. Topic distribution:\")\n",
        "    topic_dist = df['primary_topic'].value_counts()\n",
        "    for topic, count in topic_dist.items():\n",
        "        print(f\"  {topic}: {count}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def enhanced_analyze_sentiment_by_topic(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced topic-sentiment analysis with confidence weighting.\"\"\"\n",
        "    if df is None or 'primary_topic' not in df.columns:\n",
        "        return None\n",
        "\n",
        "    print(\"Enhanced sentiment analysis by topic...\")\n",
        "\n",
        "    # Enhanced topic aggregation with confidence weighting\n",
        "    topic_agg = {\n",
        "        'text': 'count',\n",
        "        'topic_confidence': 'mean',\n",
        "\n",
        "        # Enhanced metrics for all models\n",
        "        'finbert_tone_score': ['mean', 'std'],\n",
        "        'finbert_tone_calibrated': ['mean', 'std'],\n",
        "        'finbert_tone_entropy': 'mean',\n",
        "        'finbert_tone_positive': 'mean',\n",
        "        'finbert_tone_neutral': 'mean',\n",
        "        'finbert_tone_negative': 'mean',\n",
        "\n",
        "        'prosus_score': ['mean', 'std'],\n",
        "        'prosus_calibrated': ['mean', 'std'],\n",
        "        'prosus_entropy': 'mean',\n",
        "        'prosus_positive': 'mean',\n",
        "        'prosus_neutral': 'mean',\n",
        "        'prosus_negative': 'mean',\n",
        "\n",
        "        'ensemble_confidence': ['mean', 'std']\n",
        "    }\n",
        "\n",
        "    # Group by topic and optionally quarter\n",
        "    if 'quarter' in df.columns:\n",
        "        topic_sentiment_df = df.groupby(['quarter', 'primary_topic']).agg(topic_agg).reset_index()\n",
        "    else:\n",
        "        topic_sentiment_df = df.groupby('primary_topic').agg(topic_agg).reset_index()\n",
        "\n",
        "    # Flatten column names\n",
        "    topic_sentiment_df.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col\n",
        "                                  for col in topic_sentiment_df.columns]\n",
        "\n",
        "    print(f\"Enhanced topic-sentiment analysis complete: {topic_sentiment_df.shape}\")\n",
        "    return topic_sentiment_df\n",
        "\n",
        "# Apply enhanced topic analysis\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ENHANCED TOPIC-CONDITIONAL SENTIMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "enhanced_topics_q1_df = enhanced_extract_financial_topics(enhanced_jpm_q1_2025_df)\n",
        "enhanced_topics_q2_df = enhanced_extract_financial_topics(enhanced_jpm_q2_2025_df)\n",
        "enhanced_topics_multi_df = enhanced_extract_financial_topics(enhanced_jpm_multi_2025_df)\n",
        "\n",
        "# Enhanced topic-sentiment analysis\n",
        "enhanced_topic_sentiment_q1_df = enhanced_analyze_sentiment_by_topic(enhanced_topics_q1_df)\n",
        "enhanced_topic_sentiment_q2_df = enhanced_analyze_sentiment_by_topic(enhanced_topics_q2_df)\n",
        "enhanced_topic_sentiment_multi_df = enhanced_analyze_sentiment_by_topic(enhanced_topics_multi_df)\n"
      ],
      "metadata": {
        "id": "KVyXPh1n_CVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae0848b-b7aa-457f-b4be-5c63186e6f83"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ENHANCED TOPIC-CONDITIONAL SENTIMENT\n",
            "==================================================\n",
            "Enhanced financial topic extraction...\n",
            "Enhanced topic extraction complete. Topic distribution:\n",
            "  general: 505\n",
            "  market_risk: 24\n",
            "  economic_outlook: 13\n",
            "  regulatory_environment: 9\n",
            "  credit_risk: 7\n",
            "  profitability: 6\n",
            "  capital_management: 5\n",
            "  operational_risk: 5\n",
            "  digital_transformation: 4\n",
            "Enhanced financial topic extraction...\n",
            "Enhanced topic extraction complete. Topic distribution:\n",
            "  general: 494\n",
            "  market_risk: 9\n",
            "  digital_transformation: 7\n",
            "  capital_management: 5\n",
            "  operational_risk: 5\n",
            "  regulatory_environment: 4\n",
            "  credit_risk: 3\n",
            "  profitability: 2\n",
            "  revenue_growth: 2\n",
            "  economic_outlook: 1\n",
            "Enhanced financial topic extraction...\n",
            "Enhanced topic extraction complete. Topic distribution:\n",
            "  general: 999\n",
            "  market_risk: 33\n",
            "  economic_outlook: 14\n",
            "  regulatory_environment: 13\n",
            "  digital_transformation: 11\n",
            "  capital_management: 10\n",
            "  operational_risk: 10\n",
            "  credit_risk: 10\n",
            "  profitability: 8\n",
            "  revenue_growth: 2\n",
            "Enhanced sentiment analysis by topic...\n",
            "Enhanced topic-sentiment analysis complete: (9, 22)\n",
            "Enhanced sentiment analysis by topic...\n",
            "Enhanced topic-sentiment analysis complete: (10, 22)\n",
            "Enhanced sentiment analysis by topic...\n",
            "Enhanced topic-sentiment analysis complete: (19, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Anomaly Detection\n",
        "\n",
        "def enhanced_detect_sentiment_anomalies(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Enhanced anomaly detection with multiple methods.\"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    print(\"Enhanced sentiment anomaly detection...\")\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Enhanced anomaly detection methods\n",
        "\n",
        "    # 1. Statistical anomalies (Z-scores)\n",
        "    numeric_columns = ['finbert_tone_score', 'prosus_score', 'ensemble_confidence',\n",
        "                      'finbert_tone_entropy', 'prosus_entropy']\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            z_scores = np.abs(stats.zscore(df[col].fillna(df[col].mean())))\n",
        "            df[f'{col}_zscore'] = z_scores\n",
        "            df[f'{col}_anomaly'] = z_scores > 2.5\n",
        "\n",
        "    # 2. Model disagreement anomalies\n",
        "    if 'finbert_tone_label' in df.columns and 'prosus_label' in df.columns:\n",
        "        df['model_disagreement'] = df['finbert_tone_label'] != df['prosus_label']\n",
        "\n",
        "        # Severe disagreement (high confidence but different predictions)\n",
        "        if 'finbert_tone_calibrated' in df.columns and 'prosus_calibrated' in df.columns:\n",
        "            high_confidence_mask = (df['finbert_tone_calibrated'] > 0.8) & (df['prosus_calibrated'] > 0.8)\n",
        "            df['severe_disagreement'] = df['model_disagreement'] & high_confidence_mask\n",
        "\n",
        "    # 3. Ensemble vs individual model anomalies\n",
        "    if 'ensemble_label' in df.columns:\n",
        "        for model_col in ['finbert_tone_label', 'prosus_label']:\n",
        "            if model_col in df.columns:\n",
        "                df[f'ensemble_{model_col}_disagreement'] = df['ensemble_label'] != df[model_col]\n",
        "\n",
        "    # 4. Confidence calibration anomalies\n",
        "    for model in ['finbert_tone', 'prosus']:\n",
        "        score_col = f'{model}_score'\n",
        "        calibrated_col = f'{model}_calibrated'\n",
        "        if score_col in df.columns and calibrated_col in df.columns:\n",
        "            confidence_diff = np.abs(df[score_col] - df[calibrated_col])\n",
        "            df[f'{model}_calibration_diff'] = confidence_diff\n",
        "            df[f'{model}_calibration_anomaly'] = confidence_diff > 0.3\n",
        "\n",
        "    # 5. Text-based anomalies\n",
        "    if 'sentence_length' in df.columns:\n",
        "        length_z = np.abs(stats.zscore(df['sentence_length']))\n",
        "        df['length_anomaly'] = length_z > 3\n",
        "\n",
        "    # 6. Topic-sentiment mismatch anomalies\n",
        "    if 'primary_topic' in df.columns:\n",
        "        # Define expected sentiment patterns for topics\n",
        "        topic_sentiment_expectations = {\n",
        "            'revenue_growth': 'positive',\n",
        "            'profitability': 'positive',\n",
        "            'credit_risk': 'negative',\n",
        "            'operational_risk': 'negative',\n",
        "            'market_risk': 'negative'\n",
        "        }\n",
        "\n",
        "        df['topic_sentiment_mismatch'] = False\n",
        "        for topic, expected_sentiment in topic_sentiment_expectations.items():\n",
        "            topic_mask = df['primary_topic'] == topic\n",
        "            if 'ensemble_label' in df.columns:\n",
        "                mismatch_mask = topic_mask & (df['ensemble_label'] != expected_sentiment)\n",
        "                df.loc[mismatch_mask, 'topic_sentiment_mismatch'] = True\n",
        "\n",
        "    # Count total anomalies\n",
        "    anomaly_columns = [col for col in df.columns if 'anomaly' in col or 'disagreement' in col or 'mismatch' in col]\n",
        "    if anomaly_columns:\n",
        "        df['total_anomaly_flags'] = df[anomaly_columns].sum(axis=1)\n",
        "        anomaly_count = df['total_anomaly_flags'].sum()\n",
        "        print(f\"  Detected {anomaly_count} total anomaly flags across {len(df)} records\")\n",
        "\n",
        "        # Enhanced anomaly reporting\n",
        "        for col in anomaly_columns:\n",
        "            count = df[col].sum()\n",
        "            pct = (count / len(df)) * 100\n",
        "            print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply enhanced anomaly detection\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ENHANCED ANOMALY DETECTION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "enhanced_anomaly_q1_df = enhanced_detect_sentiment_anomalies(enhanced_topics_q1_df)\n",
        "enhanced_anomaly_q2_df = enhanced_detect_sentiment_anomalies(enhanced_topics_q2_df)\n",
        "enhanced_anomaly_multi_df = enhanced_detect_sentiment_anomalies(enhanced_topics_multi_df)\n"
      ],
      "metadata": {
        "id": "9Y-j-4FZ_KcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a0ab41-a89c-42a0-9fb6-ae3650c476f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ENHANCED ANOMALY DETECTION\n",
            "==================================================\n",
            "Enhanced sentiment anomaly detection...\n",
            "  Detected 491 total anomaly flags across 578 records\n",
            "  finbert_tone_score_anomaly: 30 (5.2%)\n",
            "  prosus_score_anomaly: 17 (2.9%)\n",
            "  ensemble_confidence_anomaly: 3 (0.5%)\n",
            "  finbert_tone_entropy_anomaly: 9 (1.6%)\n",
            "  prosus_entropy_anomaly: 8 (1.4%)\n",
            "  model_disagreement: 146 (25.3%)\n",
            "  severe_disagreement: 73 (12.6%)\n",
            "  ensemble_finbert_tone_label_disagreement: 78 (13.5%)\n",
            "  ensemble_prosus_label_disagreement: 68 (11.8%)\n",
            "  finbert_tone_calibration_anomaly: 15 (2.6%)\n",
            "  prosus_calibration_anomaly: 1 (0.2%)\n",
            "  length_anomaly: 9 (1.6%)\n",
            "  topic_sentiment_mismatch: 34 (5.9%)\n",
            "Enhanced sentiment anomaly detection...\n",
            "  Detected 422 total anomaly flags across 532 records\n",
            "  finbert_tone_score_anomaly: 28 (5.3%)\n",
            "  prosus_score_anomaly: 12 (2.3%)\n",
            "  ensemble_confidence_anomaly: 2 (0.4%)\n",
            "  finbert_tone_entropy_anomaly: 26 (4.9%)\n",
            "  prosus_entropy_anomaly: 7 (1.3%)\n",
            "  model_disagreement: 119 (22.4%)\n",
            "  severe_disagreement: 66 (12.4%)\n",
            "  ensemble_finbert_tone_label_disagreement: 60 (11.3%)\n",
            "  ensemble_prosus_label_disagreement: 59 (11.1%)\n",
            "  finbert_tone_calibration_anomaly: 16 (3.0%)\n",
            "  prosus_calibration_anomaly: 0 (0.0%)\n",
            "  length_anomaly: 9 (1.7%)\n",
            "  topic_sentiment_mismatch: 18 (3.4%)\n",
            "Enhanced sentiment anomaly detection...\n",
            "  Detected 927 total anomaly flags across 1110 records\n",
            "  finbert_tone_score_anomaly: 57 (5.1%)\n",
            "  prosus_score_anomaly: 30 (2.7%)\n",
            "  ensemble_confidence_anomaly: 6 (0.5%)\n",
            "  finbert_tone_entropy_anomaly: 46 (4.1%)\n",
            "  prosus_entropy_anomaly: 17 (1.5%)\n",
            "  model_disagreement: 265 (23.9%)\n",
            "  severe_disagreement: 139 (12.5%)\n",
            "  ensemble_finbert_tone_label_disagreement: 138 (12.4%)\n",
            "  ensemble_prosus_label_disagreement: 127 (11.4%)\n",
            "  finbert_tone_calibration_anomaly: 31 (2.8%)\n",
            "  prosus_calibration_anomaly: 1 (0.1%)\n",
            "  length_anomaly: 18 (1.6%)\n",
            "  topic_sentiment_mismatch: 52 (4.7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Enhanced Results\n",
        "\n",
        "def save_enhanced_sentiment_results(df: pd.DataFrame, filename: str, description: str):\n",
        "    \"\"\"Save enhanced sentiment analysis results.\"\"\"\n",
        "    if df is None:\n",
        "        print(f\"Cannot save {description} - dataset is None\")\n",
        "        return\n",
        "\n",
        "    print(f\"Saving {description}...\")\n",
        "\n",
        "    # Save to results directory\n",
        "    results_path = results_sentiment_path / filename\n",
        "    df.to_csv(results_path, index=False)\n",
        "    print(f\"  Results: {results_path}\")\n",
        "\n",
        "    # Save to colab for easy access\n",
        "    colab_results_path = colab_base / \"results/sentiment/jpm\" / filename\n",
        "    colab_results_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(colab_results_path, index=False)\n",
        "    print(f\"  Colab: {colab_results_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING ENHANCED RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save enhanced sentence-level results\n",
        "save_enhanced_sentiment_results(enhanced_anomaly_q1_df, \"enhanced_sentiment_sentence_jpm_q1_2025.csv\", \"Enhanced Q1 sentence-level\")\n",
        "save_enhanced_sentiment_results(enhanced_anomaly_q2_df, \"enhanced_sentiment_sentence_jpm_q2_2025.csv\", \"Enhanced Q2 sentence-level\")\n",
        "save_enhanced_sentiment_results(enhanced_anomaly_multi_df, \"enhanced_sentiment_sentence_jpm_multi_2025.csv\", \"Enhanced Multi sentence-level\")\n",
        "\n",
        "# Save enhanced Q&A-level results\n",
        "save_enhanced_sentiment_results(enhanced_qa_level_q1_df, \"enhanced_sentiment_qa_jpm_q1_2025.csv\", \"Enhanced Q1 Q&A-level\")\n",
        "save_enhanced_sentiment_results(enhanced_qa_level_q2_df, \"enhanced_sentiment_qa_jpm_q2_2025.csv\", \"Enhanced Q2 Q&A-level\")\n",
        "save_enhanced_sentiment_results(enhanced_qa_level_multi_df, \"enhanced_sentiment_qa_jpm_multi_2025.csv\", \"Enhanced Multi Q&A-level\")\n",
        "\n",
        "# Save enhanced speaker-level results\n",
        "save_enhanced_sentiment_results(enhanced_speaker_level_q1_df, \"enhanced_sentiment_speaker_jpm_q1_2025.csv\", \"Enhanced Q1 speaker-level\")\n",
        "save_enhanced_sentiment_results(enhanced_speaker_level_q2_df, \"enhanced_sentiment_speaker_jmp_q2_2025.csv\", \"Enhanced Q2 speaker-level\")\n",
        "save_enhanced_sentiment_results(enhanced_speaker_level_multi_df, \"enhanced_sentiment_speaker_jpm_multi_2025.csv\", \"Enhanced Multi speaker-level\")\n",
        "\n",
        "# Save enhanced topic-sentiment results\n",
        "save_enhanced_sentiment_results(enhanced_topic_sentiment_q1_df, \"enhanced_sentiment_topic_jpm_q1_2025.csv\", \"Enhanced Q1 topic-sentiment\")\n",
        "save_enhanced_sentiment_results(enhanced_topic_sentiment_q2_df, \"enhanced_sentiment_topic_jpm_q2_2025.csv\", \"Enhanced Q2 topic-sentiment\")\n",
        "save_enhanced_sentiment_results(enhanced_topic_sentiment_multi_df, \"enhanced_sentiment_topic_jpm_multi_2025.csv\", \"Enhanced Multi topic-sentiment\")\n",
        "\n",
        "# Save performance evaluation results\n",
        "if performance_results:\n",
        "    performance_path = results_sentiment_path / \"enhanced_performance_evaluation.json\"\n",
        "    with open(performance_path, 'w') as f:\n",
        "        json.dump(performance_results, f, indent=2, default=str)\n",
        "    print(f\"Performance evaluation: {performance_path}\")\n"
      ],
      "metadata": {
        "id": "v1pkBBqV_QqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bed950-cc84-40ad-da87-b2aa03e8d7f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING ENHANCED RESULTS\n",
            "============================================================\n",
            "Saving Enhanced Q1 sentence-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_sentence_jpm_q1_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_sentence_jpm_q1_2025.csv\n",
            "Saving Enhanced Q2 sentence-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_sentence_jpm_q2_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_sentence_jpm_q2_2025.csv\n",
            "Saving Enhanced Multi sentence-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_sentence_jpm_multi_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_sentence_jpm_multi_2025.csv\n",
            "Saving Enhanced Q1 Q&A-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_qa_jpm_q1_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_qa_jpm_q1_2025.csv\n",
            "Saving Enhanced Q2 Q&A-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_qa_jpm_q2_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_qa_jpm_q2_2025.csv\n",
            "Saving Enhanced Multi Q&A-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_qa_jpm_multi_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_qa_jpm_multi_2025.csv\n",
            "Saving Enhanced Q1 speaker-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_speaker_jpm_q1_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_speaker_jpm_q1_2025.csv\n",
            "Saving Enhanced Q2 speaker-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_speaker_jmp_q2_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_speaker_jmp_q2_2025.csv\n",
            "Saving Enhanced Multi speaker-level...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_speaker_jpm_multi_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_speaker_jpm_multi_2025.csv\n",
            "Saving Enhanced Q1 topic-sentiment...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_topic_jpm_q1_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_topic_jpm_q1_2025.csv\n",
            "Saving Enhanced Q2 topic-sentiment...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_topic_jpm_q2_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_topic_jpm_q2_2025.csv\n",
            "Saving Enhanced Multi topic-sentiment...\n",
            "  Results: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_topic_jpm_multi_2025.csv\n",
            "  Colab: /content/cam_ds_ai_project/results/sentiment/jpm/enhanced_sentiment_topic_jpm_multi_2025.csv\n",
            "Performance evaluation: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_performance_evaluation.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Enhanced Summary Report\n",
        "\n",
        "def create_enhanced_summary():\n",
        "    \"\"\"Create comprehensive enhanced analysis summary.\"\"\"\n",
        "    summary = {\n",
        "        \"analysis_timestamp\": pd.Timestamp.now().isoformat(),\n",
        "        \"bank_code\": BANK_CODE,\n",
        "        \"enhanced_features\": [\n",
        "            \"confidence_calibration\",\n",
        "            \"ensemble_methods\",\n",
        "            \"enhanced_topic_extraction\",\n",
        "            \"advanced_anomaly_detection\",\n",
        "            \"performance_optimization\",\n",
        "            \"text_feature_engineering\"\n",
        "        ],\n",
        "        \"models_used\": [\n",
        "            \"Enhanced yiyanghkust/finbert-tone\",\n",
        "            \"Enhanced ProsusAI/finbert\",\n",
        "            \"Ensemble Model\",\n",
        "            \"VADER Sentiment\",\n",
        "            \"ML Classifier Backup\"\n",
        "        ],\n",
        "        \"analysis_levels\": [\n",
        "            \"enhanced_sentence_level\",\n",
        "            \"enhanced_qa_level\",\n",
        "            \"enhanced_speaker_level\",\n",
        "            \"enhanced_topic_conditional\"\n",
        "        ],\n",
        "        \"performance_improvements\": {},\n",
        "        \"datasets_analyzed\": {},\n",
        "        \"enhancement_metrics\": {}\n",
        "    }\n",
        "\n",
        "    # Add dataset information\n",
        "    enhanced_datasets_info = [\n",
        "        (\"enhanced_sentence_jpm_q1_2025\", enhanced_anomaly_q1_df),\n",
        "        (\"enhanced_sentence_jpm_q2_2025\", enhanced_anomaly_q2_df),\n",
        "        (\"enhanced_sentence_jpm_multi_2025\", enhanced_anomaly_multi_df),\n",
        "        (\"enhanced_qa_jpm_q1_2025\", enhanced_qa_level_q1_df),\n",
        "        (\"enhanced_qa_jpm_q2_2025\", enhanced_qa_level_q2_df),\n",
        "        (\"enhanced_qa_jpm_multi_2025\", enhanced_qa_level_multi_df)\n",
        "    ]\n",
        "\n",
        "    for name, df in enhanced_datasets_info:\n",
        "        if df is not None:\n",
        "            # Basic info\n",
        "            summary[\"datasets_analyzed\"][name] = {\n",
        "                \"shape\": df.shape,\n",
        "                \"memory_mb\": round(df.memory_usage(deep=True).sum() / 1024**2, 2)\n",
        "            }\n",
        "\n",
        "            # Enhanced features count\n",
        "            enhanced_columns = [col for col in df.columns if any(x in col for x in\n",
        "                              ['calibrated', 'entropy', 'ensemble', 'enhanced', 'anomaly'])]\n",
        "            summary[\"datasets_analyzed\"][name][\"enhanced_features_count\"] = len(enhanced_columns)\n",
        "\n",
        "            # Performance metrics if available\n",
        "            if name in performance_results:\n",
        "                summary[\"datasets_analyzed\"][name][\"performance_metrics\"] = performance_results[name]\n",
        "\n",
        "    # Add enhancement metrics\n",
        "    if manual_labels_df is not None:\n",
        "        manual_count = validation_report.get('manually_labeled_count', 0)\n",
        "        summary[\"enhancement_metrics\"] = {\n",
        "            \"manual_labels_used\": manual_count,\n",
        "            \"confidence_calibration_enabled\": manual_count > 10,\n",
        "            \"ensemble_models_count\": len(ensemble.models),\n",
        "            \"enhanced_topics_count\": len([col for col in enhanced_anomaly_multi_df.columns\n",
        "                                        if col.startswith('topic_') and col.endswith('_count')]) if enhanced_anomaly_multi_df is not None else 0\n",
        "        }\n",
        "\n",
        "    # Performance improvements\n",
        "    if performance_results and evaluator.evaluation_results:\n",
        "        best_model_info = evaluator.compare_models()\n",
        "        if best_model_info.get('best_model'):\n",
        "            summary[\"performance_improvements\"] = {\n",
        "                \"best_performing_model\": best_model_info['best_model'],\n",
        "                \"model_rankings\": best_model_info.get('model_rankings', {}),\n",
        "                \"performance_summary\": best_model_info.get('performance_summary', {})\n",
        "            }\n",
        "\n",
        "    # Save summary\n",
        "    summary_path = results_sentiment_path / \"enhanced_sentiment_analysis_summary.json\"\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "    return summary, summary_path\n",
        "\n",
        "enhanced_summary, enhanced_summary_path = create_enhanced_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ENHANCED SENTIMENT ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"Summary report: {enhanced_summary_path}\")\n",
        "print(\"\\nEnhanced results generated:\")\n",
        "for dataset_name, info in enhanced_summary[\"datasets_analyzed\"].items():\n",
        "    if info is not None:\n",
        "        enhanced_features = info.get('enhanced_features_count', 0)\n",
        "        print(f\"  {dataset_name}: {info['shape']} - {info['memory_mb']} MB ({enhanced_features} enhanced features)\")\n",
        "\n",
        "total_records = sum(info['shape'][0] for info in enhanced_summary[\"datasets_analyzed\"].values() if info is not None)\n",
        "total_memory = sum(info['memory_mb'] for info in enhanced_summary[\"datasets_analyzed\"].values() if info is not None)\n",
        "\n",
        "print(f\"\\nTotal enhanced records: {total_records:,}\")\n",
        "print(f\"Total memory usage: {total_memory:.2f} MB\")\n",
        "\n",
        "# Performance summary\n",
        "if enhanced_summary.get(\"performance_improvements\"):\n",
        "    best_model = enhanced_summary[\"performance_improvements\"].get(\"best_performing_model\")\n",
        "    if best_model:\n",
        "        print(f\"\\nBest performing model: {best_model}\")\n",
        "\n",
        "        # Show performance metrics for best model\n",
        "        perf_summary = enhanced_summary[\"performance_improvements\"].get(\"performance_summary\", {})\n",
        "        if best_model in perf_summary:\n",
        "            metrics = perf_summary[best_model]\n",
        "            print(f\"  F1-Score: {metrics.get('weighted_f1', 0):.3f}\")\n",
        "            print(f\"  Precision: {metrics.get('weighted_precision', 0):.3f}\")\n",
        "            print(f\"  Recall: {metrics.get('weighted_recall', 0):.3f}\")\n",
        "            print(f\"  Accuracy: {metrics.get('accuracy', 0):.3f}\")\n",
        "\n",
        "# Enhancement summary\n",
        "enhancement_metrics = enhanced_summary.get(\"enhancement_metrics\", {})\n",
        "if enhancement_metrics:\n",
        "    print(f\"\\nEnhancement Features:\")\n",
        "    print(f\"  Manual labels utilized: {enhancement_metrics.get('manual_labels_used', 0)}\")\n",
        "    print(f\"  Confidence calibration: {'Enabled' if enhancement_metrics.get('confidence_calibration_enabled') else 'Disabled'}\")\n",
        "    print(f\"  Ensemble models: {enhancement_metrics.get('ensemble_models_count', 0)}\")\n",
        "    print(f\"  Enhanced topics: {enhancement_metrics.get('enhanced_topics_count', 0)}\")\n",
        "\n",
        "print(f\"\\nKey Enhancements Applied:\")\n",
        "for feature in enhanced_summary.get(\"enhanced_features\", []):\n",
        "    print(f\"  âœ“ {feature.replace('_', ' ').title()}\")\n",
        "\n",
        "print(f\"\\nNext step: Run 04b_model_finetuning.ipynb for fine-tuning with manual labels\")\n",
        "print(f\"          Then continue to 05_model_comparison_jpm_enhanced.ipynb\")\n"
      ],
      "metadata": {
        "id": "stcjdz7W_MxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c39648-6013-492c-9c7f-628749cbceb2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Comparison Summary:\n",
            "==================================================\n",
            "Enhanced FinBERT-tone:\n",
            "  F1: 0.894\n",
            "  Precision: 0.897\n",
            "  Recall: 0.895\n",
            "  Accuracy: 0.895\n",
            "Enhanced ProsusAI:\n",
            "  F1: 0.853\n",
            "  Precision: 0.855\n",
            "  Recall: 0.855\n",
            "  Accuracy: 0.855\n",
            "Ensemble Model:\n",
            "  F1: 0.916\n",
            "  Precision: 0.918\n",
            "  Recall: 0.920\n",
            "  Accuracy: 0.920\n",
            "\n",
            "Best performing model: Ensemble Model\n",
            "\n",
            "============================================================\n",
            "ENHANCED SENTIMENT ANALYSIS COMPLETE\n",
            "============================================================\n",
            "Summary report: /content/drive/MyDrive/CAM_DS_AI_Project/results/sentiment/jpm/enhanced_sentiment_analysis_summary.json\n",
            "\n",
            "Enhanced results generated:\n",
            "  enhanced_sentence_jpm_q1_2025: (578, 80) - 0.8 MB (20 enhanced features)\n",
            "  enhanced_sentence_jpm_q2_2025: (532, 80) - 0.73 MB (20 enhanced features)\n",
            "  enhanced_sentence_jpm_multi_2025: (1110, 80) - 1.53 MB (20 enhanced features)\n",
            "  enhanced_qa_jpm_q1_2025: (97, 40) - 0.15 MB (11 enhanced features)\n",
            "  enhanced_qa_jpm_q2_2025: (121, 40) - 0.15 MB (11 enhanced features)\n",
            "  enhanced_qa_jpm_multi_2025: (218, 40) - 0.31 MB (11 enhanced features)\n",
            "\n",
            "Total enhanced records: 2,656\n",
            "Total memory usage: 3.67 MB\n",
            "\n",
            "Best performing model: Ensemble Model\n",
            "  F1-Score: 0.916\n",
            "  Precision: 0.918\n",
            "  Recall: 0.920\n",
            "  Accuracy: 0.920\n",
            "\n",
            "Enhancement Features:\n",
            "  Manual labels utilized: 200\n",
            "  Confidence calibration: Enabled\n",
            "  Ensemble models: 2\n",
            "  Enhanced topics: 0\n",
            "\n",
            "Key Enhancements Applied:\n",
            "  âœ“ Confidence Calibration\n",
            "  âœ“ Ensemble Methods\n",
            "  âœ“ Enhanced Topic Extraction\n",
            "  âœ“ Advanced Anomaly Detection\n",
            "  âœ“ Performance Optimization\n",
            "  âœ“ Text Feature Engineering\n",
            "\n",
            "Next step: Run 04b_model_finetuning.ipynb for fine-tuning with manual labels\n",
            "          Then continue to 05_model_comparison_jpm_enhanced.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Research Questions Analysis Preview\n",
        "\n",
        "def preview_enhanced_research_questions(df: pd.DataFrame):\n",
        "    \"\"\"Preview enhanced analysis for key research questions.\"\"\"\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    print(\"\\nENHanced Research Questions Preview\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Question 1: Enhanced banker vs analyst sentiment divergence\n",
        "    if 'speaker_role' in df.columns and 'ensemble_label' in df.columns:\n",
        "        print(\"1. Enhanced Banker vs Analyst Sentiment (Ensemble Model):\")\n",
        "\n",
        "        speaker_sentiment = df.groupby('speaker_role')['ensemble_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
        "        print(speaker_sentiment.round(3))\n",
        "\n",
        "        # Enhanced divergence with confidence weighting\n",
        "        if 'ensemble_confidence' in df.columns:\n",
        "            weighted_sentiment = df.groupby('speaker_role').apply(\n",
        "                lambda x: (x['ensemble_label'] == 'positive').sum() * x['ensemble_confidence'].mean()\n",
        "            )\n",
        "            print(f\"  Confidence-weighted positive sentiment:\")\n",
        "            for role, score in weighted_sentiment.items():\n",
        "                print(f\"    {role}: {score:.3f}\")\n",
        "\n",
        "    # Question 2: Enhanced temporal analysis with calibrated confidence\n",
        "    if 'quarter' in df.columns and 'finbert_tone_calibrated' in df.columns:\n",
        "        print(\"\\n2. Enhanced Temporal Analysis (Calibrated Confidence):\")\n",
        "\n",
        "        quarter_confidence = df.groupby('quarter')['finbert_tone_calibrated'].mean()\n",
        "        print(\"  Average calibrated confidence by quarter:\")\n",
        "        for quarter, conf in quarter_confidence.items():\n",
        "            print(f\"    {quarter}: {conf:.3f}\")\n",
        "\n",
        "    # Question 3: Topic-specific sentiment patterns\n",
        "    if 'primary_topic' in df.columns and 'ensemble_label' in df.columns:\n",
        "        print(\"\\n3. Topic-Specific Sentiment Patterns:\")\n",
        "\n",
        "        topic_sentiment = df.groupby('primary_topic')['ensemble_label'].value_counts(normalize=True).unstack(fill_value=0)\n",
        "\n",
        "        # Show top 5 topics by volume\n",
        "        topic_counts = df['primary_topic'].value_counts().head(5)\n",
        "        for topic in topic_counts.index:\n",
        "            if topic in topic_sentiment.index:\n",
        "                pos_pct = topic_sentiment.loc[topic, 'positive'] if 'positive' in topic_sentiment.columns else 0\n",
        "                print(f\"    {topic}: {pos_pct:.1%} positive\")\n",
        "\n",
        "# Preview enhanced analysis\n",
        "if enhanced_anomaly_multi_df is not None:\n",
        "    preview_enhanced_research_questions(enhanced_anomaly_multi_df)\n",
        "\n",
        "print(f\"\\nðŸ’¡ Enhanced sentiment analysis with performance optimization complete!\")\n",
        "print(f\"   Ready for fine-tuning in next notebook: 04b_model_finetuning.ipynb\")"
      ],
      "metadata": {
        "id": "lDZ6XOn9_akp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8930d73a-0c42-4b8f-d565-ef73c22b6403"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ENHanced Research Questions Preview\n",
            "----------------------------------------\n",
            "1. Enhanced Banker vs Analyst Sentiment (Ensemble Model):\n",
            "ensemble_label  negative  neutral  positive\n",
            "speaker_role                               \n",
            "analyst            0.045    0.868     0.086\n",
            "cfo                0.068    0.736     0.196\n",
            "executive          0.068    0.843     0.089\n",
            "  Confidence-weighted positive sentiment:\n",
            "    analyst: 17.122\n",
            "    cfo: 79.352\n",
            "    executive: 28.260\n",
            "\n",
            "2. Enhanced Temporal Analysis (Calibrated Confidence):\n",
            "  Average calibrated confidence by quarter:\n",
            "    q1_2025: 0.888\n",
            "    q2_2025: 0.898\n",
            "\n",
            "3. Topic-Specific Sentiment Patterns:\n",
            "    general: 13.1% positive\n",
            "    market_risk: 18.2% positive\n",
            "    economic_outlook: 7.1% positive\n",
            "    regulatory_environment: 15.4% positive\n",
            "    digital_transformation: 45.5% positive\n",
            "\n",
            "ðŸ’¡ Enhanced sentiment analysis with performance optimization complete!\n",
            "   Ready for fine-tuning in next notebook: 04b_model_finetuning.ipynb\n"
          ]
        }
      ]
    }
  ]
}