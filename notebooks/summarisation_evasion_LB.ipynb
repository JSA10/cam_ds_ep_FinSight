{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b820ac6",
   "metadata": {},
   "source": [
    "# **Summarisation & Evasion Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e2962",
   "metadata": {},
   "source": [
    "# **Handover Notes:** [delete after]\n",
    "- Library imports and versions are saved in environments/summarisation_evasion_env.txt\n",
    "- This notebook was originally built for a macbook pro M3 chip so some settings may need to be altered depending on your machine\n",
    "- All files related/ generated by this notebook can be found in notebooks/summarisation_evasion_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b76a8",
   "metadata": {},
   "source": [
    "### **Work progress**\n",
    "1. **Complete**\n",
    "- Summarise banker answers using baseline model.\n",
    "- Use Local RAG pipeline to bring in relevant external documents (PRA risk definitions) to create PRA aligned summaries.\n",
    "- Developed a evasion detection prototype that generates evasion scores based on bankers answers (uses baseline model, LLM- natural language inference using RoBERTa and a blended score)\n",
    "- Used jpm_2025 transcripts to get the pipeline working. Validated the evasion pipeline using jpm-23-1q data (involved human labelling the answer as Direct or Evasive- file saved in notebooks/summarisation_evasion_files).\n",
    "\n",
    "2. **Not complete**\n",
    "- Visualisations e.g. how many evasive answers were there? etc - apply evasion pipeline to dataset and generate statistics on evasiveness \n",
    "- Need to test pipleine on larger data set (e.g. jpm 2023-2025) and check against HSBC to make conclusions & comment on generalisability (answering research question: How does one bank’s tone and thematic profile compare to peers? Are divergences systemic or firm specific?)\n",
    "- Summarisation pipeline could be improved using a two-stage pipeline: by first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned.\n",
    "- Post-processing on the output file for the PRA aligned summaries by Mistral model so they are clearer- can this output be fed into another model to extract more insights/ detect evasion or risk?\n",
    "- Increase the size of the validation set for the evasion pipeline prototype (e.g. more human labelling)\n",
    "- Need to fine tune the evasion pipeline to increase accuracy\n",
    "- Optional extensions e.g. using Agents, more complex RAG pipeline (including more useful context for the model), validation of instances of evasion using external news sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e874e23",
   "metadata": {},
   "source": [
    "# 1. **Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670282",
   "metadata": {},
   "source": [
    "# **2. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f5884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "import math\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set SEED.\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91bb8c",
   "metadata": {},
   "source": [
    "# **3. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743f6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "jpm_2025_df = pd.read_csv('../data/processed/jpm/all_jpm_2025.csv')\n",
    "\n",
    "# View the data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75628c",
   "metadata": {},
   "source": [
    "# **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b70dc",
   "metadata": {},
   "source": [
    "- Used all_jpm_2025.csv dataset\n",
    "- Preliminary preprocessing to label roles as analyst vs banker (invalid roles were corrected) to make downstream analysis easier. Created a new column 'role_normalised'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc49b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec70c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_number  answer_number                            speaker_name  \\\n",
       "201               35            5.0  Chief Financial Officer, JPMorganChase   \n",
       "205               36            3.0  Chief Financial Officer, JPMorganChase   \n",
       "\n",
       "                                            role             company  \\\n",
       "201  And then some. Theres a lot of value added.       JPMorganChase   \n",
       "205                                         Okay  there you have it.   \n",
       "\n",
       "                                               content  year quarter  \\\n",
       "201  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "205  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "\n",
       "                                            source_pdf  \n",
       "201  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "205  data/raw/jpm/jpm-2q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles.\n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = jpm_2025_df[~jpm_2025_df['role'].isin(valid_roles)]\n",
    "\n",
    "# Number of rows with invalid roles.\n",
    "print('Number of rows:', invalid_roles_df.shape[0])\n",
    "\n",
    "# View the rows.\n",
    "invalid_roles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e31cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the correct role information.\n",
    "jpm_2025_df.at[205, 'role'] = 'Chief Financial Officer'\n",
    "jpm_2025_df.at[209, 'role'] = 'Chief Financial Officer'\n",
    "\n",
    "# Verify the roles have been updated.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_2025_df['role_normalised'] = jpm_2025_df['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755f26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf565",
   "metadata": {},
   "source": [
    "# **5. Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401764b",
   "metadata": {},
   "source": [
    "## **5.1 Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee15b",
   "metadata": {},
   "source": [
    "- Initial model exploration using BART and mistral-7B-instruct to summarise banker's answers (no additional context given to model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92395bc",
   "metadata": {},
   "source": [
    "### **5.1.1 BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the w\n"
     ]
    }
   ],
   "source": [
    "# Filter data to banker answers only.\n",
    "banker_answers = jpm_2025_df[jpm_2025_df['role_normalised'] == 'banker']['content'].tolist()\n",
    "print(banker_answers[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54559054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The main thing that we see there, what would appear to be a certain amount of frontloading of spending ahead of people expecting price increases from tariffs. So ironically, that's actually somewhat supportive, all else equal. In terms of our corporate clients, obviously, they've been reacting to the changes in tariff policy.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (BART)\n",
    "bart = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "sample_text = banker_answers[0]\n",
    "summary_bart = bart(sample_text, max_length=80, min_length=30, do_sample=False)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f0d9b",
   "metadata": {},
   "source": [
    "- bart was able to extract ket ideas, focussing on fronloading of spending and tariff policy. \n",
    "- Compressed the response into two sentences and the summary is coherent, removing filler phrases.\n",
    "- However, the summary is not fully neutral (e.g. includes ironically) and preserves tone\n",
    "- Also there is a loss of context- e.g. consumer side vs wholesale side distinction is no longer explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024bd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: Corporates are taking a wait-and-see approach to tariff policy. Some sectors are going to be much more exposed than others. Small business and smaller corporates are probably a little more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Prompt conditioning to make PRA relevant.\n",
    "prompt = \"Summarise this answer, focusing on risk, capital and evasion of detail: \" + sample_text\n",
    "summary_bart_prompted = bart(prompt, max_length=80, min_length=30)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart_prompted[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e7996",
   "metadata": {},
   "source": [
    "- Prompted summary shifts emphasis and includes interpretation around risk, even though those words were no explicit in the original\n",
    "- This version is more aligned to evasion detection but moves away from concrete detail \n",
    "- Improved approach would be to have a two stage-pipeline: first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaabd6",
   "metadata": {},
   "source": [
    "### **5.1.2 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f4b90",
   "metadata": {},
   "source": [
    "- Mistral model: mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Mistral-7B-Instruct model download: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF?show_file_info=mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Also saved in shared team folder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a368c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides. On the consumer side, there has been some frontloading of spending ahead of expected price increases from tariffs, which may distort the data and make it difficult to draw larger conclusions. On the corporate side, clients are reacting to changes in tariff policy by shifting their focus towards short-term work and optimizing supply chains. The speaker characterizes the attitude of corporate clients as a wait-and-see attitude, with smaller clients and smaller corporates being more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with basic prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral = output['choices'][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec467d",
   "metadata": {},
   "source": [
    "- Preserves details and nuance and is more contextual and interpretive than the BART baseline model.\n",
    "- However, the result is longer with heavier phrasing and includes phrases like 'distort the data' which is not explicit in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides of their business. On the consumer side, they have observed some frontloading of spending ahead of expected price increases from tariffs, which may distort data and make it difficult to draw larger conclusions. On the corporate side, clients are shifting their focus towards optimizing supply chains and responding to the current environment, rather than prioritizing more strategic work. The speaker notes that smaller clients and smaller corporates may be more challenged than larger ones, which have more experience dealing with these types of changes and more resources to manage them. Overall, the speaker suggests that it is difficult to make long-term decisions at this time due to the uncertainty surrounding the current environment.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with more detailed prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions. Focus on risk, capital and evasion of detail.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral_prompted = output['choices'][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral_prompted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1e33d",
   "metadata": {},
   "source": [
    "- This summary brings in risk- language and is closer to the task objective.\n",
    "- However, some interpretations are generated by the model rather than explicitly detailed in the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813acc2",
   "metadata": {},
   "source": [
    "## **5.2 Adding Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1b7a",
   "metadata": {},
   "source": [
    "Retrieve PRA risk categories to give greater PRA focus to summaries (local RAG loop).\n",
    "- measure cosine similarity between transcript chunks and PRA risk categories (vectors)\n",
    "- retrieve the top 2-3 most relevant risk categories \n",
    "- prepend them to the summarisation prompt to make summaries PRA-aligned instead of just summarised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2bd1",
   "metadata": {},
   "source": [
    "- Attempting to use BART resulted in prompt echoing.\n",
    "- New attempt using Mistral-7B-Instruct.\n",
    "- Using sentence-BERT vs TF-IDF for vectorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9dab",
   "metadata": {},
   "source": [
    "### **5.2.1 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56464c24",
   "metadata": {},
   "source": [
    "**Process**\n",
    "- Performed some light cleaning of the transcript to remove whitespace.\n",
    "- Split the transcript into smaller chunks that the model can summarise to avoid truncation\n",
    "- Loaded the PRA categories csv file (contains category and definition)\n",
    "- Embedded the PRA categories and chunks, evaluated the similarity to extract the PRA risk categories that were relevant to the text\n",
    "- Summarised the chunk using detailed prompted and relevant PRA categories as additional context. \n",
    "\n",
    "**Output File**:\n",
    "- The output file of this can be found in notebooks/summarisation_evasion_files, name = jpm_mistral_pra_summary.json\n",
    "- It is in the format: summary, evidence, PRA category that relates to summary and reasoning for selecting these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e695d9e",
   "metadata": {},
   "source": [
    "- Needed to use a lot of fine tuning for the prompt and set strict rules for the model\n",
    "- Need to be very clear about the output expected or else the model deviates a lot, especially as it processes more data.\n",
    "- Include lines about lack of evidence if not the model may hallucinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b05ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove whitespace in text.\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f9ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the transcript into smaller chunks.\n",
    "def chunk_text(text: str, max_chars: int = 6000):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split into sentences \n",
    "    chunks, current_chunk, current_len = [], [], 0 # list of chunks, sentences collecting for current chunk, character count for current chunk\n",
    "\n",
    "    for s in sentences:\n",
    "        if current_len + len(s) + 1 <= max_chars: # if the characters of current chunk + new sentence is below the limit:\n",
    "            current_chunk.append(s) # add sentence to current chunk \n",
    "            current_len += len(s) + 1 # update running character count \n",
    "        \n",
    "        else: # if the characters is above the limit:\n",
    "            chunks.append(' '.join(current_chunk)) # add the current chunk to the final chunk list\n",
    "            current_chunk, current_len = [s], len(s) # start a new chunk containing the sentence and update current len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk)) # add any sentences in current chunk after loop ends \n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe2b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PRA categories and definitions from CSV.\n",
    "def load_pra_categories(path: Path):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            (row.get('category', '').strip(), [row.get('definition', '').strip()])\n",
    "            for row in reader if row.get('category')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db60595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sentence-BERT embedding index for PRA categories.\n",
    "def build_embedding_index(pra_categories):\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    docs = [f\"{name} {' '.join(defs)}\" for name, defs in pra_categories]\n",
    "    pra_risk_embeddings = embedder.encode(docs, batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "    return embedder, np.asarray(pra_risk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the relevant PRA categories to the transcript chunks.\n",
    "def find_rel_categories(chunk, pra_categories, embedder, pra_risk_embeddings, top_k=2):\n",
    "    query_vec = embedder.encode([chunk], normalize_embeddings=True) # turns chunk into embedding\n",
    "    sims = cosine_similarity(query_vec, pra_risk_embeddings).ravel() # compares the chunk to each category doc \n",
    "    top_indices = np.argsort(-sims)[:top_k] # sorts scores descending and selected top k cateogories \n",
    "\n",
    "    return [pra_categories[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8f1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON\n",
    "def parse_tagged_json(raw):\n",
    "    m = re.search(r\"<json>\\s*(\\{[\\s\\S]*?\\})\\s*</json>\", raw, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise the text chunks.\n",
    "def summarise_chunk(model, chunk, relevant_categories, max_evidence=5):\n",
    "\n",
    "    # Build PRA notes (limit to 2 bullets per category)\n",
    "    lines = []\n",
    "    for name, definition in relevant_categories:\n",
    "        lines.append(f'- {name}:')\n",
    "        for d in list(definition)[:2]:\n",
    "            lines.append(f'- {d}')\n",
    "    notes_block = '\\n'.join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a careful data extraction model. \"\n",
    "        \"Return ONLY valid JSON wrapped in <json>...</json> tags.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "TRANSCRIPT:\n",
    "{chunk}\n",
    "\n",
    "PRA NOTES:\n",
    "{notes_block}\n",
    "\n",
    "TASK:\n",
    "Return JSON ONLY, wrapped exactly like this:\n",
    "<json>{{\"summary\": \"...\", \"evidence\": [\"...\"], \"pra_categories\": [{{\"category\":\"...\",\"why\":\"...\"}}]}}</json>\n",
    "\n",
    "RULES:\n",
    "- 4-6 sentence neutral summary.\n",
    "- Up to {max_evidence} evidence bullets (quotes/facts).\n",
    "- 1-3 pra_categories objects.\n",
    "- If evidence is lacking, use a single bullet \"Insufficient evidence\".\n",
    "- Only choose categories supported by the evidence.\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = model.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=700,\n",
    "        repeat_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    raw = (response['choices'][0]['message']['content'] or '').strip()\n",
    "\n",
    "    # Parse the tagged JSON\n",
    "    parsed = parse_tagged_json(raw)\n",
    "\n",
    "    # Fallback if model didn’t follow instructions\n",
    "    if not parsed:\n",
    "        return (\n",
    "            {'summary': '', 'evidence': ['Insufficient evidence'], 'pra_categories': []},\n",
    "            raw,\n",
    "        )\n",
    "\n",
    "    # Light coercion to guarantee keys exist\n",
    "    result = {\n",
    "        'summary': parsed.get('summary', '') or '',\n",
    "        'evidence': parsed.get('evidence', []) or [],\n",
    "        'pra_categories': parsed.get('pra_categories', []) or []\n",
    "    }\n",
    "    return result, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e398a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables.\n",
    "MODEL_PATH = '/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "PRA_NOTES_PATH = '../data/RAG-resources/PRA_risk_categories.csv'\n",
    "TRANSCRIPT_PATH = '../data/processed/jpm/all_jpm_2025.csv'\n",
    "OUTPUT_PATH = pathlib.Path('../notebooks/summarisation_evasion_files/jpm_mistral_pra_summary_raw.json')\n",
    "TOP_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c63a4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 3559 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 110 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2495.33 MiB, ( 9858.72 / 10922.67)\n",
      "load_tensors: offloading 20 repeating layers to GPU\n",
      "load_tensors: offloaded 20/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  2495.33 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x386d40ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x3668141b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x386e8f3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x372817f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x372811220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x372817940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x372817c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x386e8fbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x366814470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x366814730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x386e8f8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x3668149f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x386e8ff50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x366814cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x366815110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x3668154b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x366815880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x366815c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x386e90720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x386e909e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x386e90ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x386e90f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x386e91300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x12a66a8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x11a65ea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x366816020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x3668163f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x3668167c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x366816b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x386e916d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x386e91aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x386e91e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x386e92240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x386e92610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x386e929e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x386e92db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x386e93180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x386e93550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x386e93920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x37281b530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x386e93cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x386e940c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x386e94490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x386e94860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x386e94c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x386e95000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x386e953d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x386e957a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x37281b7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x366816ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x366817390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x366817730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x366818650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x386e95b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x386e95f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x386e96310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x37281bab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x37281bd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x36f2a1be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x366818910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x366818c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x366818fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x366812740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x386e96670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x386e96b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x36f2b81b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x386d6bb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x386d6be30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x366812ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x366812ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x36f2dfcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x366813320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x366811d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x36f2eba70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x36681acf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x366808510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x3668087d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x36680b490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x386e974c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x386e971f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x386e97890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x386e97bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x386e98050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x386e98420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x36680b750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x36680baa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x36680c5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x36680c990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x36680cd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x36680d130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x36f2ebd30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x36f2ebff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x36680d500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x36680d8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x36681b830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x386d6c0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x386d6c3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x36681bd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x36681c2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x36f295c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x36681c620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x36f2d6a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x36681c9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x386d6c670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x36681ce00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x36681d1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x36681d5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x386e98780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x386e98bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x386e98f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x386e99360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x36f2aef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x36f2af1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x36f2af4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x386e996c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x386e99b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x386e99ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x36681d900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x36f2f4130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x386e9a250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x36681dcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x36681e130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x36f2c37f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x36f2a0840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x386d6c930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x386d6cbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x386d6ceb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x36681e490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x386e9a6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x386d6d170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x386e9aa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x386d6d480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x36681e860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x386e9af10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x386e9b240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x386e9b610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x386e9b970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x386d6d7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x386e9bf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x386e9c230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x386e9c5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x386e9c970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x386e9cd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x386e9d110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x36f2be340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x386e9d4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x386d6dba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x386d6e000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x36f2ae560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x36f2fdf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x386e9d8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x386e9dc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x386e9e050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x36681ec30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x36681f090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x386e9e6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x386e9e9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x386e9ec60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x386e9ef90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x36f2cb330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x386e9f5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x386e9f860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x386d6e360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x386d6e760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x386d6eb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x386d6ef00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x386d6f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x386e9fb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x386d6f6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x36681f460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x36681f830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x36681fc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x36681ffd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x386e9fe80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x386d6fa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x3668203a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x386ea05e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x386ea08a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x386ea0b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x386ea1160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x366820770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x386d6fe40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x36f2cb5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x36f2cb8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x366820ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x366820f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x366821300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x386d70210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x366821660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x386d70750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x36f2ad370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x36f2d7380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x386ea14c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x386ea1890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x386ea1cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x386ea20c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x386ea2490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x386d70a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x386d70dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x386ea2b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x386d71130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x386d71540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x386d71910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x386d71ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x386d720b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x386d72480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x386d72850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x386d72c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x386d72ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x386d733c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x386ea2e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x386ea3110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x386ea3410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x36f2c5b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x366821b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x366821ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x366822270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x366822640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x386ea37e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x386ea3bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x386ea3f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x386ea4350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x386ea4720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x386d73720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x36f296e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x36f2a0240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x36f2d1c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x36f2b0610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x36f2e9730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x386ea4a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x386ea4ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x386ea5290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x386ea5660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x386ea5a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x386ea5e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x386ea61d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x36f2c6ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x386d73b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x386d73f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x386d74320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x386d746f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x3668229a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x366822e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x386ea68b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x3668231d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x386ea6b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x386ea6e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x36f2725a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x386ea70f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x366823570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x3668239d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x386ea7500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x366823d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x36f272860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x386ea78d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x386ea7ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x386ea8070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x386ea8440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x386ea8810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x3668240d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x366824570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x366824910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x36f272b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x386ea8e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x386ea9110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x386ea93d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x36f272de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x386d74ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x386d74e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x36f2730a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x366824d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x386ea9730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x386ea9b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x386ea9ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x386eaa2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x36f273360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x36f273620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x36f2738e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x386d751f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x386d75550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x386eaa690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x36f273ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x386eaaa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x386d759f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x386d75d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x386eaae40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x386eab2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x386d760b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x386d76510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x386eab600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x36f273e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x386eaba80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x36f274120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x36f2743e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x386eabe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x36f2746a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x386d768b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x36f274960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x36f274c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x386eac1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x36f274ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x36f2751a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x3668250c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x386eac620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x386eac9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x366825600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x3668258c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x386eacd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x366825c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x386ead1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x366826030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x36f275460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x386d76c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x386ead560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x366826400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x3668267d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x366826ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x366826f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x386ead930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x386eadd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x386eae0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x36f275720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x366827340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x366827710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x386eae430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x386eae8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x36f2759e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x366828130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x386d77020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x386d77480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x386eaec60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x366828490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x366828830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x386eaf030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x386d77850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x386eaf400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x386eaf7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x36f275ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x36f275f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x36f276220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x366828c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x366828fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x386d77c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x386d77ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x386d783c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x386d78790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x386eafde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x386eb00a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x366829330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x36f2764e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x366829700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x386eb0610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x386eb08d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x366829d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x36682a000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x386eb0b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x386eb0ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x386eb12b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x386eb1680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x386d78af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x386eb1a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x386eb1e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x36682a2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x36682a6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x36682aab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x36682aeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x36f2767a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x36682b250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x36682b640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x36f276a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x386eb21f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x386eb25c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x36682b9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x36682bdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x36682c170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x36682c540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x36682c910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x36682cce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x36682d0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x36682d480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x36682d850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x36682dc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x36682dff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x36682e3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x386eb2990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x386eb2d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x36f276d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x36682eac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x36682ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x36682f040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x36682f300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x386eb3150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x36f276fe0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   300.01 MiB\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 171 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   18585.98 ms /  1764 tokens (   10.54 ms per token,    94.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17936.85 ms /   279 runs   (   64.29 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   36589.33 ms /  2043 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1707 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   17951.35 ms /  1707 tokens (   10.52 ms per token,    95.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22406.75 ms /   343 runs   (   65.33 ms per token,    15.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   40450.88 ms /  2050 tokens\n",
      "llama_perf_context_print:    graphs reused =        331\n",
      "Llama.generate: 11 prefix-match hit, remaining 1996 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20158.30 ms /  1996 tokens (   10.10 ms per token,    99.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27050.95 ms /   414 runs   (   65.34 ms per token,    15.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   47328.71 ms /  2410 tokens\n",
      "llama_perf_context_print:    graphs reused =        400\n",
      "Llama.generate: 11 prefix-match hit, remaining 1923 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   19938.00 ms /  1923 tokens (   10.37 ms per token,    96.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30883.70 ms /   373 runs   (   82.80 ms per token,    12.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   50949.27 ms /  2296 tokens\n",
      "llama_perf_context_print:    graphs reused =        360\n",
      "Llama.generate: 11 prefix-match hit, remaining 1799 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20397.24 ms /  1799 tokens (   11.34 ms per token,    88.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24709.85 ms /   352 runs   (   70.20 ms per token,    14.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   45203.08 ms /  2151 tokens\n",
      "llama_perf_context_print:    graphs reused =        340\n",
      "Llama.generate: 11 prefix-match hit, remaining 2023 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   21015.96 ms /  2023 tokens (   10.39 ms per token,    96.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34450.48 ms /   484 runs   (   71.18 ms per token,    14.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   55627.97 ms /  2507 tokens\n",
      "llama_perf_context_print:    graphs reused =        468\n",
      "Llama.generate: 12 prefix-match hit, remaining 2044 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   23228.91 ms /  2044 tokens (   11.36 ms per token,    87.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37243.59 ms /   487 runs   (   76.48 ms per token,    13.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   60635.19 ms /  2531 tokens\n",
      "llama_perf_context_print:    graphs reused =        471\n",
      "Llama.generate: 12 prefix-match hit, remaining 1918 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   21965.39 ms /  1918 tokens (   11.45 ms per token,    87.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28628.28 ms /   385 runs   (   74.36 ms per token,    13.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   50714.54 ms /  2303 tokens\n",
      "llama_perf_context_print:    graphs reused =        372\n",
      "Llama.generate: 11 prefix-match hit, remaining 1770 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   22291.89 ms /  1770 tokens (   12.59 ms per token,    79.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26506.86 ms /   399 runs   (   66.43 ms per token,    15.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   48919.94 ms /  2169 tokens\n",
      "llama_perf_context_print:    graphs reused =        385\n",
      "Llama.generate: 11 prefix-match hit, remaining 1715 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   18539.54 ms /  1715 tokens (   10.81 ms per token,    92.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21738.51 ms /   319 runs   (   68.15 ms per token,    14.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   40365.89 ms /  2034 tokens\n",
      "llama_perf_context_print:    graphs reused =        308\n",
      "Llama.generate: 11 prefix-match hit, remaining 1789 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20029.53 ms /  1789 tokens (   11.20 ms per token,    89.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19266.00 ms /   283 runs   (   68.08 ms per token,    14.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   39368.59 ms /  2072 tokens\n",
      "llama_perf_context_print:    graphs reused =        273\n",
      "Llama.generate: 11 prefix-match hit, remaining 1959 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20862.08 ms /  1959 tokens (   10.65 ms per token,    93.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52060.77 ms /   699 runs   (   74.48 ms per token,    13.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   73234.66 ms /  2658 tokens\n",
      "llama_perf_context_print:    graphs reused =        676\n",
      "Llama.generate: 11 prefix-match hit, remaining 1748 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   18481.40 ms /  1748 tokens (   10.57 ms per token,    94.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20161.45 ms /   301 runs   (   66.98 ms per token,    14.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   38721.94 ms /  2049 tokens\n",
      "llama_perf_context_print:    graphs reused =        290\n",
      "Llama.generate: 11 prefix-match hit, remaining 1856 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   18968.62 ms /  1856 tokens (   10.22 ms per token,    97.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23462.55 ms /   334 runs   (   70.25 ms per token,    14.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   42525.62 ms /  2190 tokens\n",
      "llama_perf_context_print:    graphs reused =        323\n",
      "Llama.generate: 11 prefix-match hit, remaining 1881 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   23806.58 ms /  1881 tokens (   12.66 ms per token,    79.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22345.23 ms /   279 runs   (   80.09 ms per token,    12.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   46233.84 ms /  2160 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1938 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   23072.23 ms /  1938 tokens (   11.91 ms per token,    84.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51323.45 ms /   699 runs   (   73.42 ms per token,    13.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   74678.91 ms /  2637 tokens\n",
      "llama_perf_context_print:    graphs reused =        676\n",
      "Llama.generate: 11 prefix-match hit, remaining 1842 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20299.32 ms /  1842 tokens (   11.02 ms per token,    90.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21241.26 ms /   313 runs   (   67.86 ms per token,    14.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   41621.19 ms /  2155 tokens\n",
      "llama_perf_context_print:    graphs reused =        302\n",
      "Llama.generate: 11 prefix-match hit, remaining 1898 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20332.43 ms /  1898 tokens (   10.71 ms per token,    93.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21721.55 ms /   317 runs   (   68.52 ms per token,    14.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   42130.60 ms /  2215 tokens\n",
      "llama_perf_context_print:    graphs reused =        306\n",
      "Llama.generate: 11 prefix-match hit, remaining 1983 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   21249.31 ms /  1983 tokens (   10.72 ms per token,    93.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25584.64 ms /   364 runs   (   70.29 ms per token,    14.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   46932.22 ms /  2347 tokens\n",
      "llama_perf_context_print:    graphs reused =        352\n",
      "Llama.generate: 11 prefix-match hit, remaining 2050 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   22520.90 ms /  2050 tokens (   10.99 ms per token,    91.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29406.97 ms /   394 runs   (   74.64 ms per token,    13.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   52053.27 ms /  2444 tokens\n",
      "llama_perf_context_print:    graphs reused =        381\n",
      "Llama.generate: 11 prefix-match hit, remaining 1804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   20090.77 ms /  1804 tokens (   11.14 ms per token,    89.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26448.90 ms /   382 runs   (   69.24 ms per token,    14.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   46647.83 ms /  2186 tokens\n",
      "llama_perf_context_print:    graphs reused =        369\n",
      "Llama.generate: 11 prefix-match hit, remaining 2062 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =   22458.74 ms /  2062 tokens (   10.89 ms per token,    91.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26894.84 ms /   372 runs   (   72.30 ms per token,    13.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   49453.64 ms /  2434 tokens\n",
      "llama_perf_context_print:    graphs reused =        359\n",
      "Llama.generate: 11 prefix-match hit, remaining 853 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18586.71 ms\n",
      "llama_perf_context_print: prompt eval time =    8332.38 ms /   853 tokens (    9.77 ms per token,   102.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14788.88 ms /   254 runs   (   58.22 ms per token,    17.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   23176.48 ms /  1107 tokens\n",
      "llama_perf_context_print:    graphs reused =        246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote final JSON to: /Users/laurenbrixey/Documents/GitHub Repositories/cam_ds_ep_FinSight/notebooks/summarisation_evasion_files/jpm_mistral_pra_summary_raw.json\n"
     ]
    }
   ],
   "source": [
    "# Runner code.\n",
    "pra_categories = load_pra_categories(Path(PRA_NOTES_PATH))\n",
    "embedder, category_embeddings = build_embedding_index(pra_categories)\n",
    "\n",
    "# Load and chunk transcript\n",
    "transcript_text = Path(TRANSCRIPT_PATH).read_text(encoding='utf-8')\n",
    "transcript_chunks = chunk_text(transcript_text)\n",
    "\n",
    "n_threads = max(4, (os.cpu_count() or 8) - 2)\n",
    "\n",
    "# Define the model.\n",
    "model = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=20,\n",
    "    chat_format='mistral-instruct',\n",
    "    n_threads=n_threads,\n",
    ")\n",
    "\n",
    "raw_outputs = []\n",
    "\n",
    "for i, chunk in enumerate(transcript_chunks, 1):\n",
    "    try:\n",
    "        top_categories = find_rel_categories(\n",
    "            chunk, pra_categories, embedder, category_embeddings, top_k=TOP_K\n",
    "        )\n",
    "        _, raw = summarise_chunk(\n",
    "            model, chunk, top_categories, max_evidence=5\n",
    "        )\n",
    "        raw_outputs.append({'chunk': i, 'raw': raw})\n",
    "\n",
    "    except Exception:\n",
    "        raw_outputs.append({'chunk': i, 'raw': ''})\n",
    "\n",
    "final_output = {'raw_outputs': raw_outputs}\n",
    "\n",
    "OUTPUT_PATH.write_text(json.dumps(final_output, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f'Wrote final JSON to: {OUTPUT_PATH.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ff6d7",
   "metadata": {},
   "source": [
    "- Need to preprocess the output so it is visually clearer (summary, evidence, PRA categories (name & why the model chose this))\n",
    "- Can this information be fed to the model again and can it detect any early PRA risk indicators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9d93f",
   "metadata": {},
   "source": [
    "# **6. Evasion Scoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654e09d",
   "metadata": {},
   "source": [
    "- Use LLM to summarise answer and then tag with an evasion score.\n",
    "- Detect evasiveness of bankers in relation to analyst questions and give an evasiveness score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb359de2",
   "metadata": {},
   "source": [
    "## **6.1 Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c133a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a48b5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair each analyst question with all the banker's answers.\n",
    "def create_qa_pairs(df):\n",
    "    questions = df[df['role_normalised'] == 'analyst']\n",
    "    answers = df[df['role_normalised'] == 'banker']\n",
    "\n",
    "    qa_pairs = []\n",
    "\n",
    "    for q_num, q_row in questions.groupby('question_number'):\n",
    "        q_text = ' '.join(q_row['content'].astype(str))\n",
    "        a_rows = answers[answers['question_number'] == q_num]\n",
    "        if not a_rows.empty:\n",
    "            a_text = ' '.join(a_rows['content'].astype(str))\n",
    "            qa_pairs.append({\n",
    "                'question_number': q_num, \n",
    "                'question': q_text,\n",
    "                'answer': a_text\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93c8ff9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yes. Good morning. This question is for Jamie....</td>\n",
       "      <td>I just – before Jamie answers that, Erika, I j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Got it. And a second follow-up question. And I...</td>\n",
       "      <td>Yeah, Erika, it's a good question. But the tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you. Operator: Thank you. Our next quest...</td>\n",
       "      <td>Thanks, Erika. Operator: I apologize. Our next...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  Good morning, Jeremy. Wondering if you could s...   \n",
       "1                2  Yeah. And just one question on the NII ex. Mar...   \n",
       "2                3  Yes. Good morning. This question is for Jamie....   \n",
       "3                4  Got it. And a second follow-up question. And I...   \n",
       "4                5  Thank you. Operator: Thank you. Our next quest...   \n",
       "\n",
       "                                              answer  \n",
       "0  Sure, Ken. So I mean, at a high level, I would...  \n",
       "1  Yeah, that's a good question, Ken. You're righ...  \n",
       "2  I just – before Jamie answers that, Erika, I j...  \n",
       "3  Yeah, Erika, it's a good question. But the tru...  \n",
       "4  Thanks, Erika. Operator: I apologize. Our next...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create qa pairs.\n",
    "jpm_2025_qa_pairs_df = create_qa_pairs(jpm_2025_df)\n",
    "\n",
    "# View the results.\n",
    "jpm_2025_qa_pairs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f97b9",
   "metadata": {},
   "source": [
    "## **6.2 Evasion Detection (prototype)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031e1b4",
   "metadata": {},
   "source": [
    "1. **Baseline Evasion score** (rule-based) is made up of three components:\n",
    "- **Cosine similarity**- similarity of the question and answer, lower similarity = more evasive\n",
    "- **Numeric specificity check**- does the question require a number, if so does the answer contain a number?, e.g. requests for financial data\n",
    "- **Evasive phrases**- does the answer contain evasive phrases?, presence = more evasive\n",
    "\n",
    "2. **LLM evasion score** (RoBERTa-MNLI) uses entailment/neutral/contradiction between the question and answer\n",
    "- Lower entailment (and higher neutral + contradiction) = more evasive\n",
    "  \n",
    "3. **Blended evasion score** combines both scores including a weight for the LLM component\n",
    "- Rationale is that baseline enforces precision while the LLM will capture semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd36971",
   "metadata": {},
   "source": [
    "- Filter 2023-2025 dataset. \n",
    "- 2024-2025 = training dataset\n",
    "- 1/2 2023 = validation dataset (fine-tune thresholds)\n",
    "- 1/2 2023 = test dataset (test new thresholds and evaluate results)\n",
    "- Test 3 different LLM models and ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a717495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import RoBERTa model and tokenizer.\n",
    "roberta_name = 'roberta-large-mnli'\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "roberta = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "roberta.eval()\n",
    "\n",
    "# RoBERTa has 3 labels\n",
    "assert roberta.config.num_labels == 3\n",
    "\n",
    "# Label order for roberta-large-mnli\n",
    "id2label_roberta = {0: 'contradiction', 1: 'neutral', 2: 'entailment'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6cfc4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import DeBERTA model and tokenizer.\n",
    "deberta_name = 'microsoft/deberta-v3-large-mnli'\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "deberta = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# # RoBERTa has 3 labels\n",
    "# assert roberta.config.num_labels == 3\n",
    "\n",
    "# # Label order for roberta-large-mnli\n",
    "# id2label_roberta = {0: 'contradiction', 1: 'neutral', 2: 'entailment'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7a30f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import finance-tuned model and tokenizer\n",
    "finance_roberta_name = 'yiyanghkust/roberta-large-qqp-mnli'\n",
    "finance_roberta_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "finance_roberta = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fddddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (change this if not using a macbook) \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')   # Apple Metal\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "model.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c60cf",
   "metadata": {},
   "source": [
    "### **Baseline evasion score functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9c22283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of evasive phrases\n",
    "EVASIVE_PHRASES = [\n",
    "    r\"\\btoo early\\b\",\n",
    "    r\"\\bcan't (?:comment|share|discuss)\\b\",\n",
    "    r\"\\bwon't (?:comment|share|provide)\\b\",\n",
    "    r\"\\bno (?:update|comment)\\b\",\n",
    "    r\"\\bwe (?:don't|do not) (?:break out|provide guidance)\\b\",\n",
    "    r\"\\bnot (?:going to|able to) (?:comment|share|provide)\\b\",\n",
    "    r\"\\bwe'll (?:come back|circle back)\\b\",\n",
    "    r\"\\bnot something we disclose\\b\",\n",
    "    r\"\\bas (?:we|I) (?:said|mentioned)\\b\",\n",
    "    r\"\\bgenerally speaking\\b\",\n",
    "    r\"\\bit's premature\\b\",\n",
    "    r\"\\bit's difficult to say\\b\",\n",
    "    r\"\\bI (?:wouldn't|won't) want to (?:speculate|get into)\\b\",\n",
    "    r\"\\bI (?:think|guess|suppose)\\b\",\n",
    "    r\"\\bkind of\\b\",\n",
    "    r\"\\bsort of\\b\",\n",
    "    r\"\\baround\\b\",\n",
    "    r\"\\broughly\\b\",\n",
    "    r\"\\bwe (?:prefer|plan) not to\\b\",\n",
    "    r\"\\bwe're not prepared to\\b\",\n",
    "]\n",
    "\n",
    "# List of words that suggest the answer needs specific financial numbers to properly answer the question.\n",
    "SPECIFICITY_TRIGGERS = [\n",
    "    \"how much\",\"how many\",\"what is\",\"what are\",\"when\",\"which\",\"where\",\"who\",\"why\",\n",
    "    \"range\",\"guidance\",\"margin\",\"capex\",\"opex\",\"revenue\",\"sales\",\"eps\",\"ebitda\",\n",
    "    \"timeline\",\"date\",\"target\",\"growth\",\"update\",\"split\",\"dividend\",\"cost\",\"price\",\n",
    "    \"units\",\"volumes\",\"gross\",\"net\",\"tax\",\"percentage\",\"utilization\",\"order book\"\n",
    "]\n",
    "\n",
    "NUMERIC_PATTERN = r\"(?:\\d+(?:\\.\\d+)?%|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b|£|\\$|€)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70af0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between question and answers.\n",
    "def cosine_sim(q, a):\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform([q, a]) # converts text to vectors \n",
    "    sim = float(cosine_similarity(vec[0], vec[1])[0, 0]) # calculate the cosine similarity between the two vectors\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9a484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute baseline evasion score.\n",
    "def baseline_evasion_score(q, a):\n",
    "    # 1. Cosine similarity\n",
    "    sim = cosine_sim(q, a) # calculates cosine similarity using previous function\n",
    "    sim_component = (1 - sim) * 45 # less similar the answer is, the bigger the contribution to the evasion score, scaled by 45\n",
    "\n",
    "    # 2. Numerical specificity- Does the question require and answer with financial figures/ a specific answer?\n",
    "    needs_num = any(t in q.lower() for t in SPECIFICITY_TRIGGERS) # true if the question requires a numeric/ specific answer\n",
    "    has_num = bool(re.search(NUMERIC_PATTERN, a)) # true if the answer includes a number \n",
    "    numeric_component = 25 if needs_num and not has_num else 0 # score of 25 if the question needs a number but the answer doesn't give one\n",
    "\n",
    "    # 3. Evasive phrases- does the answer contain evasive phrases?\n",
    "    phrase_hits = sum(len(re.findall(p, a.lower())) for p in EVASIVE_PHRASES) # counts how many times an evasive phrase appears in the answer\n",
    "    phrase_component = min(3, phrase_hits) * 8 # max of 3 hits counted, each hit = 8 points \n",
    "\n",
    "    # Final evasion score.\n",
    "    score = min(100, sim_component + numeric_component + phrase_component) # adds components together and caps score at 100\n",
    "    \n",
    "    return score, sim, phrase_hits, needs_num, has_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f5e81",
   "metadata": {},
   "source": [
    "### **LLM and blended evasion score functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4f1aa",
   "metadata": {},
   "source": [
    "- Can test RoBERTa vs deberta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "58f83a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute llm label scores: entailment, contradiction and neutral\n",
    "def llm_label_scores(question, answer, model, tokenizer, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Function to calculate probabilities based on RoBERTa-MNLI labels.\n",
    "    def probs(premise, hypothesis):\n",
    "        inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, max_length=512).to(device)  # tokenize the input\n",
    "        with torch.no_grad():  # disable gradient tracking \n",
    "            logits = model(**inputs).logits  # runs the model and outputs raw scores\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).squeeze().tolist()  # converts raw scores to probabilities\n",
    "\n",
    "        # RoBERTa-MNLI label order: [contradiction, neutral, entailment]\n",
    "        # contradiction = contradicts the question, neutral = related but not committing, entailment = directly answers the question\n",
    "        return {'contradiction': probs[0], 'neutral': probs[1], 'entailment': probs[2]}\n",
    "\n",
    "    # Get probability in both directions (e.g. does answer entail question, does question entail answer?)\n",
    "    pA, pB = probs(answer, question), probs(question, answer)\n",
    "\n",
    "    # Calculates scores.\n",
    "    entail = math.sqrt(max(1e-9, pA['entailment'] * pB['entailment']))\n",
    "    neutral = 0.5 * (pA['neutral'] + pB['neutral'])\n",
    "    contradiction = 0.5 * (pA['contradiction'] + pB['contradiction'])\n",
    "\n",
    "    # Normalises the scores.\n",
    "    s = entail + neutral + contradiction\n",
    "    entail, neutral, contradiction = entail/s, neutral/s, contradiction/s\n",
    "    return {\n",
    "        'entailment': entail,\n",
    "        'neutral': neutral,\n",
    "        'contradiction': contradiction\n",
    "    }\n",
    "\n",
    "# Function to compute LLM evasion score from the label scores.\n",
    "def llm_evasion_score(entail, neutral, contradiction):\n",
    "    # Maps (entail, neutral, contradiction) -> evasion score in range 0..100\n",
    "    evasion = (1 - entail) * 60 + neutral * 30 + contradiction * 10\n",
    "    return max(0.0, min(100.0, evasion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "69ae2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_tokenizers = {\n",
    "        'roberta': (roberta, roberta_tokenizer),\n",
    "        'deberta': (deberta, deberta_tokenizer),\n",
    "        'finance_roberta': (finance_roberta, finance_roberta_tokenizer)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9aa9911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute blended evasion score and return all scores.\n",
    "def compute_all_evasion_scores(q, a, LLM_WEIGHT=0.30, device=torch.device, models_and_tokenizers=models_and_tokenizers):\n",
    "    \n",
    "    # Compute baseline evasion score.\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # # Compute LLM evasion score.\n",
    "    # models_and_tokenizers = {\n",
    "    #     'roberta': (roberta, roberta_tokenizer),\n",
    "    #     'deberta': (deberta, deberta_tokenizer),\n",
    "    #     'finance_roberta': (finance_roberta, finance_roberta_tokenizer)\n",
    "    #     }\n",
    "\n",
    "    # Individual LLM scores.\n",
    "    llm_scores = {}\n",
    "    for name, (m, t) in models_and_tokenizers.items():\n",
    "        llm_label = llm_label_scores(q, a, m, t, device)\n",
    "        llm_scores[name] = llm_evasion_score(llm_label['entailment'], llm_label['neutral'], llm_label['contradiction'])\n",
    "\n",
    "    # Ensemble LLM score.\n",
    "    llm_avg = float(np.mean(list(llm_scores.values()))) if llm_scores else 0.0\n",
    "\n",
    "\n",
    "    # Compute blended score.\n",
    "    blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * llm_avg, 0.0, 100.0))\n",
    "\n",
    "    return {\n",
    "        'baseline': base_score,\n",
    "        'llm_individual': llm_scores,\n",
    "        'llm_avg': llm_avg,\n",
    "        'blended': blended_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a7618e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label based on the score.\n",
    "def label_from_score(score, threshold):\n",
    "    return 'Evasive' if score >= threshold else 'Direct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1fd47",
   "metadata": {},
   "source": [
    "### **Main Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27f819db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preliminary thresholds.\n",
    "LLM_WEIGHT = 0.30\n",
    "EVASION_THRESHOLD_BASE = 60.0\n",
    "EVASION_THRESHOLD_LLM = 50.0\n",
    "EVASION_THRESHOLD_BLENDED = 60.0\n",
    "device = torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "81ea294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evasion Pipeline.\n",
    "def evasion_pipeline(df, models_and_tokenizers, device, LLM_WEIGHT, EVASION_THRESHOLD_BASE, EVASION_THRESHOLD_LLM, EVASION_THRESHOLD_BLENDED):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row['question']), str(row['answer'])\n",
    "        output = compute_all_evasion_scores(q, a, LLM_WEIGHT, device, models_and_tokenizers)\n",
    "\n",
    "        pred_base = label_from_score(output['baseline'], EVASION_THRESHOLD_BASE)\n",
    "        pred_llm_avg = label_from_score(output['llm_avg'], EVASION_THRESHOLD_LLM)\n",
    "        pred_blended = label_from_score(output['blended'], EVASION_THRESHOLD_BLENDED)\n",
    "\n",
    "        record = {\n",
    "            'question_number': row.get('question_number'),\n",
    "            'question': q,\n",
    "            'answer': a,\n",
    "\n",
    "            # Evasion Scores\n",
    "            'evasion_score_baseline': int(output['baseline']),\n",
    "            'evasion_score_llm_avg': int(output['llm_avg']),\n",
    "            \"evasion_score_blended\": int(output['blended']),\n",
    "\n",
    "            # Predicted labels.\n",
    "            'prediction_baseline': pred_base,\n",
    "            'prediction_llm_avg': pred_llm_avg,\n",
    "            'prediction_blended': pred_blended,\n",
    "        }\n",
    "\n",
    "        for model_name, score in output['llm_individual'].items():\n",
    "            record[f'evasion_score_{model_name}'] = int(score)\n",
    "            record[f'prediction_{model_name}'] = label_from_score(score, EVASION_THRESHOLD_LLM)\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e865ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick device (instance!)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move each model once\n",
    "for name, (m, t) in models_and_tokenizers.items():\n",
    "    m.to(device)\n",
    "    m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evasion pipeline.\n",
    "jpm_2025_evasion_results = evasion_pipeline(\n",
    "    jpm_2025_qa_pairs_df, \n",
    "    models_and_tokenizers=models_and_tokenizers, \n",
    "    device=device, \n",
    "    LLM_WEIGHT=0.30, \n",
    "    EVASION_THRESHOLD_BASE=60.0,\n",
    "    EVASION_THRESHOLD_LLM=50.0,\n",
    "    EVASION_THRESHOLD_BLENDED=60.0)\n",
    "\n",
    "# View results.\n",
    "jpm_2025_evasion_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c0cfa",
   "metadata": {},
   "source": [
    "### **Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43c32e",
   "metadata": {},
   "source": [
    "- Built a validation set of 26 examples from 2023 1q jpm results, human-labelled these evasive or direct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f492696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2023 transcript.\n",
    "jpm_1q_23_df = pd.read_csv('../data/processed/jpm/jpm-1q23-earnings-call-transcript_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_1q_23_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22775960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "1            qa              NaN            NaN  Steven Chubak   \n",
       "2            qa              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "1                             analyst    Wolfe Research LLC   \n",
       "2             Chief Financial Officer  JPMorgan Chase & Co.   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "1                                 Hey, good morning.  2023      Q1   \n",
       "2                               Good morning, Steve.  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "\n",
       "   is_pleasantry role_normalised  \n",
       "0          False          banker  \n",
       "1           True         analyst  \n",
       "2           True          banker  \n",
       "3          False         analyst  \n",
       "4          False          banker  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_1q_23_df['role_normalised'] = jpm_1q_23_df['role'].map(role_map)\n",
    "\n",
    "# View the dataset.\n",
    "jpm_1q_23_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955b5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>...elevated macro uncertainties.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matt O'Connor</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Deutsche Bank Securities, Inc.</td>\n",
       "      <td>Okay. And then just separately to squeeze in –...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>That'll be every quarter for the rest of our l...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>qa</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Cheap.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>qa</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matt O'Connor</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Deutsche Bank Securities, Inc.</td>\n",
       "      <td>Okay. All right. Thank you. Operator: We have ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   section  question_number  answer_number   speaker_name  \\\n",
       "3       qa              1.0            NaN  Steven Chubak   \n",
       "4       qa              1.0            1.0    Jamie Dimon   \n",
       "5       qa              1.0            1.0  Steven Chubak   \n",
       "6       qa              1.0            2.0    Jamie Dimon   \n",
       "7       qa              1.0            2.0  Steven Chubak   \n",
       "..     ...              ...            ...            ...   \n",
       "93      qa             26.0            NaN  Matt O'Connor   \n",
       "94      qa             26.0            1.0    Jamie Dimon   \n",
       "95      qa             26.0            2.0  Jeremy Barnum   \n",
       "96      qa             26.0            3.0    Jamie Dimon   \n",
       "97      qa             27.0            NaN  Matt O'Connor   \n",
       "\n",
       "                                  role                         company  \\\n",
       "3                              analyst              Wolfe Research LLC   \n",
       "4   Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "5                              analyst              Wolfe Research LLC   \n",
       "6   Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "7                              analyst              Wolfe Research LLC   \n",
       "..                                 ...                             ...   \n",
       "93                             analyst  Deutsche Bank Securities, Inc.   \n",
       "94  Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "95             Chief Financial Officer            JPMorgan Chase & Co.   \n",
       "96  Chairman & Chief Executive Officer            JPMorgan Chase & Co.   \n",
       "97                             analyst  Deutsche Bank Securities, Inc.   \n",
       "\n",
       "                                              content  year quarter  \\\n",
       "3   So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4   Well, I think you were already kind of complet...  2023      Q1   \n",
       "5   Got it. And just in terms of appetite for the ...  2023      Q1   \n",
       "6                                           Oh, yeah.  2023      Q1   \n",
       "7                    ...elevated macro uncertainties.  2023      Q1   \n",
       "..                                                ...   ...     ...   \n",
       "93  Okay. And then just separately to squeeze in –...  2023      Q1   \n",
       "94  That'll be every quarter for the rest of our l...  2023      Q1   \n",
       "95                                             Cheap.  2023      Q1   \n",
       "96                                             Cheap.  2023      Q1   \n",
       "97  Okay. All right. Thank you. Operator: We have ...  2023      Q1   \n",
       "\n",
       "    is_pleasantry role_normalised  \n",
       "3           False         analyst  \n",
       "4           False          banker  \n",
       "5           False         analyst  \n",
       "6           False          banker  \n",
       "7           False         analyst  \n",
       "..            ...             ...  \n",
       "93          False         analyst  \n",
       "94          False          banker  \n",
       "95          False          banker  \n",
       "96          False          banker  \n",
       "97          False         analyst  \n",
       "\n",
       "[85 rows x 11 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out presention and is_pleasantry == True\n",
    "jpm_1q_23_df = jpm_1q_23_df[jpm_1q_23_df['section'] == 'qa']\n",
    "jpm_1q_23_df = jpm_1q_23_df[jpm_1q_23_df['is_pleasantry'] == False]\n",
    "\n",
    "# View the dataset.\n",
    "jpm_1q_23_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0              1.0  So, Jamie, I was actually hoping to get your p...   \n",
       "1              2.0  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2              3.0  Yeah, and as a follow-up on the point about ra...   \n",
       "3              4.0  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4              5.0  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  \n",
       "0  Well, I think you were already kind of complet...  \n",
       "1  Yeah, sure. So let me just summarize the drive...  \n",
       "2  Well first of all, I don't quite believe it. S...  \n",
       "3  Yeah. John, it's a really good question, and w...  \n",
       "4  Yeah. I wouldn't use the word credit crunch if...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 26\n"
     ]
    }
   ],
   "source": [
    "# Create qa pairs.\n",
    "jpm_1q_23_qa_pairs_df = create_qa_pairs(jpm_1q_23_df)\n",
    "\n",
    "# View dataset.\n",
    "display(jpm_1q_23_qa_pairs_df.head())\n",
    "\n",
    "# View shape.\n",
    "print('Number of samples:', jpm_1q_23_qa_pairs_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eacd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test set. \n",
    "jpm_1q_23_test_set_df = jpm_1q_23_qa_pairs_df.copy()\n",
    "\n",
    "# Create a blank label column and export to CSV for human to label.\n",
    "jpm_1q_23_test_set_df['label'] = ''  # fill with Direct or Evasive\n",
    "jpm_1q_23_test_set_df.to_csv('jpm_1q_23_test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacd1e4",
   "metadata": {},
   "source": [
    "- The test set was human labelled with either 'direct' or 'evasive'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7d1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer    label  \n",
       "0  Well, I think you were already kind of complet...  Evasive  \n",
       "1  Yeah, sure. So let me just summarize the drive...   Direct  \n",
       "2  Well first of all, I don't quite believe it. S...   Direct  \n",
       "3  Yeah. John, it's a really good question, and w...  Evasive  \n",
       "4  Yeah. I wouldn't use the word credit crunch if...   Direct  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the labelled test set. \n",
    "jpm_1q_23_test_set_labelled_df = pd.read_csv('../notebooks/summarisation_evasion_files/jpm_1q_23_test_set_labelled.csv')\n",
    "\n",
    "# View dataset.\n",
    "jpm_1q_23_test_set_labelled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43f3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_llm</th>\n",
       "      <th>prediction_blended</th>\n",
       "      <th>human_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>100</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>99</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Well, I think you were already kind of complet...                      55   \n",
       "1  Yeah, sure. So let me just summarize the drive...                      80   \n",
       "2  Well first of all, I don't quite believe it. S...                      40   \n",
       "3  Yeah. John, it's a really good question, and w...                      78   \n",
       "4  Yeah. I wouldn't use the word credit crunch if...                      55   \n",
       "\n",
       "   evasion_score_llm  evasion_score_blended prediction_baseline  \\\n",
       "0                 79                     79              Direct   \n",
       "1                 78                    100             Evasive   \n",
       "2                 83                     65              Direct   \n",
       "3                 73                     99             Evasive   \n",
       "4                 81                     80              Direct   \n",
       "\n",
       "  prediction_llm prediction_blended human_label  \n",
       "0        Evasive            Evasive     Evasive  \n",
       "1        Evasive            Evasive      Direct  \n",
       "2        Evasive            Evasive      Direct  \n",
       "3        Evasive            Evasive     Evasive  \n",
       "4        Evasive            Evasive      Direct  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evasion pipeline.\n",
    "jpm_1q_23_evasion_results = evasion_pipeline(jpm_1q_23_test_set_labelled_df)\n",
    "\n",
    "# Reappend the human label.\n",
    "jpm_1q_23_evasion_results['human_label'] = jpm_1q_23_test_set_labelled_df['label']\n",
    "\n",
    "# View results.\n",
    "jpm_1q_23_evasion_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9831d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the evasion scores vs true labels.\n",
    "def evaluate_evasion_scores(df):\n",
    "\n",
    "    # True labels: 1 = Evasive, 0 = Direct (using 'human_label').\n",
    "    y_true = (df[\"human_label\"].astype(str).str.strip().str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "    # Convert predicted label strings to binary (1 = Evasive, 0 = Direct).\n",
    "    def to_binary(pred_series):\n",
    "        return (pred_series.astype(str).str.strip().str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "    # Convert predicted labels to binary.\n",
    "    y_pred_base  = to_binary(df[\"prediction_baseline\"])\n",
    "    y_pred_llm   = to_binary(df[\"prediction_llm\"])\n",
    "    y_pred_blend = to_binary(df[\"prediction_blended\"])\n",
    "\n",
    "    return {\n",
    "        'baseline': {\n",
    "            'classification_report': classification_report(y_true, y_pred_base, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_base)\n",
    "        },\n",
    "        'llm': {\n",
    "            'classification_report': classification_report(y_true, y_pred_llm, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_llm)\n",
    "        },\n",
    "        'blended': {\n",
    "            'classification_report': classification_report(y_true, y_pred_blend, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_blend) \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results.\n",
    "eval_dict = evaluate_evasion_scores(jpm_1q_23_evasion_results)\n",
    "baseline_eval, llm_eval, blended_eval = eval_dict['baseline'], eval_dict['llm'], eval_dict['blended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.588     0.714     0.645        14\n",
      "     Evasive      0.556     0.417     0.476        12\n",
      "\n",
      "    accuracy                          0.577        26\n",
      "   macro avg      0.572     0.565     0.561        26\n",
      "weighted avg      0.573     0.577     0.567        26\n",
      "\n",
      "[[10  4]\n",
      " [ 7  5]]\n"
     ]
    }
   ],
   "source": [
    "# View baseline results.\n",
    "base_cr, base_cm = baseline_eval['classification_report'], baseline_eval['confusion_matrix']\n",
    "\n",
    "print(base_cr)\n",
    "print(base_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.000     0.000     0.000        14\n",
      "     Evasive      0.462     1.000     0.632        12\n",
      "\n",
      "    accuracy                          0.462        26\n",
      "   macro avg      0.231     0.500     0.316        26\n",
      "weighted avg      0.213     0.462     0.291        26\n",
      "\n",
      "[[ 0 14]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "# View llm results.\n",
    "llm_cr, llm_cm = llm_eval['classification_report'], llm_eval['confusion_matrix']\n",
    "\n",
    "print(llm_cr)\n",
    "print(llm_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      1.000     0.071     0.133        14\n",
      "     Evasive      0.480     1.000     0.649        12\n",
      "\n",
      "    accuracy                          0.500        26\n",
      "   macro avg      0.740     0.536     0.391        26\n",
      "weighted avg      0.760     0.500     0.371        26\n",
      "\n",
      "[[ 1 13]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "# View blended results.\n",
    "blended_cr, blended_cm = blended_eval['classification_report'], blended_eval['confusion_matrix']\n",
    "\n",
    "print(blended_cr)\n",
    "print(blended_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80e12c",
   "metadata": {},
   "source": [
    "- Baseline (rule-based) accuracy is currently the highest at 60%\n",
    "- Fine tune the thresholds using grid search to determine the optimal threshold to give the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d786791",
   "metadata": {},
   "source": [
    "### **Tuning threshold**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec7381",
   "metadata": {},
   "source": [
    "- Recall will be the priority over accuracy to prevent missing evasive answers. \n",
    "- Some false positives are tolerable and regulator can review flagged answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199945e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>human_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>100</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>99</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie – there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0                1  So, Jamie, I was actually hoping to get your p...   \n",
       "1                2  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "2                3  Yeah, and as a follow-up on the point about ra...   \n",
       "3                4  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "4                5  Okay. And then I wanted to ask Jamie – there's...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Well, I think you were already kind of complet...                      55   \n",
       "1  Yeah, sure. So let me just summarize the drive...                      80   \n",
       "2  Well first of all, I don't quite believe it. S...                      40   \n",
       "3  Yeah. John, it's a really good question, and w...                      78   \n",
       "4  Yeah. I wouldn't use the word credit crunch if...                      55   \n",
       "\n",
       "   evasion_score_llm  evasion_score_blended human_label  \n",
       "0                 79                     79     Evasive  \n",
       "1                 78                    100      Direct  \n",
       "2                 83                     65      Direct  \n",
       "3                 73                     99     Evasive  \n",
       "4                 81                     80      Direct  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain validation set with evasion scores and drop predictions.\n",
    "jpm_1q_23_tuning = jpm_1q_23_evasion_results.drop(['prediction_baseline', 'prediction_llm', 'prediction_blended'], axis=1)\n",
    "jpm_1q_23_tuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth (1 = Evasive, 0 = Direct)\n",
    "def extract_y_true(df):\n",
    "    return (df['human_label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculate metrics for each threshold.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def tune_threshold(df, score_col, thr_grid):\n",
    "    y_true = extract_y_true(df)                     # get true labels\n",
    "    scores = df[score_col].astype(float).values     # get raw evasion scores \n",
    "\n",
    "    rows = []\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (scores >= thr).astype(int) # label response evasive (1) if score is higher than threshold\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        rows.append({\n",
    "            'threshold': float(thr),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    results = pd.DataFrame(rows).sort_values(\n",
    "        by=['recall', 'f1', 'precision'],\n",
    "        ascending=[False, False, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold ranges around current thresholds.\n",
    "thr_base_grid = np.arange(40, 85, 5)\n",
    "thr_llm_grid = np.arange(35, 85, 5)\n",
    "thr_blend_grid = np.arange(40, 85, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search for each detector.\n",
    "base_results = tune_threshold(jpm_1q_23_tuning, 'evasion_score_baseline', thr_base_grid)\n",
    "llm_results = tune_threshold(jpm_1q_23_tuning, 'evasion_score_llm', thr_llm_grid)\n",
    "blend_results = tune_threshold(jpm_1q_23_tuning, 'evasion_score_blended', thr_blend_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Baseline Threshold: 40.0\n",
      "Best LLM Threshold: 70.0\n",
      "Best Blended Threshold 40.0\n"
     ]
    }
   ],
   "source": [
    "# Extract the best thresholds.\n",
    "best_base_thr = base_results.loc[0, 'threshold']\n",
    "best_llm_thr = llm_results.loc[0, 'threshold']\n",
    "best_blend_thr = blend_results.loc[0, 'threshold']\n",
    "\n",
    "print('Best Baseline Threshold:', best_base_thr)\n",
    "print('Best LLM Threshold:', best_llm_thr)\n",
    "print('Best Blended Threshold', best_base_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a41a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 baseline configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       40.0   0.454545  0.833333  0.588235  0.461538\n",
      "1       45.0   0.500000  0.666667  0.571429  0.538462\n",
      "2       50.0   0.583333  0.583333  0.583333  0.615385\n",
      "3       55.0   0.545455  0.500000  0.521739  0.576923\n",
      "4       65.0   0.714286  0.416667  0.526316  0.653846\n",
      "\n",
      "Top 5 llm configs:\n",
      "    threshold  precision  recall        f1  accuracy\n",
      "0       70.0   0.521739     1.0  0.685714  0.576923\n",
      "1       65.0   0.480000     1.0  0.648649  0.500000\n",
      "2       35.0   0.461538     1.0  0.631579  0.461538\n",
      "3       40.0   0.461538     1.0  0.631579  0.461538\n",
      "4       45.0   0.461538     1.0  0.631579  0.461538\n",
      "\n",
      "Top 5 blended configs:\n",
      "    threshold  precision  recall        f1  accuracy\n",
      "0       55.0   0.480000     1.0  0.648649  0.500000\n",
      "1       60.0   0.480000     1.0  0.648649  0.500000\n",
      "2       40.0   0.461538     1.0  0.631579  0.461538\n",
      "3       45.0   0.461538     1.0  0.631579  0.461538\n",
      "4       50.0   0.461538     1.0  0.631579  0.461538\n"
     ]
    }
   ],
   "source": [
    "# Inspect trade-offs.\n",
    "print('\\nTop 5 baseline configs:\\n', base_results.head())\n",
    "print('\\nTop 5 llm configs:\\n', llm_results.head())\n",
    "print('\\nTop 5 blended configs:\\n', blend_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51b9dc",
   "metadata": {},
   "source": [
    "**Baseline:**\n",
    "- 40.0 gave the highest recall (0.83) but lower precision (0.45)\n",
    "- 50.0 is more balanced (0.58 recall and 0.58 precision)\n",
    "\n",
    "**LLM**\n",
    "- Every top configuration has recall = 1.0 which shows the LLM detector is very sensitive \n",
    "- Raising the threshold increases precision slightly = 70.0\n",
    "\n",
    "**Blended**\n",
    "- Similar to LLM result, 55.0 or 60.0 gave slightly higher precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
