{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc758a1",
   "metadata": {},
   "source": [
    "# **Evasion Detection Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d970815",
   "metadata": {},
   "source": [
    "# **1. Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1387398",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to contain the evasion detection pipeline. \n",
    "1. **Baseline Evasion score** (rule-based) is made up of three components:\n",
    "- **Cosine similarity**- similarity of the question and answer, lower similarity = more evasive\n",
    "- **Numeric specificity check**- does the question require a number, if so does the answer contain a number?, e.g. requests for financial data\n",
    "- **Evasive phrases**- does the answer contain evasive phrases?, presence = more evasive\n",
    "\n",
    "2. **LLM evasion score** (RoBERTa-MNLI) uses entailment/neutral/contradiction between the question and answer\n",
    "- Lower entailment (and higher neutral + contradiction) = more evasive\n",
    "  \n",
    "3. **Blended evasion score** combines both scores including a weight for the LLM component\n",
    "- Rationale is that baseline enforces precision while the LLM will capture semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8372746",
   "metadata": {},
   "source": [
    "# **1. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "431bddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import transformers, datasets, inspect\n",
    "from llama_cpp import Llama \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set SEED.\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d4ae2",
   "metadata": {},
   "source": [
    "# **2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30edeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "1            qa              NaN            NaN  Steven Chubak   \n",
       "2            qa              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "1                             analyst    Wolfe Research LLC   \n",
       "2             Chief Financial Officer  JPMorgan Chase & Co.   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "1                                 Hey, good morning.  2023      Q1   \n",
       "2                               Good morning, Steve.  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "\n",
       "   is_pleasantry                                         source_pdf  \n",
       "0          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "1           True  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "2           True  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "3          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  \n",
       "4          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1411\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "all_jpm_2023_2025 = pd.read_csv('../data/processed/jpm/all_jpm_2023_2025.csv')\n",
    "\n",
    "# View dataset.\n",
    "display(all_jpm_2023_2025.head())\n",
    "\n",
    "# Number of rows.\n",
    "print('Number of rows:', all_jpm_2023_2025.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7647de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1241\n"
     ]
    }
   ],
   "source": [
    "# Remove pleasantries.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025[all_jpm_2023_2025['is_pleasantry'] == False]\n",
    "print('Number of rows:', all_jpm_2023_2025_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b9e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 23\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025_cleaned['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31340c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025_cleaned.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c042a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 0\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025_cleaned['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2752faaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay',\n",
       "       \"We're fundamentally\", 'Thanks', 'Almost no chance.'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View roles.\n",
    "all_jpm_2023_2025_cleaned['role'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc3188",
   "metadata": {},
   "source": [
    "- Some text has leaked into role column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0d690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>qa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Who knows how important politics are in all th...</td>\n",
       "      <td>We're fundamentally</td>\n",
       "      <td>as I said, I think on the press call, happy to...</td>\n",
       "      <td>little bit cautious about the pull-forward dyn...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-1q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>qa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Chief Financial Officer, JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Thanks</td>\n",
       "      <td>Glenn.</td>\n",
       "      <td>Operator: Next, we'll go to the line of Matt O...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer, JPMorgan C...</td>\n",
       "      <td>Almost no chance.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Well, but having – it's very important. While ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q3</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-3q24-earnings-conference-call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     section  question_number  answer_number  \\\n",
       "305       qa             22.0            4.0   \n",
       "309       qa             23.0            3.0   \n",
       "650       qa             10.0            3.0   \n",
       "924       qa              8.0            2.0   \n",
       "1059      qa             22.0            4.0   \n",
       "1063      qa             23.0            3.0   \n",
       "1274      qa             23.0            1.0   \n",
       "\n",
       "                                           speaker_name  \\\n",
       "305              Chief Financial Officer, JPMorganChase   \n",
       "309              Chief Financial Officer, JPMorganChase   \n",
       "650   Who knows how important politics are in all th...   \n",
       "924       Chief Financial Officer, JPMorgan Chase & Co.   \n",
       "1059             Chief Financial Officer, JPMorganChase   \n",
       "1063             Chief Financial Officer, JPMorganChase   \n",
       "1274  Chairman & Chief Executive Officer, JPMorgan C...   \n",
       "\n",
       "                                             role  \\\n",
       "305   And then some. Theres a lot of value added.   \n",
       "309                                          Okay   \n",
       "650                           We're fundamentally   \n",
       "924                                        Thanks   \n",
       "1059  And then some. Theres a lot of value added.   \n",
       "1063                                         Okay   \n",
       "1274                            Almost no chance.   \n",
       "\n",
       "                                                company  \\\n",
       "305                                       JPMorganChase   \n",
       "309                                  there you have it.   \n",
       "650   as I said, I think on the press call, happy to...   \n",
       "924                                              Glenn.   \n",
       "1059                                      JPMorganChase   \n",
       "1063                                 there you have it.   \n",
       "1274                                      JPMorganChase   \n",
       "\n",
       "                                                content  year quarter  \\\n",
       "305   Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "309   But it's not like I thought it would do badly,...  2025      Q2   \n",
       "650   little bit cautious about the pull-forward dyn...  2024      Q1   \n",
       "924   Operator: Next, we'll go to the line of Matt O...  2024      Q2   \n",
       "1059  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "1063  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "1274  Well, but having – it's very important. While ...  2024      Q3   \n",
       "\n",
       "      is_pleasantry                                         source_pdf  \n",
       "305           False  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...  \n",
       "309           False  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...  \n",
       "650           False  data/raw/jpm/jpm-1q24-earnings-call-transcript...  \n",
       "924           False  data/raw/jpm/jpm-2q24-earnings-call-transcript...  \n",
       "1059          False  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "1063          False  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "1274          False  data/raw/jpm/jpm-3q24-earnings-conference-call...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles. \n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = all_jpm_2023_2025_cleaned[~all_jpm_2023_2025_cleaned['role'].isin(valid_roles)]\n",
    "invalid_roles_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2103540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the correct role information.\n",
    "all_jpm_2023_2025_cleaned.loc[[305, 309, 924, 1059, 1063], 'role'] = 'Chief Financial Officer'\n",
    "all_jpm_2023_2025_cleaned.loc[[1274], 'role'] = 'Chairman & Chief Executive Officer'\n",
    "\n",
    "# Drop nonsence row.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025_cleaned.drop(index=650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80ad9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the roles have been updated.\n",
    "all_jpm_2023_2025_cleaned['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ac15ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise role names.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Map roles.\n",
    "all_jpm_2023_2025_cleaned['role_normalised'] = all_jpm_2023_2025_cleaned['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "829a8e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  question_number  answer_number   speaker_name  \\\n",
       "0  presentation              NaN            NaN  Jeremy Barnum   \n",
       "3            qa              1.0            NaN  Steven Chubak   \n",
       "4            qa              1.0            1.0    Jamie Dimon   \n",
       "5            qa              1.0            1.0  Steven Chubak   \n",
       "6            qa              1.0            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role               company  \\\n",
       "0             Chief Financial Officer         JPMorganChase   \n",
       "3                             analyst    Wolfe Research LLC   \n",
       "4  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "5                             analyst    Wolfe Research LLC   \n",
       "6  Chairman & Chief Executive Officer  JPMorgan Chase & Co.   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Thanks, and good morning, everyone. The presen...  2023      Q1   \n",
       "3  So, Jamie, I was actually hoping to get your p...  2023      Q1   \n",
       "4  Well, I think you were already kind of complet...  2023      Q1   \n",
       "5  Got it. And just in terms of appetite for the ...  2023      Q1   \n",
       "6                                          Oh, yeah.  2023      Q1   \n",
       "\n",
       "   is_pleasantry                                         source_pdf  \\\n",
       "0          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "3          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "4          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "5          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "6          False  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   \n",
       "\n",
       "  role_normalised  \n",
       "0          banker  \n",
       "3         analyst  \n",
       "4          banker  \n",
       "5         analyst  \n",
       "6          banker  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1217\n"
     ]
    }
   ],
   "source": [
    "# View dataset.\n",
    "display(all_jpm_2023_2025_cleaned.head())\n",
    "print('Number of rows:', all_jpm_2023_2025_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8054baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset.\n",
    "all_jpm_2023_2025_cleaned.to_csv('../data/processed/jpm/cleaned/all_jpm_2023_2025_cleaned') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6934907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to remove duplicates within questions and answers. \n",
    "def clean_repeats(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # 1) Normalize whitespace\n",
    "    t = ' '.join(text.split()).strip()\n",
    "    if not t:\n",
    "        return t\n",
    "\n",
    "    # 2) If the whole-string is a back-to-back duplicate (A+A) = keep first half\n",
    "    mid = len(t) // 2\n",
    "    if len(t) % 2 == 0 and t[:mid] == t[mid:]:\n",
    "        t = t[:mid]\n",
    "\n",
    "    # 3) Collapse immediate repeated token spans (n-grams)\n",
    "    toks = t.split()\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        matched = False\n",
    "        max_span = min(50, len(toks) - i)  # cap span to remaining length\n",
    "        for n in range(max_span, 4, -1):  # try longer spans first: 50..5\n",
    "            if i + 2*n <= len(toks) and toks[i:i+n] == toks[i+n:i+2*n]:\n",
    "                out.extend(toks[i:i+n])  # keep one copy\n",
    "                i += 2*n                # skip the duplicate block\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            out.append(toks[i])\n",
    "            i += 1\n",
    "    t = ' '.join(out)\n",
    "\n",
    "    # 4) Remove duplicate sentences globally (order-preserving)\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', t)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for s in sents:\n",
    "        s_norm = s.strip()\n",
    "        if not s_norm:\n",
    "            continue\n",
    "        key = ' '.join(s_norm.lower().split())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(s_norm)\n",
    "    return ' '.join(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c551bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert datasets into question and answer pairs.\n",
    "def create_qa_pairs(df, min_answer_words=30):\n",
    "    # Keep only the Q&A section.\n",
    "    qa_df = df[df['section'].astype(str).str.lower() == 'qa'].copy()\n",
    "\n",
    "    # Split into roles.\n",
    "    analyst_rows = qa_df[qa_df['role_normalised'] == 'analyst'].copy()\n",
    "    banker_rows  = qa_df[qa_df['role_normalised'] == 'banker' ].copy()\n",
    "\n",
    "    # Keys to keep quarters separated\n",
    "    key_q = ['year', 'quarter', 'question_number']\n",
    "\n",
    "    # Build full question text per (year, quarter, question_number)\n",
    "    question_text_map = (\n",
    "        analyst_rows\n",
    "        .groupby(key_q, dropna=False)['content']\n",
    "        .apply(lambda parts: clean_repeats(' '.join(parts.astype(str))))\n",
    "        .rename('question')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Ensure bankers have an answer_number — sequential per (year, quarter, question_number) if missing\n",
    "    if 'answer_number' not in banker_rows.columns or banker_rows['answer_number'].isna().any():\n",
    "        banker_rows = banker_rows.sort_index().copy()\n",
    "        banker_rows['answer_number'] = (\n",
    "            banker_rows\n",
    "            .groupby(key_q, dropna=False)\n",
    "            .cumcount() + 1\n",
    "        )\n",
    "\n",
    "    # Combine multiple banker utterances belonging to the same answer\n",
    "    banker_answers = (\n",
    "        banker_rows\n",
    "        .groupby(key_q + ['answer_number'], dropna=False)\n",
    "        .agg({\n",
    "            'content':        lambda parts: clean_repeats(' '.join(parts.astype(str))),\n",
    "            'speaker_name':   'first',\n",
    "            'role':           'first',\n",
    "            'role_normalised':'first',\n",
    "            'source_pdf':     'first'\n",
    "        })\n",
    "        .rename(columns={'content': 'answer'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge question text back onto each answer row\n",
    "    qa_pairs = banker_answers.merge(\n",
    "        question_text_map,\n",
    "        on=key_q,\n",
    "        how='left',\n",
    "        validate='many_to_one'\n",
    "    )\n",
    "\n",
    "    # Order columns for readability\n",
    "    column_order = [\n",
    "        'year', 'quarter', 'question_number', 'answer_number',\n",
    "        'question', 'answer',\n",
    "        'speaker_name', 'role', 'role_normalised',\n",
    "        'source_pdf'\n",
    "    ]\n",
    "    qa_pairs = qa_pairs.reindex(columns=[c for c in column_order if c in qa_pairs.columns])\n",
    "\n",
    "    # Sort and reset index.\n",
    "    qa_pairs = qa_pairs.sort_values(['year', 'quarter', 'question_number', 'answer_number']).reset_index(drop=True)\n",
    "\n",
    "    # Drop duplicate answers.\n",
    "    qa_pairs = qa_pairs.drop_duplicates(subset=['answer'])\n",
    "\n",
    "    # Drop short answers below threshold to ensure quality answers.\n",
    "    qa_pairs = qa_pairs[qa_pairs['answer'].astype(str).str.split().str.len() >= int(min_answer_words)]\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8425a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create q&A pairs.\n",
    "all_jpm_2023_2025_qa = create_qa_pairs(all_jpm_2023_2025_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b82548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 309\n"
     ]
    }
   ],
   "source": [
    "# View number of examples.\n",
    "print('Number of examples:', all_jpm_2023_2025_qa.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e070042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into prediction set and validation/training/test set.\n",
    "jpm_2025_predict_qa = all_jpm_2023_2025_qa[all_jpm_2023_2025_qa['year'] == 2025]\n",
    "jpm_2023_2024_qa = all_jpm_2023_2025_qa[all_jpm_2023_2025_qa['year'].isin([2023, 2024])]\n",
    "\n",
    "# Save the datasets.\n",
    "jpm_2025_predict_qa.to_csv('../data/processed/jpm/cleaned/jpm_2025_predict_qa.csv') \n",
    "jpm_2023_2024_qa.to_csv('../data/processed/jpm/cleaned/jpm_2023_2024_qa.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c481b",
   "metadata": {},
   "source": [
    "The jpm_2023_2024_qa dataset was then manually labelled according to whether the banker's answer was deemed 'Direct' or 'Evasive'. The label was appended by a new column 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa178cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>role_normalised</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good morning. Thanks for all the comments on t...</td>\n",
       "      <td>Yeah. Matt, not particularly updating. I think...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then just separately, you bought bac...</td>\n",
       "      <td>Yeah. Good question. And I think you framed it...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thanks. Jeremy, could you give a little more c...</td>\n",
       "      <td>Yeah. Actually, John, this quarter, that's all...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then, just to follow up on the NII, ...</td>\n",
       "      <td>Sure. Yeah, happy to do that, John. So, I thin...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hey. Good morning. Maybe just to follow up in ...</td>\n",
       "      <td>Yeah. Both good questions. So let's do reprice...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year quarter  question_number  answer_number  \\\n",
       "0  2023      Q4              1.0            1.0   \n",
       "1  2023      Q4              2.0            1.0   \n",
       "2  2023      Q4              3.0            1.0   \n",
       "3  2023      Q4              4.0            1.0   \n",
       "4  2023      Q4              5.0            1.0   \n",
       "\n",
       "                                            question  \\\n",
       "0  Good morning. Thanks for all the comments on t...   \n",
       "1  Okay. And then just separately, you bought bac...   \n",
       "2  Thanks. Jeremy, could you give a little more c...   \n",
       "3  Okay. And then, just to follow up on the NII, ...   \n",
       "4  Hey. Good morning. Maybe just to follow up in ...   \n",
       "\n",
       "                                              answer   speaker_name  \\\n",
       "0  Yeah. Matt, not particularly updating. I think...  Jeremy Barnum   \n",
       "1  Yeah. Good question. And I think you framed it...  Jeremy Barnum   \n",
       "2  Yeah. Actually, John, this quarter, that's all...  Jeremy Barnum   \n",
       "3  Sure. Yeah, happy to do that, John. So, I thin...  Jeremy Barnum   \n",
       "4  Yeah. Both good questions. So let's do reprice...  Jeremy Barnum   \n",
       "\n",
       "                      role role_normalised  \\\n",
       "0  Chief Financial Officer          banker   \n",
       "1  Chief Financial Officer          banker   \n",
       "2  Chief Financial Officer          banker   \n",
       "3  Chief Financial Officer          banker   \n",
       "4  Chief Financial Officer          banker   \n",
       "\n",
       "                                          source_pdf   label  \n",
       "0  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "1  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "2  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "3  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  \n",
       "4  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the labelled dataset.\n",
    "jpm_2023_2024_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_2023_2024_qa_labelled.csv')\n",
    "\n",
    "# View the dataset.\n",
    "jpm_2023_2024_qa_labelled = jpm_2023_2024_qa_labelled.drop('Unnamed: 0', axis=1)\n",
    "jpm_2023_2024_qa_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a43aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split into test, training and validation datasets, preserve number of evasive cases per set.\n",
    "def train_val_test(df, group_key, test_fraction, val_fraction, random_state):\n",
    "\n",
    "    # Split test from full data.\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_fraction, random_state=random_state)\n",
    "    idx_trainval, idx_test = next(gss1.split(df, groups=df[group_key]))\n",
    "    train_and_val = df.iloc[idx_trainval].reset_index(drop=True)\n",
    "    test_set = df.iloc[idx_test].reset_index(drop=True)\n",
    "\n",
    "    # Split VAL from the remaining data (val is relative to full size)\n",
    "    val_fraction_of_remaining = val_fraction / (1.0 - test_fraction)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_fraction_of_remaining, random_state=random_state + 1)\n",
    "    idx_train, idx_val = next(gss2.split(train_and_val, groups=train_and_val[group_key]))\n",
    "    train_set = train_and_val.iloc[idx_train].reset_index(drop=True)\n",
    "    val_set = train_and_val.iloc[idx_val].reset_index(drop=True)\n",
    "\n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ba04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a group key so answers for the same question are not split between datasets.\n",
    "jpm_2023_2024_qa_labelled['group_key'] = (\n",
    "    jpm_2023_2024_qa_labelled[\"year\"].astype(str) + \"_\" +\n",
    "    jpm_2023_2024_qa_labelled[\"quarter\"].astype(str) + \"_\" +\n",
    "    jpm_2023_2024_qa_labelled[\"question_number\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d87dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test, training and validation datasets.\n",
    "jpm_train, jpm_val, jpm_test = train_val_test(\n",
    "    jpm_2023_2024_qa_labelled,\n",
    "    group_key='group_key',\n",
    "    test_fraction=0.30,\n",
    "    val_fraction=0.20,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf10685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 107 (evasive: 22)\n",
      "Number of validation examples: 43 (evasive: 11)\n",
      "Number of test examples: 65 (evasive: 9)\n"
     ]
    }
   ],
   "source": [
    "# View the split. \n",
    "print(f'Number of training examples: {jpm_train.shape[0]} (evasive: {jpm_train[jpm_train[\"label\"] == \"Evasive\"].shape[0]})')\n",
    "print(f'Number of validation examples: {jpm_val.shape[0]} (evasive: {jpm_val[jpm_val[\"label\"] == \"Evasive\"].shape[0]})')\n",
    "print(f'Number of test examples: {jpm_test.shape[0]} (evasive: {jpm_test[jpm_test[\"label\"] == \"Evasive\"].shape[0]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0adfe513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets.\n",
    "jpm_train.to_csv('../data/processed/jpm/cleaned/jpm_train.csv') \n",
    "jpm_val.to_csv('../data/processed/jpm/cleaned/jpm_val.csv') \n",
    "jpm_test.to_csv('../data/processed/jpm/cleaned/jpm_test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd7965",
   "metadata": {},
   "source": [
    "# **3. Rule-based Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698b7be",
   "metadata": {},
   "source": [
    "## **3.1 Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f012907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of evasive phrases\n",
    "EVASIVE_PHRASES = [\n",
    "    r\"\\btoo early\\b\",\n",
    "    r\"\\bcan't (?:comment|share|discuss)\\b\",\n",
    "    r\"\\bwon't (?:comment|share|provide)\\b\",\n",
    "    r\"\\bno (?:update|comment)\\b\",\n",
    "    r\"\\bwe (?:don't|do not) (?:break out|provide guidance)\\b\",\n",
    "    r\"\\bnot (?:going to|able to) (?:comment|share|provide)\\b\",\n",
    "    r\"\\bwe'll (?:come back|circle back)\\b\",\n",
    "    r\"\\bnot something we disclose\\b\",\n",
    "    r\"\\bas (?:we|I) (?:said|mentioned)\\b\",\n",
    "    r\"\\bgenerally speaking\\b\",\n",
    "    r\"\\bit's premature\\b\",\n",
    "    r\"\\bit's difficult to say\\b\",\n",
    "    r\"\\bI (?:wouldn't|won't) want to (?:speculate|get into)\\b\",\n",
    "    r\"\\bI (?:think|guess|suppose)\\b\",\n",
    "    r\"\\bkind of\\b\",\n",
    "    r\"\\bsort of\\b\",\n",
    "    r\"\\baround\\b\",\n",
    "    r\"\\broughly\\b\",\n",
    "    r\"\\bwe (?:prefer|plan) not to\\b\",\n",
    "    r\"\\bwe're not prepared to\\b\",\n",
    "]\n",
    "\n",
    "# List of words that suggest the answer needs specific financial numbers to properly answer the question.\n",
    "SPECIFICITY_TRIGGERS = [\n",
    "    \"how much\",\"how many\",\"what is\",\"what are\",\"when\",\"which\",\"where\",\"who\",\"why\",\n",
    "    \"range\",\"guidance\",\"margin\",\"capex\",\"opex\",\"revenue\",\"sales\",\"eps\",\"ebitda\",\n",
    "    \"timeline\",\"date\",\"target\",\"growth\",\"update\",\"split\",\"dividend\",\"cost\",\"price\",\n",
    "    \"units\",\"volumes\",\"gross\",\"net\",\"tax\",\"percentage\",\"utilization\",\"order book\"\n",
    "]\n",
    "\n",
    "NUMERIC_PATTERN = r\"(?:\\d+(?:\\.\\d+)?%|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b|£|\\$|€)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4585b1",
   "metadata": {},
   "source": [
    "## **3.2 Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59f8212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between question and answers.\n",
    "def cosine_sim(q, a):\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform([q, a]) # converts text to vectors \n",
    "    sim = float(cosine_similarity(vec[0], vec[1])[0, 0]) # calculate the cosine similarity between the two vectors\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38d819de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute baseline evasion score.\n",
    "def baseline_evasion_score(q, a):\n",
    "    # 1. Cosine similarity\n",
    "    sim = cosine_sim(q, a) # calculates cosine similarity using previous function\n",
    "    sim_component = (1 - sim) * 45 # less similar the answer is, the bigger the contribution to the evasion score, scaled by 45\n",
    "\n",
    "    # 2. Numerical specificity- Does the question require and answer with financial figures/ a specific answer?\n",
    "    needs_num = any(t in q.lower() for t in SPECIFICITY_TRIGGERS) # true if the question requires a numeric/ specific answer\n",
    "    has_num = bool(re.search(NUMERIC_PATTERN, a)) # true if the answer includes a number \n",
    "    numeric_component = 25 if needs_num and not has_num else 0 # score of 25 if the question needs a number but the answer doesn't give one\n",
    "\n",
    "    # 3. Evasive phrases- does the answer contain evasive phrases?\n",
    "    phrase_hits = sum(len(re.findall(p, a.lower())) for p in EVASIVE_PHRASES) # counts how many times an evasive phrase appears in the answer\n",
    "    phrase_component = min(3, phrase_hits) * 8 # max of 3 hits counted, each hit = 8 points \n",
    "\n",
    "    # Final evasion score.\n",
    "    score = min(100, sim_component + numeric_component + phrase_component) # adds components together and caps score at 100\n",
    "    \n",
    "    return score, sim, phrase_hits, needs_num, has_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25ce77",
   "metadata": {},
   "source": [
    "# **4. LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2afef3",
   "metadata": {},
   "source": [
    "## **4.1 Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09efe1",
   "metadata": {},
   "source": [
    "Small, lightweight models were selected for this to prevent memory overload and long training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de9e0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 107/107 [00:00<00:00, 2442.70 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 4259.99 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 4053.43 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training distilroberta-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70/140 00:34 < 00:35, 1.97 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720518</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655463</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642544</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.579545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.620321</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.616477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.585227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (F1-optimized): {'thr': 0.5493720173835754, 'precision': 0.2682926829268293, 'recall': 1.0, 'f1': 0.4230769230769231}\n",
      "VAL predicted positives at thr=0.549: 41/43\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL (argmax): {'eval_loss': 0.6554630398750305, 'eval_accuracy': 0.2558139534883721, 'eval_precision': 0.2558139534883721, 'eval_recall': 1.0, 'eval_f1': 0.4074074074074074, 'eval_auc': 0.5, 'eval_runtime': 0.4895, 'eval_samples_per_second': 87.839, 'eval_steps_per_second': 6.128, 'epoch': 5.0}\n",
      "TEST (thresholded): {'accuracy': 0.23076923076923078, 'precision': 0.15254237288135594, 'recall': 1.0, 'f1': 0.2647058823529412, 'threshold': 0.5493720173835754, 'confusion_matrix': [[6, 50], [0, 9]]}\n",
      "TEST predicted positives at thr=0.549: 59/65\n",
      "Model saved to: /Users/laurenbrixey/trained_models/evasion/distilroberta-base_direct_evasive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 107/107 [00:00<00:00, 2709.61 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 4008.87 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 3278.69 examples/s]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training microsoft/deberta-v3-small\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70/140 00:54 < 00:55, 1.25 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694409</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.649149</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.528409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633938</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.491477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.688178</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.426136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.664632</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.482955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (F1-optimized): {'thr': 0.5717650651931763, 'precision': 0.30303030303030304, 'recall': 0.9090909090909091, 'f1': 0.45454545454545453}\n",
      "VAL predicted positives at thr=0.572: 33/43\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL (argmax): {'eval_loss': 0.6491491794586182, 'eval_accuracy': 0.2558139534883721, 'eval_precision': 0.2558139534883721, 'eval_recall': 1.0, 'eval_f1': 0.4074074074074074, 'eval_auc': 0.5284090909090908, 'eval_runtime': 0.9301, 'eval_samples_per_second': 46.231, 'eval_steps_per_second': 3.225, 'epoch': 5.0}\n",
      "TEST (thresholded): {'accuracy': 0.5230769230769231, 'precision': 0.19444444444444445, 'recall': 0.7777777777777778, 'f1': 0.3111111111111111, 'threshold': 0.5717650651931763, 'confusion_matrix': [[27, 29], [2, 7]]}\n",
      "TEST predicted positives at thr=0.572: 36/65\n",
      "Model saved to: /Users/laurenbrixey/trained_models/evasion/microsoft_deberta-v3-small_direct_evasive\n",
      "\n",
      "Summary: {'distilroberta-base': {'val_threshold': {'thr': 0.5493720173835754, 'precision': 0.2682926829268293, 'recall': 1.0, 'f1': 0.4230769230769231}, 'test_metrics': {'accuracy': 0.23076923076923078, 'precision': 0.15254237288135594, 'recall': 1.0, 'f1': 0.2647058823529412}}, 'microsoft/deberta-v3-small': {'val_threshold': {'thr': 0.5717650651931763, 'precision': 0.30303030303030304, 'recall': 0.9090909090909091, 'f1': 0.45454545454545453}, 'test_metrics': {'accuracy': 0.5230769230769231, 'precision': 0.19444444444444445, 'recall': 0.7777777777777778, 'f1': 0.3111111111111111}}}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Evasion Detection Trainer (F1-optimized threshold)\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# ===========\n",
    "# Config\n",
    "# ===========\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_TRAIN = 8\n",
    "BATCH_EVAL = 16\n",
    "USE_FOCAL = False          # set True to enable focal loss\n",
    "FOCAL_GAMMA = 2.0\n",
    "MINORITY_WEIGHT_MULT = 2.0 # try 2.0–3.0 if collapse persists\n",
    "\n",
    "# Keep EVERYTHING out of your repo\n",
    "os.environ[\"HF_HOME\"] = os.path.expanduser(\"~/hf_cache\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.expanduser(\"~/hf_datasets\")\n",
    "RUNS_DIR   = os.path.expanduser(\"~/ml_runs/evasion\")\n",
    "MODELS_DIR = os.path.expanduser(\"~/trained_models/evasion\")\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ===========\n",
    "# Utilities\n",
    "# ===========\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.manual_seed(seed)  # ignore if not present\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def prepare_text(df):\n",
    "    # helps model separate Q from A\n",
    "    return (df[\"question\"].fillna(\"\") + \" </s> \" + df[\"answer\"].fillna(\"\"))\n",
    "\n",
    "label_map = {'direct': 0, 'evasive': 1}\n",
    "def normalise_labels(s):\n",
    "    return s.astype(str).str.lower().map(label_map)\n",
    "\n",
    "def choose_threshold_by_f1_strict(y_true, prob_pos, min_pos=1, max_pos=None):\n",
    "    \"\"\"\n",
    "    Choose threshold maximizing F1 on VAL using *strict* rules:\n",
    "      - Sweep unique probability cutpoints (midpoints between sorted unique probs).\n",
    "      - Skip degenerate thresholds that predict all-positives or all-negatives.\n",
    "      - Tie-breakers: higher precision, then threshold closer to 0.5.\n",
    "    \"\"\"\n",
    "    n = len(prob_pos)\n",
    "    if max_pos is None:\n",
    "        max_pos = n - 1\n",
    "\n",
    "    uniq = np.unique(prob_pos)\n",
    "    if len(uniq) == 1:\n",
    "        cuts = [uniq[0]]  # model is flat; only one option\n",
    "    else:\n",
    "        cuts = list((uniq[:-1] + uniq[1:]) / 2.0)\n",
    "        cuts = [max(0.0, float(uniq[0] - 1e-6))] + cuts + [min(1.0, float(uniq[-1] + 1e-6))]\n",
    "\n",
    "    best = {\"thr\": 0.5, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    for thr in cuts:\n",
    "        pred1 = (prob_pos >= thr).astype(int)\n",
    "        pos = int(pred1.sum())\n",
    "        if pos < min_pos or pos > max_pos:\n",
    "            continue  # skip degenerate operating points\n",
    "\n",
    "        pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, pred1, average=\"binary\", pos_label=1, zero_division=0\n",
    "        )\n",
    "        if (f1 > best[\"f1\"] or\n",
    "            (f1 == best[\"f1\"] and pr > best[\"precision\"]) or\n",
    "            (f1 == best[\"f1\"] and pr == best[\"precision\"] and abs(thr - 0.5) < abs(best[\"thr\"] - 0.5))):\n",
    "            best = {\"thr\": float(thr), \"precision\": float(pr), \"recall\": float(rc), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Custom Trainer (weighted/focal)\n",
    "# ===========================\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, use_focal=False, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._class_weights = None\n",
    "        if class_weights is not None:\n",
    "            self._class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "        self.use_focal = use_focal\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self._class_weights is not None and self._class_weights.device != logits.device:\n",
    "            self._class_weights = self._class_weights.to(logits.device)\n",
    "\n",
    "        if not self.use_focal:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=self._class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Focal Loss\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            probs = torch.exp(log_probs)\n",
    "            pt = probs.gather(1, labels.unsqueeze(1)).squeeze(1)      # p_t\n",
    "            focal_factor = (1 - pt) ** self.gamma\n",
    "            nll = F.nll_loss(log_probs, labels, reduction='none', weight=self._class_weights)\n",
    "            loss = (focal_factor * nll).mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Main training function\n",
    "# ===========================\n",
    "def train_model(model_name, jpm_train, jpm_val, jpm_test,\n",
    "                max_len=MAX_LEN, freeze_backbone=False, seed=SEED,\n",
    "                use_focal=USE_FOCAL, focal_gamma=FOCAL_GAMMA,\n",
    "                minority_weight_mult=MINORITY_WEIGHT_MULT):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # --- Prepare splits (expects: question, answer, label) ---\n",
    "    for _, df in [('train', jpm_train), ('val', jpm_val), ('test', jpm_test)]:\n",
    "        df['text'] = prepare_text(df)\n",
    "        df['labels'] = normalise_labels(df['label'])\n",
    "        df.dropna(subset=['labels'], inplace=True)\n",
    "        df['labels'] = df['labels'].astype('int64')\n",
    "\n",
    "    train_ds = Dataset.from_pandas(jpm_train[[\"text\",\"labels\"]], preserve_index=False)\n",
    "    val_ds   = Dataset.from_pandas(jpm_val[[\"text\",\"labels\"]],   preserve_index=False)\n",
    "    test_ds  = Dataset.from_pandas(jpm_test[[\"text\",\"labels\"]],  preserve_index=False)\n",
    "\n",
    "    # --- Tokenization ---\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_len)\n",
    "\n",
    "    enc_train = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "    enc_val   = val_ds.map(tokenize,   batched=True, remove_columns=[\"text\"])\n",
    "    enc_test  = test_ds.map(tokenize,  batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    # --- Model ---\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2, problem_type='single_label_classification'\n",
    "    )\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'classifier' not in name and 'score' not in name:\n",
    "                p.requires_grad = False\n",
    "\n",
    "    # --- Class weights (amp minority) ---\n",
    "    y = np.array(train_ds['labels'])\n",
    "    cw = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y)\n",
    "    cw[1] = cw[1] * float(minority_weight_mult)\n",
    "    class_weights = cw\n",
    "\n",
    "    # --- Training args ---\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(RUNS_DIR, model_name.replace('/','_')),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=50,\n",
    "        learning_rate=3e-5 if \"roberta\" in model_name else 4e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.06,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=False,                      # MPS doesn't use fp16\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        seed=seed,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "    # --- Metrics (argmax) for trainer/early stopping ---\n",
    "    def compute_metrics(p):\n",
    "        preds = p.predictions.argmax(-1)\n",
    "        labels = p.label_ids\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        pr, rc, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", pos_label=1, zero_division=0)\n",
    "        try:\n",
    "            proba = torch.softmax(torch.tensor(p.predictions), dim=-1).numpy()[:,1]\n",
    "            auc = roc_auc_score(labels, proba)\n",
    "        except Exception:\n",
    "            auc = float('nan')\n",
    "        return {\"accuracy\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1, \"auc\": auc}\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=enc_train,\n",
    "        eval_dataset=enc_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights,\n",
    "        use_focal=use_focal,\n",
    "        gamma=focal_gamma,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    print(f\"\\nTraining {model_name}\")\n",
    "    trainer.train()\n",
    "\n",
    "    # --- Pick threshold on VAL (strict F1; avoid degenerate all-pos/neg) ---\n",
    "    val_pred = trainer.predict(enc_val)\n",
    "    val_probs = torch.softmax(torch.tensor(val_pred.predictions), dim=-1).numpy()[:, 1]\n",
    "    val_y = val_pred.label_ids\n",
    "\n",
    "    best = choose_threshold_by_f1_strict(val_y, val_probs, min_pos=1, max_pos=len(val_y)-1)\n",
    "    print(f\"Chosen threshold (F1-optimized): {best}\")\n",
    "\n",
    "    # Sanity: show how many positives at chosen threshold\n",
    "    val_pos = int((val_probs >= best[\"thr\"]).sum())\n",
    "    print(f\"VAL predicted positives at thr={best['thr']:.3f}: {val_pos}/{len(val_y)}\")\n",
    "\n",
    "    # --- Evaluate on TEST with chosen threshold ---\n",
    "    test_pred = trainer.predict(enc_test)\n",
    "    test_probs = torch.softmax(torch.tensor(test_pred.predictions), dim=-1).numpy()[:, 1]\n",
    "    test_y = test_pred.label_ids\n",
    "    test_pred1 = (test_probs >= best[\"thr\"]).astype(int)\n",
    "\n",
    "    tpr, trc, tf1, _ = precision_recall_fscore_support(test_y, test_pred1, average=\"binary\", pos_label=1, zero_division=0)\n",
    "    tacc = accuracy_score(test_y, test_pred1)\n",
    "    cm = confusion_matrix(test_y, test_pred1).tolist()\n",
    "\n",
    "    print(\"VAL (argmax):\", trainer.evaluate())\n",
    "    print(\"TEST (thresholded):\",\n",
    "          {\"accuracy\": tacc, \"precision\": tpr, \"recall\": trc, \"f1\": tf1,\n",
    "           \"threshold\": best[\"thr\"], \"confusion_matrix\": cm})\n",
    "    print(f\"TEST predicted positives at thr={best['thr']:.3f}: {int(test_pred1.sum())}/{len(test_y)}\")\n",
    "\n",
    "    # --- Save final model OUTSIDE the repo ---\n",
    "    save_path = os.path.join(MODELS_DIR, f\"{model_name.replace('/','_')}_direct_evasive\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    trainer.save_model(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "    return {\"val_threshold\": best, \"test_metrics\": {\"accuracy\": tacc, \"precision\": tpr, \"recall\": trc, \"f1\": tf1}}, (model, tokenizer)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Run (expects jpm_train/jpm_val/jpm_test to exist with columns: question, answer, label)\n",
    "# ===========================\n",
    "results = {}\n",
    "for model_name in ['distilroberta-base', 'microsoft/deberta-v3-small']:\n",
    "    res, _ = train_model(\n",
    "        model_name=model_name,\n",
    "        jpm_train=jpm_train,\n",
    "        jpm_val=jpm_val,\n",
    "        jpm_test=jpm_test,\n",
    "        max_len=MAX_LEN,\n",
    "        freeze_backbone=False,\n",
    "        seed=SEED,\n",
    "        use_focal=USE_FOCAL,\n",
    "        focal_gamma=FOCAL_GAMMA,\n",
    "        minority_weight_mult=MINORITY_WEIGHT_MULT\n",
    "    )\n",
    "    results[model_name] = res\n",
    "\n",
    "print(\"\\nSummary:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcba36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training distilroberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 107/107 [00:00<00:00, 2367.94 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 1868.50 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 1881.11 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56/140 00:27 < 00:42, 1.98 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.665799</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.519886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647968</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.551136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642327</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.514205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.635433</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.548295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (val-optimized): {'thr': 0.1, 'recall': 1.0, 'precision': 0.2558139534883721, 'f1': 0.4074074074074074}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL (argmax): {'eval_loss': 0.6657994985580444, 'eval_accuracy': 0.2558139534883721, 'eval_precision': 0.2558139534883721, 'eval_recall': 1.0, 'eval_f1': 0.4074074074074074, 'eval_auc': 0.5198863636363635, 'eval_runtime': 0.4842, 'eval_samples_per_second': 88.804, 'eval_steps_per_second': 6.196, 'epoch': 4.0}\n",
      "TEST (thresholded): {'accuracy': 0.13846153846153847, 'precision': 0.13846153846153847, 'recall': 1.0, 'f1': 0.24324324324324326, 'threshold': 0.1}\n",
      "Model saved to: /Users/laurenbrixey/trained_models/evasion/distilroberta-base_direct_evasive\n",
      "\n",
      "Training microsoft/deberta-v3-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 107/107 [00:00<00:00, 4098.06 examples/s]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 4341.72 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 4086.05 examples/s]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70/140 00:52 < 00:53, 1.31 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697952</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646467</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.414773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634962</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.434659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.629265</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.465909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.661363</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.414773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (val-optimized): {'thr': 0.1, 'recall': 1.0, 'precision': 0.2558139534883721, 'f1': 0.4074074074074074}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL (argmax): {'eval_loss': 0.6464669704437256, 'eval_accuracy': 0.2558139534883721, 'eval_precision': 0.2558139534883721, 'eval_recall': 1.0, 'eval_f1': 0.4074074074074074, 'eval_auc': 0.4147727272727273, 'eval_runtime': 0.9102, 'eval_samples_per_second': 47.241, 'eval_steps_per_second': 3.296, 'epoch': 5.0}\n",
      "TEST (thresholded): {'accuracy': 0.13846153846153847, 'precision': 0.13846153846153847, 'recall': 1.0, 'f1': 0.24324324324324326, 'threshold': 0.1}\n",
      "Model saved to: /Users/laurenbrixey/trained_models/evasion/microsoft_deberta-v3-small_direct_evasive\n"
     ]
    }
   ],
   "source": [
    "# import os, numpy as np, torch\n",
    "# import torch.nn.functional as F\n",
    "# from datasets import Dataset\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForSequenceClassification,\n",
    "#     Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "# )\n",
    "# from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# # --- 1) Keep EVERYTHING out of your repo ------------------------------------\n",
    "# os.environ[\"HF_HOME\"] = os.path.expanduser(\"~/hf_cache\")               # models/tokenizers cache\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = os.path.expanduser(\"~/hf_datasets\")  # datasets cache\n",
    "\n",
    "# RUNS_DIR   = os.path.expanduser(\"~/ml_runs/evasion\")                    # trainer checkpoints\n",
    "# MODELS_DIR = os.path.expanduser(\"~/trained_models/evasion\")             # final saved models\n",
    "# os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "# os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# # --- 2) Data prep ------------------------------------------------------------\n",
    "# def prepare_text(df):\n",
    "#     return (df[\"question\"].fillna(\"\") + \" </s> \" + df[\"answer\"].fillna(\"\"))\n",
    "\n",
    "# label_map = {'direct': 0, 'evasive': 1}\n",
    "# def normalise_labels(s):\n",
    "#     return s.astype(str).str.lower().map(label_map)\n",
    "\n",
    "# for split_name, df in [('train', jpm_train), ('val', jpm_val), ('test', jpm_test)]:\n",
    "#     df['text'] = prepare_text(df)\n",
    "#     df['labels'] = normalise_labels(df['label'])\n",
    "#     df.dropna(subset=['labels'], inplace=True)\n",
    "#     df['labels'] = df['labels'].astype('int64')\n",
    "\n",
    "# jpm_train_llm = Dataset.from_pandas(jpm_train[[\"text\",\"labels\"]], preserve_index=False)\n",
    "# jpm_val_llm   = Dataset.from_pandas(jpm_val[[\"text\",\"labels\"]],   preserve_index=False)\n",
    "# jpm_test_llm  = Dataset.from_pandas(jpm_test[[\"text\",\"labels\"]],  preserve_index=False)\n",
    "\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# # --- 3) Trainer with weighted CE or focal loss ------------------------------\n",
    "# class WeightedTrainer(Trainer):\n",
    "#     def __init__(self, class_weights=None, use_focal=False, gamma=2.0, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self._class_weights = None\n",
    "#         if class_weights is not None:\n",
    "#             self._class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "#         self.use_focal = use_focal\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "#         labels = inputs.get(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get(\"logits\")\n",
    "\n",
    "#         if self._class_weights is not None and self._class_weights.device != logits.device:\n",
    "#             self._class_weights = self._class_weights.to(logits.device)\n",
    "\n",
    "#         if not self.use_focal:\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss(weight=self._class_weights)\n",
    "#             loss = loss_fct(logits, labels)\n",
    "#         else:\n",
    "#             log_probs = F.log_softmax(logits, dim=-1)\n",
    "#             probs = torch.exp(log_probs)\n",
    "#             pt = probs.gather(1, labels.unsqueeze(1)).squeeze(1)      # p_t\n",
    "#             focal_factor = (1 - pt) ** self.gamma\n",
    "#             nll = F.nll_loss(log_probs, labels, reduction='none', weight=self._class_weights)\n",
    "#             loss = (focal_factor * nll).mean()\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# # --- 4) Training loop with fixes + thresholding -----------------------------\n",
    "# def train_model(model_name, max_len=256, freeze_backbone=False, seed=42, use_focal=False):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "#     def tokenize(batch):\n",
    "#         return tokenizer(\n",
    "#             batch['text'],\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             max_length=max_len\n",
    "#         )\n",
    "\n",
    "#     enc_train = jpm_train_llm.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "#     enc_val   = jpm_val_llm.map(tokenize,   batched=True, remove_columns=[\"text\"])\n",
    "#     enc_test  = jpm_test_llm.map(tokenize,  batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         model_name, num_labels=2, problem_type='single_label_classification'\n",
    "#     )\n",
    "\n",
    "#     if freeze_backbone:\n",
    "#         for name, p in model.named_parameters():\n",
    "#             if 'classifier' not in name and 'score' not in name:\n",
    "#                 p.requires_grad = False\n",
    "\n",
    "#     # Class weights (make minority class hit harder)\n",
    "#     y = np.array(jpm_train_llm['labels'])\n",
    "#     cw = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y)\n",
    "#     cw[1] = cw[1] * 2.0  # try 2.0–3.0 if needed\n",
    "#     class_weights = cw\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=os.path.join(RUNS_DIR, model_name.replace('/','_')),\n",
    "#         eval_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         save_total_limit=2,\n",
    "#         logging_steps=50,\n",
    "#         learning_rate=3e-5 if \"roberta\" in model_name else 4e-5,\n",
    "#         weight_decay=0.01,\n",
    "#         warmup_ratio=0.06,\n",
    "#         gradient_accumulation_steps=1,\n",
    "#         per_device_train_batch_size=8,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         num_train_epochs=10,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1\",\n",
    "#         greater_is_better=True,\n",
    "#         fp16=False,                         # MPS doesn't use fp16\n",
    "#         report_to=\"none\",\n",
    "#         remove_unused_columns=False,\n",
    "#         seed=seed,\n",
    "#         dataloader_pin_memory=False,\n",
    "#     )\n",
    "\n",
    "#     def compute_metrics(p):\n",
    "#         preds = p.predictions.argmax(-1)\n",
    "#         labels = p.label_ids\n",
    "#         acc = accuracy_score(labels, preds)\n",
    "#         pr, rc, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", pos_label=1, zero_division=0)\n",
    "#         try:\n",
    "#             proba = torch.softmax(torch.tensor(p.predictions), dim=-1).numpy()[:,1]\n",
    "#             auc = roc_auc_score(labels, proba)\n",
    "#         except Exception:\n",
    "#             auc = float('nan')\n",
    "#         return {\"accuracy\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1, \"auc\": auc}\n",
    "\n",
    "#     trainer = WeightedTrainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         train_dataset=enc_train,\n",
    "#         eval_dataset=enc_val,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         class_weights=class_weights,\n",
    "#         use_focal=use_focal,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "#     # --- Post-training: choose threshold on VAL to maximize recall (then f1) ---\n",
    "#     val_pred = trainer.predict(enc_val)\n",
    "#     val_probs = torch.softmax(torch.tensor(val_pred.predictions), dim=-1).numpy()[:,1]\n",
    "#     val_y = val_pred.label_ids\n",
    "\n",
    "#     best = {\"thr\":0.5, \"recall\":-1.0, \"precision\":0.0, \"f1\":0.0}\n",
    "#     for thr in np.linspace(0.1, 0.9, 17):\n",
    "#         pred1 = (val_probs >= thr).astype(int)\n",
    "#         pr, rc, f1, _ = precision_recall_fscore_support(val_y, pred1, average=\"binary\", pos_label=1, zero_division=0)\n",
    "#         if (rc > best[\"recall\"]) or (rc == best[\"recall\"] and f1 > best[\"f1\"]):\n",
    "#             best = {\"thr\":float(thr), \"recall\":float(rc), \"precision\":float(pr), \"f1\":float(f1)}\n",
    "#     print(f\"Chosen threshold (val-optimized): {best}\")\n",
    "\n",
    "#     # --- Evaluate on TEST with chosen threshold ---\n",
    "#     test_pred = trainer.predict(enc_test)\n",
    "#     test_probs = torch.softmax(torch.tensor(test_pred.predictions), dim=-1).numpy()[:,1]\n",
    "#     test_y = test_pred.label_ids\n",
    "#     test_pred1 = (test_probs >= best[\"thr\"]).astype(int)\n",
    "#     tpr, trc, tf1, _ = precision_recall_fscore_support(test_y, test_pred1, average=\"binary\", pos_label=1, zero_division=0)\n",
    "#     tacc = accuracy_score(test_y, test_pred1)\n",
    "#     print(\"VAL (argmax):\", trainer.evaluate())\n",
    "#     print(\"TEST (thresholded):\", {\"accuracy\": tacc, \"precision\": tpr, \"recall\": trc, \"f1\": tf1, \"threshold\": best[\"thr\"]})\n",
    "\n",
    "#     # --- Save final model OUTSIDE the repo ---\n",
    "#     save_path = os.path.join(MODELS_DIR, f\"{model_name.replace('/','_')}_direct_evasive\")\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#     trainer.save_model(save_path)\n",
    "#     tokenizer.save_pretrained(save_path)\n",
    "#     print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "#     return model, tokenizer\n",
    "\n",
    "# # --- 5) Train the models -----------------------------------------------------\n",
    "# models = [\n",
    "#     'distilroberta-base',\n",
    "#     'microsoft/deberta-v3-small'\n",
    "# ]\n",
    "\n",
    "# for m in models:\n",
    "#     print(f\"\\nTraining {m}\")\n",
    "#     _ = train_model(m, max_len=256, freeze_backbone=False, seed=42, use_focal=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90ed9f",
   "metadata": {},
   "source": [
    "## **4.2 Functions for pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_max_len(tokenizer, model):\n",
    "    m = getattr(tokenizer, \"model_max_length\", None)\n",
    "    if m is None or m == int(1e30):\n",
    "        m = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", 512)\n",
    "    return int(m or 512)\n",
    "\n",
    "def token_len(tokenizer, text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def compute_answer_budget(tokenizer, model, question, hyp_max_tokens, q_cap=128, safety_margin=12):\n",
    "    max_len = model_max_len(tokenizer, model)            # usually 512\n",
    "    specials = tokenizer.num_special_tokens_to_add(pair=True)\n",
    "    q_tokens = min(token_len(tokenizer, question), q_cap)\n",
    "    budget = max_len - specials - q_tokens - hyp_max_tokens - safety_margin\n",
    "    return max(32, budget)\n",
    "\n",
    "def chunk_answer_for_pair(tokenizer, answer, answer_budget, stride_tokens=128):\n",
    "    \"\"\"\n",
    "    Chunk the ANSWER using tokenizer.tokenize (no model max-length checks),\n",
    "    then stitch back to text with convert_tokens_to_string.\n",
    "    \"\"\"\n",
    "    toks = tokenizer.tokenize(answer)  # <-- avoids the max-length warning\n",
    "    if len(toks) <= answer_budget:\n",
    "        return [answer]\n",
    "\n",
    "    chunks, i = [], 0\n",
    "    while i < len(toks):\n",
    "        window_tokens = toks[i:i+answer_budget]\n",
    "        window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "        chunks.append(window_text)\n",
    "        if i + answer_budget >= len(toks):\n",
    "            break\n",
    "        i += max(1, answer_budget - stride_tokens)\n",
    "    return chunks\n",
    "\n",
    "def pair_logits_chunks(model, tokenizer, device, premise, hypothesis, max_length=None, stride=128):\n",
    "    if max_length is None:\n",
    "        max_length = model_max_len(tokenizer, model)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        premise,\n",
    "        hypothesis,\n",
    "        return_tensors='pt',\n",
    "        truncation='only_first',          # split/truncate Q+A only\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding='max_length'              # <-- add this\n",
    "    )\n",
    "\n",
    "    # keep only keys the model expects\n",
    "    input_names = set(getattr(tokenizer, \"model_input_names\",\n",
    "                              [\"input_ids\", \"attention_mask\", \"token_type_ids\"]))\n",
    "\n",
    "    def to_batch(enc_dict, i=None):\n",
    "        batch = {}\n",
    "        for k, v in enc_dict.items():\n",
    "            if k in input_names and isinstance(v, torch.Tensor):\n",
    "                batch[k] = (v[i:i+1] if i is not None else v).to(device)\n",
    "        return batch\n",
    "\n",
    "    # single chunk\n",
    "    if enc[\"input_ids\"].shape[0] == 1:\n",
    "        batch = to_batch(enc)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch).logits\n",
    "        return [logits.squeeze(0)]\n",
    "\n",
    "    # multiple overflowed chunks\n",
    "    logits_list = []\n",
    "    n = enc[\"input_ids\"].shape[0]\n",
    "    for i in range(n):\n",
    "        batch = to_batch(enc, i)\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch).logits\n",
    "        logits_list.append(out.squeeze(0))\n",
    "    return logits_list\n",
    "\n",
    "def get_label_idx(model, name, default):\n",
    "    id2label = getattr(model.config, \"id2label\", {})\n",
    "    if id2label:\n",
    "        for k, v in id2label.items():\n",
    "            if name in str(v).lower():\n",
    "                return int(k)\n",
    "    return default\n",
    "\n",
    "def p_entail_from_logits(logits, model, temperature=1.0):\n",
    "    nlab = logits.shape[-1]\n",
    "    ent_i = get_label_idx(model, \"entail\", 2 if nlab==3 else 1)\n",
    "    probs = torch.softmax(logits / float(temperature), dim=-1)\n",
    "    return float(probs[ent_i])\n",
    "\n",
    "# --- your templates (unchanged) ---\n",
    "DIRECT_TEMPLATES = [\n",
    "    \"The answer gives a direct and specific response to the question.\",\n",
    "    \"The answer addresses the question explicitly and concretely.\",\n",
    "    \"The answer responds directly with actionable specifics.\",\n",
    "]\n",
    "EVASIVE_TEMPLATES = [\n",
    "    \"The answer avoids giving a direct response to the question.\",\n",
    "    \"The answer is evasive or deflects without specifics.\",\n",
    "    \"The answer sidesteps the question and withholds details.\",\n",
    "]\n",
    "\n",
    "def llm_evasion_score(question, answer, model, tokenizer, device, temperature=2.0, stride=128):\n",
    "    max_len = model_max_len(tokenizer, model)\n",
    "    n_dir, n_eva = len(DIRECT_TEMPLATES), len(EVASIVE_TEMPLATES)\n",
    "\n",
    "    p_ent_direct_list, p_ent_evasive_list = [], []\n",
    "\n",
    "    premise = f\"Q: {question}\\nA: {answer}\"\n",
    "\n",
    "    # Collect P(entailment) for DIRECT hypotheses (over chunks), then mean over templates\n",
    "    for h in DIRECT_TEMPLATES:\n",
    "        logits_chunks = pair_logits_chunks(model, tokenizer, device, premise, h, max_length=max_len, stride=stride)\n",
    "        # For each chunk, compute P(entail); take the max across chunks (recall-friendly)\n",
    "        pents = [p_entail_from_logits(log, model, temperature) for log in logits_chunks]\n",
    "        p_ent_direct_list.append(max(pents))\n",
    "\n",
    "    # Same for EVASIVE hypotheses\n",
    "    for h in EVASIVE_TEMPLATES:\n",
    "        logits_chunks = pair_logits_chunks(model, tokenizer, device, premise, h, max_length=max_len, stride=stride)\n",
    "        pents = [p_entail_from_logits(log, model, temperature) for log in logits_chunks]\n",
    "        p_ent_evasive_list.append(max(pents))\n",
    "\n",
    "    # Mean over templates\n",
    "    p_ent_direct  = float(torch.tensor(p_ent_direct_list).mean())\n",
    "    p_ent_evasive = float(torch.tensor(p_ent_evasive_list).mean())\n",
    "\n",
    "    # Neutral-aware normalization (don’t force a 2-class softmax over logits)\n",
    "    denom = p_ent_evasive + p_ent_direct + 1e-9\n",
    "    p_evasive = float(p_ent_evasive / denom)\n",
    "    p_direct  = 1.0 - p_evasive\n",
    "\n",
    "    return {\n",
    "        'p_direct': p_direct,\n",
    "        'p_evasive': p_evasive,\n",
    "        'p_ent_direct': p_ent_direct,\n",
    "        'p_ent_evasive': p_ent_evasive\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af43fd",
   "metadata": {},
   "source": [
    "# **5. Evasion Detection Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac468677",
   "metadata": {},
   "source": [
    "## **5.1 Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute blended evasion score and return all scores.\n",
    "def compute_all_evasion_scores(q, a, *, models_and_tokenizers=models_and_tokenizers, device, LLM_WEIGHT=0.30):\n",
    "    \n",
    "    # Compute baseline evasion score.\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # Individual LLM scores.\n",
    "    llm_scores = {}\n",
    "    for name, (m, t) in models_and_tokenizers.items():\n",
    "        scores = llm_evasion_score(q, a, m, t, device)\n",
    "        llm_scores[name] = float(100.0 * scores['p_evasive'])\n",
    "\n",
    "    # Ensemble LLM score.\n",
    "    llm_avg = float(np.mean(list(llm_scores.values()))) if llm_scores else 0.0\n",
    "\n",
    "    # Compute blended score.\n",
    "    blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * llm_avg, 0.0, 100.0))\n",
    "\n",
    "    return {\n",
    "        'baseline': base_score,\n",
    "        'llm_individual': llm_scores,\n",
    "        'llm_avg': llm_avg,\n",
    "        'blended': blended_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label 'Direct' or 'Evasive' based on the score.\n",
    "def label_from_score(score, threshold):\n",
    "    return 'Evasive' if score >= threshold else 'Direct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607eb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evasion Pipeline.\n",
    "def evasion_pipeline(df, models_and_tokenizers, device, LLM_WEIGHT, EVASION_THRESHOLD_BASE, EVASION_THRESHOLD_LLM, EVASION_THRESHOLD_BLENDED):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row['question']), str(row['answer'])\n",
    "        output = compute_all_evasion_scores(q=q, a=a, LLM_WEIGHT=LLM_WEIGHT, models_and_tokenizers=models_and_tokenizers, device=device)\n",
    "\n",
    "        pred_base = label_from_score(output['baseline'], EVASION_THRESHOLD_BASE)\n",
    "        pred_llm_avg = label_from_score(output['llm_avg'], EVASION_THRESHOLD_LLM)\n",
    "        pred_blended = label_from_score(output['blended'], EVASION_THRESHOLD_BLENDED)\n",
    "\n",
    "        record = {\n",
    "            'question_number': row.get('question_number'),\n",
    "            'question': q,\n",
    "            'answer': a,\n",
    "\n",
    "            # Evasion Scores\n",
    "            'evasion_score_baseline': int(output['baseline']),\n",
    "            'evasion_score_llm_avg': int(output['llm_avg']),\n",
    "            \"evasion_score_blended\": int(output['blended']),\n",
    "\n",
    "            # Predicted labels.\n",
    "            'prediction_baseline': pred_base,\n",
    "            'prediction_llm_avg': pred_llm_avg,\n",
    "            'prediction_blended': pred_blended,\n",
    "        }\n",
    "\n",
    "        for model_name, score in output['llm_individual'].items():\n",
    "            record[f'evasion_score_{model_name}'] = int(score)\n",
    "            record[f'prediction_{model_name}'] = label_from_score(score, EVASION_THRESHOLD_LLM)\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58f2c8",
   "metadata": {},
   "source": [
    "## **5.2 Threshold Tuning & Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an initial run with preliminary threshold values.\n",
    "LLM_WEIGHT = 0.30\n",
    "EVASION_THRESHOLD_BASE = 30.0\n",
    "EVASION_THRESHOLD_LLM = 30.0\n",
    "EVASION_THRESHOLD_BLENDED = 30.0\n",
    "\n",
    "jpm_val_qa_scores = evasion_pipeline(\n",
    "    jpm_val_qa_labelled, \n",
    "    models_and_tokenizers, \n",
    "    device, \n",
    "    LLM_WEIGHT, \n",
    "    EVASION_THRESHOLD_BASE, \n",
    "    EVASION_THRESHOLD_LLM, \n",
    "    EVASION_THRESHOLD_BLENDED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the results and reappend the label.\n",
    "jpm_val_qa_scores['label'] = jpm_val_qa_labelled['label'].values\n",
    "jpm_val_qa_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4555b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth (1 = Evasive, 0 = Direct)\n",
    "def extract_y_true(df):\n",
    "    return (df['label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculate metrics for each threshold.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def tune_threshold(df, score_col, thr_grid):\n",
    "    y_true = extract_y_true(df)                     # get true labels\n",
    "    scores = df[score_col].astype(float).values     # get raw evasion scores \n",
    "\n",
    "    rows = []\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (scores >= thr).astype(int) # label response evasive (1) if score is higher than threshold\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        rows.append({\n",
    "            'threshold': float(thr),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    results = pd.DataFrame(rows).sort_values(\n",
    "        by=['f1', 'recall'],\n",
    "        ascending=[False, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold ranges around current thresholds.\n",
    "thr_base_grid = np.arange(40, 85, 5)\n",
    "thr_llm_grid = np.arange(35, 85, 5)\n",
    "thr_blend_grid = np.arange(40, 85, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline / blended / avg LLM \n",
    "base_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_baseline', thr_base_grid)\n",
    "llm_avg_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_llm_avg', thr_llm_grid)\n",
    "blend_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_blended', thr_blend_grid)\n",
    "\n",
    "# Individual LLM models\n",
    "roberta_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_roberta', thr_llm_grid)\n",
    "deberta_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_deberta', thr_llm_grid)\n",
    "zs_deberta_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_zs_deberta', thr_llm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68425e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Baseline Threshold: 40.0\n",
      "Best avg LLM Threshold: 50.0\n",
      "Best Blended Threshold 40.0\n",
      "Best roberta Threshold: 35.0\n",
      "Best deberta Threshold: 60.0\n",
      "Best zs deberta Threshold 55.0\n"
     ]
    }
   ],
   "source": [
    "# Extract the best thresholds based on recall.\n",
    "best_base_thr = base_results.loc[0, 'threshold']\n",
    "best_avg_llm_thr = llm_avg_results.loc[0, 'threshold']\n",
    "best_blend_thr = blend_results.loc[0, 'threshold']\n",
    "\n",
    "best_roberta_thr = roberta_results.loc[0, 'threshold']\n",
    "best_deberta_thr = deberta_results.loc[0, 'threshold']\n",
    "best_zs_derberta_thr = zs_deberta_results.loc[0, 'threshold']\n",
    "\n",
    "print('Best Baseline Threshold:', best_base_thr)\n",
    "print('Best avg LLM Threshold:', best_avg_llm_thr)\n",
    "print('Best Blended Threshold', best_base_thr)\n",
    "\n",
    "print('Best roberta Threshold:', best_roberta_thr)\n",
    "print('Best deberta Threshold:', best_deberta_thr)\n",
    "print('Best zs deberta Threshold', best_zs_derberta_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbf838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 baseline configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       40.0   0.208955  1.000000  0.345679  0.258741\n",
      "1       45.0   0.183486  0.714286  0.291971  0.321678\n",
      "2       65.0   0.209677  0.464286  0.288889  0.552448\n",
      "3       70.0   0.236842  0.321429  0.272727  0.664336\n",
      "4       55.0   0.177778  0.571429  0.271186  0.398601\n",
      "\n",
      "Top 5 llm configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       50.0   0.211538  0.785714  0.333333  0.384615\n",
      "1       35.0   0.198529  0.964286  0.329268  0.230769\n",
      "2       40.0   0.198413  0.892857  0.324675  0.272727\n",
      "3       55.0   0.215190  0.607143  0.317757  0.489510\n",
      "4       45.0   0.198198  0.785714  0.316547  0.335664\n",
      "\n",
      "Top 5 blended configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       40.0   0.201439  1.000000  0.335329  0.223776\n",
      "1       65.0   0.250000  0.464286  0.325000  0.622378\n",
      "2       45.0   0.193548  0.857143  0.315789  0.272727\n",
      "3       50.0   0.183486  0.714286  0.291971  0.321678\n",
      "4       55.0   0.184783  0.607143  0.283333  0.398601\n",
      "\n",
      "Top 5 roberta configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       35.0   0.212389  0.857143  0.340426  0.349650\n",
      "1       40.0   0.195876  0.678571  0.304000  0.391608\n",
      "2       45.0   0.200000  0.500000  0.285714  0.510490\n",
      "3       55.0   0.250000  0.285714  0.266667  0.692308\n",
      "4       50.0   0.204545  0.321429  0.250000  0.622378\n",
      "\n",
      "Top 5 deberta configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       60.0   0.236559  0.785714  0.363636  0.461538\n",
      "1       65.0   0.232877  0.607143  0.336634  0.531469\n",
      "2       40.0   0.200000  1.000000  0.333333  0.216783\n",
      "3       35.0   0.197183  1.000000  0.329412  0.202797\n",
      "4       45.0   0.188406  0.928571  0.313253  0.202797\n",
      "\n",
      "Top 5 zs deberta configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       55.0   0.222222  0.714286  0.338983  0.454545\n",
      "1       70.0   0.245902  0.535714  0.337079  0.587413\n",
      "2       50.0   0.208333  0.714286  0.322581  0.412587\n",
      "3       60.0   0.214286  0.642857  0.321429  0.468531\n",
      "4       45.0   0.200000  0.750000  0.315789  0.363636\n"
     ]
    }
   ],
   "source": [
    "# Inspect trade-offs.\n",
    "print('\\nTop 5 baseline configs:\\n', base_results.head())\n",
    "print('\\nTop 5 llm configs:\\n', llm_avg_results.head())\n",
    "print('\\nTop 5 blended configs:\\n', blend_results.head())\n",
    "\n",
    "print('\\nTop 5 roberta configs:\\n', roberta_results.head())\n",
    "print('\\nTop 5 deberta configs:\\n', deberta_results.head())\n",
    "print('\\nTop 5 zs deberta configs:\\n', zs_deberta_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c08171",
   "metadata": {},
   "source": [
    "## **5.2 Optimised Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ef6c7",
   "metadata": {},
   "source": [
    "## **5.3 2025 Predictions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
