{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b820ac6",
   "metadata": {},
   "source": [
    "# **Summarisation & Evasion Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e2962",
   "metadata": {},
   "source": [
    "# **Handover Notes:** [delete after]\n",
    "- Library imports and versions are saved in environments/summarisation_evasion_env.txt\n",
    "- This notebook was originally built for a macbook pro M3 chip so some settings may need to be altered depending on your machine\n",
    "- All files related/ generated by this notebook can be found in notebooks/summarisation_evasion_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b76a8",
   "metadata": {},
   "source": [
    "### **Work progress**\n",
    "1. **Complete**\n",
    "- Summarise banker answers using baseline model.\n",
    "- Use Local RAG pipeline to bring in relevant external documents (PRA risk definitions) to create PRA aligned summaries.\n",
    "- Developed a evasion detection prototype that generates evasion scores based on bankers answers (uses baseline model, LLM- natural language inference using RoBERTa and a blended score)\n",
    "- Used jpm_2025 transcripts to get the pipeline working. Validated the evasion pipeline using jpm-23-1q data (involved human labelling the answer as Direct or Evasive- file saved in notebooks/summarisation_evasion_files).\n",
    "\n",
    "2. **Not complete**\n",
    "- Visualisations e.g. how many evasive answers were there? etc - apply evasion pipeline to dataset and generate statistics on evasiveness \n",
    "- Need to test pipleine on larger data set (e.g. jpm 2023-2025) and check against HSBC to make conclusions & comment on generalisability (answering research question: How does one bankâ€™s tone and thematic profile compare to peers? Are divergences systemic or firm specific?)\n",
    "- Summarisation pipeline could be improved using a two-stage pipeline: by first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned.\n",
    "- Post-processing on the output file for the PRA aligned summaries by Mistral model so they are clearer- can this output be fed into another model to extract more insights/ detect evasion or risk?\n",
    "- Increase the size of the validation set for the evasion pipeline prototype (e.g. more human labelling)\n",
    "- Need to fine tune the evasion pipeline to increase accuracy\n",
    "- Optional extensions e.g. using Agents, more complex RAG pipeline (including more useful context for the model), validation of instances of evasion using external news sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e874e23",
   "metadata": {},
   "source": [
    "# 1. **Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670282",
   "metadata": {},
   "source": [
    "# **2. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f5884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "import math\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set SEED.\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91bb8c",
   "metadata": {},
   "source": [
    "# **3. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743f6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  ...                                         source_pdf\n",
       "0                1  ...  data/raw/jpm/jpm-1q25-earnings-call-transcript...\n",
       "1                1  ...  data/raw/jpm/jpm-1q25-earnings-call-transcript...\n",
       "2                2  ...  data/raw/jpm/jpm-1q25-earnings-call-transcript...\n",
       "3                2  ...  data/raw/jpm/jpm-1q25-earnings-call-transcript...\n",
       "4                2  ...  data/raw/jpm/jpm-1q25-earnings-call-transcript...\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "jpm_2025_df = pd.read_csv('../data/processed/jpm/all_jpm_2025.csv')\n",
    "\n",
    "# View the data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75628c",
   "metadata": {},
   "source": [
    "# **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b70dc",
   "metadata": {},
   "source": [
    "- Used all_jpm_2025.csv dataset\n",
    "- Preliminary preprocessing to label roles as analyst vs banker (invalid roles were corrected) to make downstream analysis easier. Created a new column 'role_normalised'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc49b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec70c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_number  ...                                         source_pdf\n",
       "201               35  ...  data/raw/jpm/jpm-2q25-earnings-call-transcript...\n",
       "205               36  ...  data/raw/jpm/jpm-2q25-earnings-call-transcript...\n",
       "\n",
       "[2 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles.\n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = jpm_2025_df[~jpm_2025_df['role'].isin(valid_roles)]\n",
    "\n",
    "# Number of rows with invalid roles.\n",
    "print('Number of rows:', invalid_roles_df.shape[0])\n",
    "\n",
    "# View the rows.\n",
    "invalid_roles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e31cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the correct role information.\n",
    "jpm_2025_df.at[205, 'role'] = 'Chief Financial Officer'\n",
    "jpm_2025_df.at[209, 'role'] = 'Chief Financial Officer'\n",
    "\n",
    "# Verify the roles have been updated.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_2025_df['role_normalised'] = jpm_2025_df['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755f26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  ...  role_normalised\n",
       "0                1  ...          analyst\n",
       "1                1  ...           banker\n",
       "2                2  ...          analyst\n",
       "3                2  ...           banker\n",
       "4                2  ...           banker\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf565",
   "metadata": {},
   "source": [
    "# **5. Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401764b",
   "metadata": {},
   "source": [
    "## **5.1 Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee15b",
   "metadata": {},
   "source": [
    "- Initial model exploration using BART and mistral-7B-instruct to summarise banker's answers (no additional context given to model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92395bc",
   "metadata": {},
   "source": [
    "### **5.1.1 BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the w\n"
     ]
    }
   ],
   "source": [
    "# Filter data to banker answers only.\n",
    "banker_answers = jpm_2025_df[jpm_2025_df['role_normalised'] == 'banker']['content'].tolist()\n",
    "print(banker_answers[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54559054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The main thing that we see there, what would appear to be a certain amount of frontloading of spending ahead of people expecting price increases from tariffs. So ironically, that's actually somewhat supportive, all else equal. In terms of our corporate clients, obviously, they've been reacting to the changes in tariff policy.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (BART)\n",
    "bart = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "sample_text = banker_answers[0]\n",
    "summary_bart = bart(sample_text, max_length=80, min_length=30, do_sample=False)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f0d9b",
   "metadata": {},
   "source": [
    "- bart was able to extract ket ideas, focussing on fronloading of spending and tariff policy. \n",
    "- Compressed the response into two sentences and the summary is coherent, removing filler phrases.\n",
    "- However, the summary is not fully neutral (e.g. includes ironically) and preserves tone\n",
    "- Also there is a loss of context- e.g. consumer side vs wholesale side distinction is no longer explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024bd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: Corporates are taking a wait-and-see approach to tariff policy. Some sectors are going to be much more exposed than others. Small business and smaller corporates are probably a little more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Prompt conditioning to make PRA relevant.\n",
    "prompt = \"Summarise this answer, focusing on risk, capital and evasion of detail: \" + sample_text\n",
    "summary_bart_prompted = bart(prompt, max_length=80, min_length=30)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_bart_prompted[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e7996",
   "metadata": {},
   "source": [
    "- Prompted summary shifts emphasis and includes interpretation around risk, even though those words were no explicit in the original\n",
    "- This version is more aligned to evasion detection but moves away from concrete detail \n",
    "- Improved approach would be to have a two stage-pipeline: first extractive summarisation to capture the context and details and then a second model to reframe the summary to be PRA and evasion aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaabd6",
   "metadata": {},
   "source": [
    "### **5.1.2 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f4b90",
   "metadata": {},
   "source": [
    "- Mistral model: mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Mistral-7B-Instruct model download: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF?show_file_info=mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
    "- Also saved in shared team folder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a368c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides. On the consumer side, there has been some frontloading of spending ahead of expected price increases from tariffs, which may distort the data and make it difficult to draw larger conclusions. On the corporate side, clients are reacting to changes in tariff policy by shifting their focus towards short-term work and optimizing supply chains. The speaker characterizes the attitude of corporate clients as a wait-and-see attitude, with smaller clients and smaller corporates being more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with basic prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral = output['choices'][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec467d",
   "metadata": {},
   "source": [
    "- Preserves details and nuance and is more contextual and interpretive than the BART baseline model.\n",
    "- However, the result is longer with heavier phrasing and includes phrases like 'distort the data' which is not explicit in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b0f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The speaker is discussing the impact of recent news flow on the consumer and corporate sides of their business. On the consumer side, they have observed some frontloading of spending ahead of expected price increases from tariffs, which may distort data and make it difficult to draw larger conclusions. On the corporate side, clients are shifting their focus towards optimizing supply chains and responding to the current environment, rather than prioritizing more strategic work. The speaker notes that smaller clients and smaller corporates may be more challenged than larger ones, which have more experience dealing with these types of changes and more resources to manage them. Overall, the speaker suggests that it is difficult to make long-term decisions at this time due to the uncertainty surrounding the current environment.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (Mistral-7B-Instruct) with more detailed prompt.\n",
    "llm = Llama(model_path='/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
    "            n_ctx=4096, n_gpu_layers=-1, verbose=False, seed=SEED)  # change path as needed \n",
    "\n",
    "prompt = f\"<s>[INST] Summarise the following answer in 2 sentences, focusing on concrete facts. Avoid opinions. Focus on risk, capital and evasion of detail.\\n\\n{sample_text}\\n[/INST]\"\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    max_tokens=180,\n",
    "    temperature=0.1,\n",
    "    stop=['</s>']\n",
    ")\n",
    "\n",
    "summary_mistral_prompted = output['choices'][0]['message']['content'].strip()  \n",
    "\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary_mistral_prompted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1e33d",
   "metadata": {},
   "source": [
    "- This summary brings in risk- language and is closer to the task objective.\n",
    "- However, some interpretations are generated by the model rather than explicitly detailed in the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813acc2",
   "metadata": {},
   "source": [
    "## **5.2 Adding Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1b7a",
   "metadata": {},
   "source": [
    "Retrieve PRA risk categories to give greater PRA focus to summaries (local RAG loop).\n",
    "- measure cosine similarity between transcript chunks and PRA risk categories (vectors)\n",
    "- retrieve the top 2-3 most relevant risk categories \n",
    "- prepend them to the summarisation prompt to make summaries PRA-aligned instead of just summarised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2bd1",
   "metadata": {},
   "source": [
    "- Attempting to use BART resulted in prompt echoing.\n",
    "- New attempt using Mistral-7B-Instruct.\n",
    "- Using sentence-BERT vs TF-IDF for vectorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a9dab",
   "metadata": {},
   "source": [
    "### **5.2.1 Mistral-7B-Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56464c24",
   "metadata": {},
   "source": [
    "**Process**\n",
    "- Performed some light cleaning of the transcript to remove whitespace.\n",
    "- Split the transcript into smaller chunks that the model can summarise to avoid truncation\n",
    "- Loaded the PRA categories csv file (contains category and definition)\n",
    "- Embedded the PRA categories and chunks, evaluated the similarity to extract the PRA risk categories that were relevant to the text\n",
    "- Summarised the chunk using detailed prompted and relevant PRA categories as additional context. \n",
    "\n",
    "**Output File**:\n",
    "- The output file of this can be found in notebooks/summarisation_evasion_files, name = jpm_mistral_pra_summary.json\n",
    "- It is in the format: summary, evidence, PRA category that relates to summary and reasoning for selecting these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e695d9e",
   "metadata": {},
   "source": [
    "- Needed to use a lot of fine tuning for the prompt and set strict rules for the model\n",
    "- Need to be very clear about the output expected or else the model deviates a lot, especially as it processes more data.\n",
    "- Include lines about lack of evidence if not the model may hallucinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b05ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove whitespace in text.\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4f9ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the transcript into smaller chunks.\n",
    "def chunk_text(text: str, max_chars: int = 6000):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split into sentences \n",
    "    chunks, current_chunk, current_len = [], [], 0 # list of chunks, sentences collecting for current chunk, character count for current chunk\n",
    "\n",
    "    for s in sentences:\n",
    "        if current_len + len(s) + 1 <= max_chars: # if the characters of current chunk + new sentence is below the limit:\n",
    "            current_chunk.append(s) # add sentence to current chunk \n",
    "            current_len += len(s) + 1 # update running character count \n",
    "        \n",
    "        else: # if the characters is above the limit:\n",
    "            chunks.append(' '.join(current_chunk)) # add the current chunk to the final chunk list\n",
    "            current_chunk, current_len = [s], len(s) # start a new chunk containing the sentence and update current len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk)) # add any sentences in current chunk after loop ends \n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe2b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PRA categories and definitions from CSV.\n",
    "def load_pra_categories(path: Path):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            (row.get('category', '').strip(), [row.get('definition', '').strip()])\n",
    "            for row in reader if row.get('category')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db60595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sentence-BERT embedding index for PRA categories.\n",
    "def build_embedding_index(pra_categories):\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    docs = [f\"{name} {' '.join(defs)}\" for name, defs in pra_categories]\n",
    "    pra_risk_embeddings = embedder.encode(docs, batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "    return embedder, np.asarray(pra_risk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the relevant PRA categories to the transcript chunks.\n",
    "def find_rel_categories(chunk, pra_categories, embedder, pra_risk_embeddings, top_k=2):\n",
    "    query_vec = embedder.encode([chunk], normalize_embeddings=True) # turns chunk into embedding\n",
    "    sims = cosine_similarity(query_vec, pra_risk_embeddings).ravel() # compares the chunk to each category doc \n",
    "    top_indices = np.argsort(-sims)[:top_k] # sorts scores descending and selected top k cateogories \n",
    "\n",
    "    return [pra_categories[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8f1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON\n",
    "def parse_tagged_json(raw):\n",
    "    m = re.search(r\"<json>\\s*(\\{[\\s\\S]*?\\})\\s*</json>\", raw, flags=re.IGNORECASE)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise the text chunks.\n",
    "def summarise_chunk(model, chunk, relevant_categories, max_evidence=5):\n",
    "\n",
    "    # Build PRA notes (limit to 2 bullets per category)\n",
    "    lines = []\n",
    "    for name, definition in relevant_categories:\n",
    "        lines.append(f'- {name}:')\n",
    "        for d in list(definition)[:2]:\n",
    "            lines.append(f'- {d}')\n",
    "    notes_block = '\\n'.join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a careful data extraction model. \"\n",
    "        \"Return ONLY valid JSON wrapped in <json>...</json> tags.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "TRANSCRIPT:\n",
    "{chunk}\n",
    "\n",
    "PRA NOTES:\n",
    "{notes_block}\n",
    "\n",
    "TASK:\n",
    "Return JSON ONLY, wrapped exactly like this:\n",
    "<json>{{\"summary\": \"...\", \"evidence\": [\"...\"], \"pra_categories\": [{{\"category\":\"...\",\"why\":\"...\"}}]}}</json>\n",
    "\n",
    "RULES:\n",
    "- 4-6 sentence neutral summary.\n",
    "- Up to {max_evidence} evidence bullets (quotes/facts).\n",
    "- 1-3 pra_categories objects.\n",
    "- If evidence is lacking, use a single bullet \"Insufficient evidence\".\n",
    "- Only choose categories supported by the evidence.\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = model.create_chat_completion(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=700,\n",
    "        repeat_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    raw = (response['choices'][0]['message']['content'] or '').strip()\n",
    "\n",
    "    # Parse the tagged JSON\n",
    "    parsed = parse_tagged_json(raw)\n",
    "\n",
    "    # Fallback if model didnâ€™t follow instructions\n",
    "    if not parsed:\n",
    "        return (\n",
    "            {'summary': '', 'evidence': ['Insufficient evidence'], 'pra_categories': []},\n",
    "            raw,\n",
    "        )\n",
    "\n",
    "    # Light coercion to guarantee keys exist\n",
    "    result = {\n",
    "        'summary': parsed.get('summary', '') or '',\n",
    "        'evidence': parsed.get('evidence', []) or [],\n",
    "        'pra_categories': parsed.get('pra_categories', []) or []\n",
    "    }\n",
    "    return result, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e398a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables.\n",
    "MODEL_PATH = '/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "PRA_NOTES_PATH = '../data/RAG-resources/PRA_risk_categories.csv'\n",
    "TRANSCRIPT_PATH = '../data/processed/jpm/all_jpm_2025.csv'\n",
    "OUTPUT_PATH = pathlib.Path('../notebooks/summarisation_evasion_files/jpm_mistral_pra_summary_raw.json')\n",
    "TOP_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c63a4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 3559 MiB free\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 110 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  2495.33 MiB, ( 9858.72 / 10922.67)\n",
      "load_tensors: offloading 20 repeating layers to GPU\n",
      "load_tensors: offloaded 20/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  2495.33 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x17ffa0f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x17ffa12f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x17ffa16c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x364e7acd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x38c3bdb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x38c3bdee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x38aed92c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x38aed9580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x364e7af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x38aed9840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x364ea6140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x17ffa1c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x364ea6400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x364ea66c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x364ea6980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x38aee4800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x38aee4ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x364ea6c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x364ea6f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x38c3be280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x38c3be790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x364ea71c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x364ea7480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x38aee4d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x38aee5040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x38c3beaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x38c3bee20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x38aee5300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x364ea7740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x364ea7b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x364ea7ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x38aee55c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x38c3bf200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x364ea82a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x38c3bf6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x38c3bfa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x38c3bfe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x38aee5880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x17ffa1f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x17ffa22a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x17ffa2640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x17ffa2a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x38aee5b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x38aee5e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x17ffa2e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x17ffa31a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x17ffa3570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x38aee60c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x38c3c04b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x38c3c0770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x38aed7c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x38aed7ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x38aed81b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x38aed8470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x38aed8730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x38aed89f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x364ea8600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x17ffa3940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x38aed1140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x364ea8a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x17ffa3da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x38c3c0ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x38c3c0f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x38aed1400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x364ea8ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x364ea9210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x364ea9570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x38c3c1220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x38c3c14e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x38aed7170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x38c3c1880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x364ea9ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x38c3c1ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x38aed7430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x364ea9db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x38aed76f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x17ff9ef40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x38c3c2040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x38c3c24e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x38aed66a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x38aed9d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x38aeda000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x38aee6900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x17ff9f200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x38aee6bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x38c3c2b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x38c3c2dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x17ff9f4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x17ff9f860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x38aee6e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x38c3c3080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x17ff9fbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x364eaa210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x364eaa570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x364eaa910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x364eaace0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x38aee7140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x38aee7400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x17ff9ff90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x364eab0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x364eab480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x364eab850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x364eabc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x364eabff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x364eac3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x364eac790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x364eacb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x38c3c3400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x17ffa03f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x17ff8e120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17ff8e3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x17ff8e760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x38c3c3760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x364eacec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x17ff8eac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x38c3c3b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x38c3c3fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x364ead320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x364ead780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17ffa4590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x38c3c4450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x38aee76c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17ffa4850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x364eadae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x38c3c4760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x17ffa4bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x38c3c4b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x38aee7980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x38aee7c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x364eadfd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x38aee7f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x17ffa4f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x38aee81c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x364eae2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x38c3c4f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x364eae630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x364eaea40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x38c3c55a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x38c3c5860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x17ffa54c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x38aee8480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x364eaeda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x38aee8740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x38c3c5ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x364eaf160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x38aee8a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x38aee8cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x38c3c5bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x364eaf4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x364eaf920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x364eafcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x364eb00c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x364eb0490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x364eb0860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x364eb0c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x38c3c6360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x38c3c66b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x17ffa5780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x38c3c6a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x38c3c6e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x17ffa5b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x38aee8f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x364eb0f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x364eb13f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x364eb17c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x364eb1b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x364eb1f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x364eb2330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x38aee9240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x38aee9500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x38c3c7520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x38c3c77e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x38c3c7aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x38c3c7dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x38c3c81a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x38c3c87e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x38c3c8aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x38c3c8d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x38c3c90e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x17ffa5f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17ffa6630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x17ffa6a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x17ffa6dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x17ffa71a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x17ffa7500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x38c3c9770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x17ffa7960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x17ffa7d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x38aee97c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x38c3c9b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x364eb2700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x364eb2ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x38aee9a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x364eb2ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x364eb3200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x364eb3660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x17ffa8090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x38c3ca000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x38c3ca2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x38c3ca580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x38c3ca840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x17ff8f3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x17ff8f6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x17ff8fa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x38c3cabe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x17ff8fe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x38c3cafb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x364eb3a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x17ff90230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x38c3cb3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x38c3cb750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x38c3cbb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x38aee9d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x38aeea000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x38aeea2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x38aeea580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x38c3cbef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x38c3cc2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x38c3cc690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x38c3cca60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x38c3cce30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x17ff90690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x17ff909f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x17ff90d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x17ff91160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x17ff91530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x17ff91900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x17ff91cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x38c3cd200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x38c3cd5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x38aeea840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x38c3cd9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x17ff92030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x364eb3d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x17ff92430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x17ff92800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x364eb41f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x38aeeab00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x38c3cdfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x38c3ce2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x38aeeadc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x38aeeb080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x38aeeb340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x38c3ce5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x364eb45c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x38c3cea20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x17ff92b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x38c3cf110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17ff92f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x38aeeb600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x17ff93390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x38aeeb8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x17ff936f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x38aeebb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x38aeebe40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107607c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x17ff93ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x364eb4920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x364eb4cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x38aeec100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x38aeec3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x38aeec6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x38aeecac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x17ff93e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x17ff942f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x17ff946c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x38aeece90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x38c3ced80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x38c3cf7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x38aeed2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x17ff94a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x38aeed650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x38aeeda60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x17ff94e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x17ff95230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x38aeede00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x17ff95600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x17ff95960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x17ff95dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x364eb50c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x38c3cfc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x17ff96120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x38aeee260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x38aeee670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x38c3d00a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x38c3d04b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x364eb5600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x38aeee980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x17ff964a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x38aeeef70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x38c3d0770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x364eb5960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x364eb5d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x38c3d0b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x38c3d0e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x38c3d10d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x38c3d1390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x17ff96860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x17ff96c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x17ff97000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x38c3d1a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x17ff973d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x38c3d1cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x38aeef230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x38aeef4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17ff977a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x17ff97b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x364eb60c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x17ff980b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x364eb74c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x17ff98370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x17ff98710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x17ff98ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x38c3d1f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x38c3d2360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x38c3d2730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x17ff98eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x38c3d2b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x38c3d2ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x17ff99280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x17ff99650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x17ff99a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x38c3d3230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x38aeef850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x364eb7a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x38aeefc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x38aeefff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x364eb7cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x38c3d36b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x38aef0450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x17ff99df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x17ff9a150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x364eb7ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x38c3d3a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x17ff9a510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x38aef07b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x38aef0c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x364eb83c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x17ff9a920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x38aef0f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x38aef13d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x38aef17a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x38aef1b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x38aef1f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x38aef2310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x38aef26e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x38aef2ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x364eb8a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x38c3d3e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x17ff9acc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x17ff9b090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x17ff9b460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x17ff9b830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x17ff9bc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x17ff9bfd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x17ff9c3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x38aef2e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x38aef3250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x17ff9c770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x38c3d41d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x17ff9cb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x38c3d4640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x17ff9cf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x17ff9d2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x17ff9d6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x17ff9da80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x17ffa88d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x17ffa8b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x17ffa8f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x38c3d4a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x38c3d4de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x17ffa9330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x364eb9800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x17ffa9700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x17ffa9ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x364eb9ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x17ffa9e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x364eb9d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x17ffaa200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x17ffaa660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x364eba040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x364eba350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x38c3d5240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x38c3d5690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x17ffaa9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x38c3d59f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x38c3d5e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x38aef35b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x364eba720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x38aef3a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x38aef3de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x364ebad30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x38c3d6430 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = Metal\n",
      "llama_kv_cache_unified: layer  13: dev = Metal\n",
      "llama_kv_cache_unified: layer  14: dev = Metal\n",
      "llama_kv_cache_unified: layer  15: dev = Metal\n",
      "llama_kv_cache_unified: layer  16: dev = Metal\n",
      "llama_kv_cache_unified: layer  17: dev = Metal\n",
      "llama_kv_cache_unified: layer  18: dev = Metal\n",
      "llama_kv_cache_unified: layer  19: dev = Metal\n",
      "llama_kv_cache_unified: layer  20: dev = Metal\n",
      "llama_kv_cache_unified: layer  21: dev = Metal\n",
      "llama_kv_cache_unified: layer  22: dev = Metal\n",
      "llama_kv_cache_unified: layer  23: dev = Metal\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      Metal compute buffer size =   300.01 MiB\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 171 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   18908.45 ms /  1764 tokens (   10.72 ms per token,    93.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18800.73 ms /   279 runs   (   67.39 ms per token,    14.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   37780.03 ms /  2043 tokens\n",
      "llama_perf_context_print:    graphs reused =        270\n",
      "Llama.generate: 11 prefix-match hit, remaining 1707 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   18527.09 ms /  1707 tokens (   10.85 ms per token,    92.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22297.35 ms /   343 runs   (   65.01 ms per token,    15.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   40916.79 ms /  2050 tokens\n",
      "llama_perf_context_print:    graphs reused =        331\n",
      "Llama.generate: 11 prefix-match hit, remaining 1996 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   20889.18 ms /  1996 tokens (   10.47 ms per token,    95.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28430.46 ms /   414 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   49442.78 ms /  2410 tokens\n",
      "llama_perf_context_print:    graphs reused =        400\n",
      "Llama.generate: 11 prefix-match hit, remaining 1923 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   20351.30 ms /  1923 tokens (   10.58 ms per token,    94.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24839.20 ms /   373 runs   (   66.59 ms per token,    15.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   45292.00 ms /  2296 tokens\n",
      "llama_perf_context_print:    graphs reused =        360\n",
      "Llama.generate: 11 prefix-match hit, remaining 1799 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   19071.85 ms /  1799 tokens (   10.60 ms per token,    94.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24120.15 ms /   352 runs   (   68.52 ms per token,    14.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   43289.24 ms /  2151 tokens\n",
      "llama_perf_context_print:    graphs reused =        340\n",
      "Llama.generate: 11 prefix-match hit, remaining 2023 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   21757.24 ms /  2023 tokens (   10.75 ms per token,    92.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34214.77 ms /   484 runs   (   70.69 ms per token,    14.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   56129.28 ms /  2507 tokens\n",
      "llama_perf_context_print:    graphs reused =        468\n",
      "Llama.generate: 12 prefix-match hit, remaining 2044 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   18909.34 ms\n",
      "llama_perf_context_print: prompt eval time =   21865.69 ms /  2044 tokens (   10.70 ms per token,    93.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34254.94 ms /   487 runs   (   70.34 ms per token,    14.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   56277.07 ms /  2531 tokens\n",
      "llama_perf_context_print:    graphs reused =        471\n",
      "Llama.generate: 12 prefix-match hit, remaining 1918 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     top_categories \u001b[38;5;241m=\u001b[39m find_rel_categories(\n\u001b[1;32m     25\u001b[0m         chunk, pra_categories, embedder, category_embeddings, top_k\u001b[38;5;241m=\u001b[39mTOP_K\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m     _, raw \u001b[38;5;241m=\u001b[39m \u001b[43msummarise_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     raw_outputs\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m: i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m: raw})\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36msummarise_chunk\u001b[0;34m(model, chunk, relevant_categories, max_evidence)\u001b[0m\n\u001b[1;32m     12\u001b[0m     system_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a careful data extraction model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn ONLY valid JSON wrapped in <json>...</json> tags.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mTRANSCRIPT:\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m- Only choose categories supported by the evidence.\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 36\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m700\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     raw \u001b[38;5;241m=\u001b[39m (response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Parse the tagged JSON\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/llama.py:2003\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m \n\u001b[1;32m   1967\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1997\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1998\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   2000\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2002\u001b[0m )\n\u001b[0;32m-> 2003\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/llama_chat_format.py:669\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    665\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    666\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 669\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/llama.py:1837\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1837\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/llama.py:1322\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1320\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1321\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1323\u001b[0m     prompt_tokens,\n\u001b[1;32m   1324\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1325\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1326\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1327\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1328\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1329\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1330\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1331\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1332\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1333\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1334\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1335\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1336\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1337\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1338\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1339\u001b[0m ):\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mvocab, token):\n\u001b[1;32m   1341\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/llama.py:914\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 914\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    916\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    917\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    918\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    932\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    933\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/llama.py:648\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    644\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    646\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits_all\n\u001b[1;32m    647\u001b[0m )\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/llama_cpp/_internals.py:322\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 322\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Runner code.\n",
    "pra_categories = load_pra_categories(Path(PRA_NOTES_PATH))\n",
    "embedder, category_embeddings = build_embedding_index(pra_categories)\n",
    "\n",
    "# Load and chunk transcript\n",
    "transcript_text = Path(TRANSCRIPT_PATH).read_text(encoding='utf-8')\n",
    "transcript_chunks = chunk_text(transcript_text)\n",
    "\n",
    "n_threads = max(4, (os.cpu_count() or 8) - 2)\n",
    "\n",
    "# Define the model.\n",
    "model = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=20,\n",
    "    chat_format='mistral-instruct',\n",
    "    n_threads=n_threads,\n",
    ")\n",
    "\n",
    "raw_outputs = []\n",
    "\n",
    "for i, chunk in enumerate(transcript_chunks, 1):\n",
    "    try:\n",
    "        top_categories = find_rel_categories(\n",
    "            chunk, pra_categories, embedder, category_embeddings, top_k=TOP_K\n",
    "        )\n",
    "        _, raw = summarise_chunk(\n",
    "            model, chunk, top_categories, max_evidence=5\n",
    "        )\n",
    "        raw_outputs.append({'chunk': i, 'raw': raw})\n",
    "\n",
    "    except Exception:\n",
    "        raw_outputs.append({'chunk': i, 'raw': ''})\n",
    "\n",
    "final_output = {'raw_outputs': raw_outputs}\n",
    "\n",
    "OUTPUT_PATH.write_text(json.dumps(final_output, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f'Wrote final JSON to: {OUTPUT_PATH.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ff6d7",
   "metadata": {},
   "source": [
    "- Need to preprocess the output so it is visually clearer (summary, evidence, PRA categories (name & why the model chose this))\n",
    "- Can this information be fed to the model again and can it detect any early PRA risk indicators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9d93f",
   "metadata": {},
   "source": [
    "# **6. Evasion Detection Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031e1b4",
   "metadata": {},
   "source": [
    "1. **Baseline Evasion score** (rule-based) is made up of three components:\n",
    "- **Cosine similarity**- similarity of the question and answer, lower similarity = more evasive\n",
    "- **Numeric specificity check**- does the question require a number, if so does the answer contain a number?, e.g. requests for financial data\n",
    "- **Evasive phrases**- does the answer contain evasive phrases?, presence = more evasive\n",
    "\n",
    "2. **LLM evasion score** (RoBERTa-MNLI) uses entailment/neutral/contradiction between the question and answer\n",
    "- Lower entailment (and higher neutral + contradiction) = more evasive\n",
    "  \n",
    "3. **Blended evasion score** combines both scores including a weight for the LLM component\n",
    "- Rationale is that baseline enforces precision while the LLM will capture semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a1455",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da77e03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Hey, good morning.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Good morning, Steve.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>True</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  ...                                         source_pdf\n",
       "0  presentation  ...  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...\n",
       "1            qa  ...  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...\n",
       "2            qa  ...  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...\n",
       "3            qa  ...  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...\n",
       "4            qa  ...  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1411\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "all_jpm_2023_2025 = pd.read_csv('../data/processed/jpm/all_jpm_2023_2025.csv')\n",
    "\n",
    "# View dataset.\n",
    "display(all_jpm_2023_2025.head())\n",
    "\n",
    "# Number of rows.\n",
    "print('Number of rows:', all_jpm_2023_2025.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94a06081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1241\n"
     ]
    }
   ],
   "source": [
    "# Remove pleasantries.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025[all_jpm_2023_2025['is_pleasantry'] == False]\n",
    "print('Number of rows:', all_jpm_2023_2025_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88a56af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 23\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025_cleaned['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7e42dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no content.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025_cleaned.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44afee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no content: 0\n"
     ]
    }
   ],
   "source": [
    "# Check content column.\n",
    "print('Number of rows with no content:', all_jpm_2023_2025_cleaned['content'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c7fc06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay',\n",
       "       \"We're fundamentally\", 'Thanks', 'Almost no chance.'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View roles.\n",
    "all_jpm_2023_2025_cleaned['role'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe99f2",
   "metadata": {},
   "source": [
    "- Some text has leaked into role column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53bf13fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>qa</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Who knows how important politics are in all th...</td>\n",
       "      <td>We're fundamentally</td>\n",
       "      <td>as I said, I think on the press call, happy to...</td>\n",
       "      <td>little bit cautious about the pull-forward dyn...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-1q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>qa</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Chief Financial Officer, JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Thanks</td>\n",
       "      <td>Glenn.</td>\n",
       "      <td>Operator: Next, we'll go to the line of Matt O...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q24-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>qa</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>qa</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer, JPMorgan C...</td>\n",
       "      <td>Almost no chance.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Well, but having â€“ it's very important. While ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q3</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/jpm-3q24-earnings-conference-call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     section  ...                                         source_pdf\n",
       "305       qa  ...  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...\n",
       "309       qa  ...  data/raw/jpm/.ipynb_checkpoints/jpm-2q25-earni...\n",
       "650       qa  ...  data/raw/jpm/jpm-1q24-earnings-call-transcript...\n",
       "924       qa  ...  data/raw/jpm/jpm-2q24-earnings-call-transcript...\n",
       "1059      qa  ...  data/raw/jpm/jpm-2q25-earnings-call-transcript...\n",
       "1063      qa  ...  data/raw/jpm/jpm-2q25-earnings-call-transcript...\n",
       "1274      qa  ...  data/raw/jpm/jpm-3q24-earnings-conference-call...\n",
       "\n",
       "[7 rows x 11 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles. \n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = all_jpm_2023_2025_cleaned[~all_jpm_2023_2025_cleaned['role'].isin(valid_roles)]\n",
    "invalid_roles_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9915ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the correct role information.\n",
    "all_jpm_2023_2025_cleaned.loc[[305, 309, 924, 1059, 1063], 'role'] = 'Chief Financial Officer'\n",
    "all_jpm_2023_2025_cleaned.loc[[1274], 'role'] = 'Chairman & Chief Executive Officer'\n",
    "\n",
    "# Drop nonsence row.\n",
    "all_jpm_2023_2025_cleaned = all_jpm_2023_2025_cleaned.drop(index=650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90cbdf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chief Financial Officer', 'analyst',\n",
       "       'Chairman & Chief Executive Officer'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the roles have been updated.\n",
    "all_jpm_2023_2025_cleaned['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff80d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise role names.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Map roles.\n",
    "all_jpm_2023_2025_cleaned['role_normalised'] = all_jpm_2023_2025_cleaned['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35773e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>is_pleasantry</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presentation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Thanks, and good morning, everyone. The presen...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Steven Chubak</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Wolfe Research LLC</td>\n",
       "      <td>Got it. And just in terms of appetite for the ...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Oh, yeah.</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>False</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section  ...  role_normalised\n",
       "0  presentation  ...           banker\n",
       "3            qa  ...          analyst\n",
       "4            qa  ...           banker\n",
       "5            qa  ...          analyst\n",
       "6            qa  ...           banker\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1217\n"
     ]
    }
   ],
   "source": [
    "# View dataset.\n",
    "display(all_jpm_2023_2025_cleaned.head())\n",
    "print('Number of rows:', all_jpm_2023_2025_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88a0a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset.\n",
    "all_jpm_2023_2025_cleaned.to_csv('../data/processed/jpm/cleaned/all_jpm_2023_2025_cleaned') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0bcec245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to remove duplicates within questions and answers. \n",
    "def clean_repeats(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # 1) Normalize whitespace\n",
    "    t = ' '.join(text.split()).strip()\n",
    "    if not t:\n",
    "        return t\n",
    "\n",
    "    # 2) If the whole-string is a back-to-back duplicate (A+A) = keep first half\n",
    "    mid = len(t) // 2\n",
    "    if len(t) % 2 == 0 and t[:mid] == t[mid:]:\n",
    "        t = t[:mid]\n",
    "\n",
    "    # 3) Collapse immediate repeated token spans (n-grams)\n",
    "    toks = t.split()\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        matched = False\n",
    "        max_span = min(50, len(toks) - i)  # cap span to remaining length\n",
    "        for n in range(max_span, 4, -1):  # try longer spans first: 50..5\n",
    "            if i + 2*n <= len(toks) and toks[i:i+n] == toks[i+n:i+2*n]:\n",
    "                out.extend(toks[i:i+n])  # keep one copy\n",
    "                i += 2*n                # skip the duplicate block\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            out.append(toks[i])\n",
    "            i += 1\n",
    "    t = ' '.join(out)\n",
    "\n",
    "    # 4) Remove duplicate sentences globally (order-preserving)\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', t)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for s in sents:\n",
    "        s_norm = s.strip()\n",
    "        if not s_norm:\n",
    "            continue\n",
    "        key = ' '.join(s_norm.lower().split())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(s_norm)\n",
    "    return ' '.join(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e44cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert datasets into question and answer pairs.\n",
    "def create_qa_pairs(df, min_answer_words=30):\n",
    "    # Keep only the Q&A section.\n",
    "    qa_df = df[df['section'].astype(str).str.lower() == 'qa'].copy()\n",
    "\n",
    "    # Split into roles.\n",
    "    analyst_rows = qa_df[qa_df['role_normalised'] == 'analyst'].copy()\n",
    "    banker_rows  = qa_df[qa_df['role_normalised'] == 'banker' ].copy()\n",
    "\n",
    "    # Keys to keep quarters separated\n",
    "    key_q = ['year', 'quarter', 'question_number']\n",
    "\n",
    "    # Build full question text per (year, quarter, question_number)\n",
    "    question_text_map = (\n",
    "        analyst_rows\n",
    "        .groupby(key_q, dropna=False)['content']\n",
    "        .apply(lambda parts: clean_repeats(' '.join(parts.astype(str))))\n",
    "        .rename('question')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Ensure bankers have an answer_number â€” sequential per (year, quarter, question_number) if missing\n",
    "    if 'answer_number' not in banker_rows.columns or banker_rows['answer_number'].isna().any():\n",
    "        banker_rows = banker_rows.sort_index().copy()\n",
    "        banker_rows['answer_number'] = (\n",
    "            banker_rows\n",
    "            .groupby(key_q, dropna=False)\n",
    "            .cumcount() + 1\n",
    "        )\n",
    "\n",
    "    # Combine multiple banker utterances belonging to the same answer\n",
    "    banker_answers = (\n",
    "        banker_rows\n",
    "        .groupby(key_q + ['answer_number'], dropna=False)\n",
    "        .agg({\n",
    "            'content':        lambda parts: clean_repeats(' '.join(parts.astype(str))),\n",
    "            'speaker_name':   'first',\n",
    "            'role':           'first',\n",
    "            'role_normalised':'first',\n",
    "            'source_pdf':     'first'\n",
    "        })\n",
    "        .rename(columns={'content': 'answer'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge question text back onto each answer row\n",
    "    qa_pairs = banker_answers.merge(\n",
    "        question_text_map,\n",
    "        on=key_q,\n",
    "        how='left',\n",
    "        validate='many_to_one'\n",
    "    )\n",
    "\n",
    "    # Order columns for readability\n",
    "    column_order = [\n",
    "        'year', 'quarter', 'question_number', 'answer_number',\n",
    "        'question', 'answer',\n",
    "        'speaker_name', 'role', 'role_normalised',\n",
    "        'source_pdf'\n",
    "    ]\n",
    "    qa_pairs = qa_pairs.reindex(columns=[c for c in column_order if c in qa_pairs.columns])\n",
    "\n",
    "    # Sort and reset index.\n",
    "    qa_pairs = qa_pairs.sort_values(['year', 'quarter', 'question_number', 'answer_number']).reset_index(drop=True)\n",
    "\n",
    "    # Drop duplicate answers.\n",
    "    qa_pairs = qa_pairs.drop_duplicates(subset=['answer'])\n",
    "\n",
    "    # Drop short answers below threshold to ensure quality answers.\n",
    "    qa_pairs = qa_pairs[qa_pairs['answer'].astype(str).str.split().str.len() >= int(min_answer_words)]\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b56e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create q&A pairs.\n",
    "all_jpm_2023_2025_qa = create_qa_pairs(all_jpm_2023_2025_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92ef0685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 309\n"
     ]
    }
   ],
   "source": [
    "# View number of examples.\n",
    "print('Number of examples:', all_jpm_2023_2025_qa.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "070a4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into prediction set and validation/test set.\n",
    "jpm_2025_predict_qa = all_jpm_2023_2025_qa[all_jpm_2023_2025_qa['year'] == 2025]\n",
    "jpm_2023_2024_qa = all_jpm_2023_2025_qa[all_jpm_2023_2025_qa['year'].isin([2023, 2024])]\n",
    "\n",
    "# Save the datasets.\n",
    "jpm_2025_predict_qa.to_csv('../data/processed/jpm/cleaned/jpm_2025_predict_qa.csv') \n",
    "jpm_2023_2024_qa.to_csv('../data/processed/jpm/cleaned/jpm_2023_2024_qa.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ee995",
   "metadata": {},
   "source": [
    "The jpm_2023_2024_qa dataset was then manually labelled according to whether the banker's answer was deemed 'Direct' or 'Evasive'. The label was appended by a new column 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "798e4794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>role_normalised</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good morning. Thanks for all the comments on t...</td>\n",
       "      <td>Yeah. Matt, not particularly updating. I think...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then just separately, you bought bac...</td>\n",
       "      <td>Yeah. Good question. And I think you framed it...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thanks. Jeremy, could you give a little more c...</td>\n",
       "      <td>Yeah. Actually, John, this quarter, that's all...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then, just to follow up on the NII, ...</td>\n",
       "      <td>Sure. Yeah, happy to do that, John. So, I thin...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>Q4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hey. Good morning. Maybe just to follow up in ...</td>\n",
       "      <td>Yeah. Both good questions. So let's do reprice...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/jpm-4q23-earnings-call-transcript...</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year quarter  ...                                         source_pdf   label\n",
       "0  2023      Q4  ...  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct\n",
       "1  2023      Q4  ...  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct\n",
       "2  2023      Q4  ...  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct\n",
       "3  2023      Q4  ...  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct\n",
       "4  2023      Q4  ...  data/raw/jpm/jpm-4q23-earnings-call-transcript...  Direct\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 215\n"
     ]
    }
   ],
   "source": [
    "# Load the labelled dataset.\n",
    "jpm_2023_2024_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_2023_2024_qa_labelled.csv')\n",
    "\n",
    "# View the dataset.\n",
    "jpm_2023_2024_qa_labelled = jpm_2023_2024_qa_labelled.drop('Unnamed: 0', axis=1)\n",
    "display(jpm_2023_2024_qa_labelled.head())\n",
    "print('Number of examples:', jpm_2023_2024_qa_labelled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "120cebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the jpm_2023_2024 dataset into test and validation ensuring answers are not leaked.\n",
    "def val_test_split(df, group_key, label_col='label', test_size=0.5):\n",
    "\n",
    "    is_evasive = df[label_col].astype(str).str.lower().eq(\"evasive\")\n",
    "    g = df.assign(_ev=is_evasive.astype(int)).groupby(group_key).agg(\n",
    "        n=(\"__dummy__\", \"size\") if \"__dummy__\" in df.columns else (\"_ev\", \"size\"),\n",
    "        ev=(\"_ev\", \"sum\")\n",
    "    )\n",
    "\n",
    "    # Order groups: evasive-heavy first, then larger groups\n",
    "    order = g.sort_values([\"ev\", \"n\"], ascending=False).index.tolist()\n",
    "\n",
    "    # Greedy pack groups into two halves balancing evasive counts, then size\n",
    "    A, B = [], []\n",
    "    evA = evB = nA = nB = 0\n",
    "    target_n_each = len(df) * (1 - test_size)\n",
    "\n",
    "    for grp in order:\n",
    "        ev, n = int(g.loc[grp, \"ev\"]), int(g.loc[grp, \"n\"])\n",
    "        # choose the side with fewer evasives; on tie, choose the smaller side by n\n",
    "        if (evA < evB) or (evA == evB and nA <= nB):\n",
    "            A.append(grp); evA += ev; nA += n\n",
    "        else:\n",
    "            B.append(grp); evB += ev; nB += n\n",
    "\n",
    "    # Build frames: A = validation, B = test (roughly 50/50 by rows)\n",
    "    val_set  = df[df[group_key].isin(A)].reset_index(drop=True)\n",
    "    test_set = df[df[group_key].isin(B)].reset_index(drop=True)\n",
    "    return val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1dc23c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a group key so that all answers for the same question stay in the same set. \n",
    "jpm_2023_2024_qa_labelled['group_key'] = (\n",
    "    jpm_2023_2024_qa_labelled['year'].astype(str) + '_' +\n",
    "    jpm_2023_2024_qa_labelled['quarter'].astype(str) + '_' +\n",
    "    jpm_2023_2024_qa_labelled['question_number'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "314840b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation examples: 108 \n",
      "label\n",
      "Direct     87\n",
      "Evasive    21\n",
      "Name: count, dtype: int64\n",
      "Number of test examples: 107 \n",
      "label\n",
      "Direct     86\n",
      "Evasive    21\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split into validation and test set.\n",
    "jpm_val_qa, jpm_test_qa = val_test_split(\n",
    "    jpm_2023_2024_qa_labelled,\n",
    "    group_key='group_key',\n",
    "    label_col='label'\n",
    ")\n",
    "\n",
    "print(f'Number of validation examples: {jpm_val_qa.shape[0]} \\n{jpm_val_qa[\"label\"].value_counts()}')\n",
    "print(f'Number of test examples: {jpm_test_qa.shape[0]} \\n{jpm_test_qa[\"label\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bb98837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets.\n",
    "jpm_val_qa.to_csv('../data/processed/jpm/cleaned/jpm_val_qa.csv')\n",
    "jpm_test_qa.to_csv('../data/processed/jpm/cleaned/jpm_test_qa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b998a",
   "metadata": {},
   "source": [
    "- Human label the validation and test dataset with evasive or direct labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f494",
   "metadata": {},
   "source": [
    "### **Load the Clean & Labelled Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e43ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and labelled validation & test datasets.\n",
    "jpm_predict_qa = pd.read_csv('../data/processed/jpm/cleaned/jpm_predict_qa.csv')\n",
    "jpm_test_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_test_qa_labelled.csv')\n",
    "jpm_val_qa_labelled = pd.read_csv('../data/processed/jpm/cleaned/jpm_val_qa_labelled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e163d15",
   "metadata": {},
   "source": [
    "### **Re-make the validation and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85644fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import Counter\n",
    "\n",
    "def _class_props(y):\n",
    "    c = Counter(y); n = sum(c.values())\n",
    "    return {k: c[k]/n for k in c}\n",
    "\n",
    "def _score_stratification(y_overall, y_subset):\n",
    "    \"\"\"Lower is better. Sum of absolute diffs in class proportions.\"\"\"\n",
    "    p_all = _class_props(y_overall)\n",
    "    p_sub = _class_props(y_subset)\n",
    "    keys = set(p_all) | set(p_sub)\n",
    "    return sum(abs(p_all.get(k,0) - p_sub.get(k,0)) for k in keys)\n",
    "\n",
    "def stratified_group_shuffle_split(\n",
    "    df: pd.DataFrame,\n",
    "    y_col: str,\n",
    "    group_col: str,\n",
    "    test_size: float = 0.25,\n",
    "    min_evasive_test: int = 100,\n",
    "    evasive_label='Evasive',  # or 1 if your 2-class uses ints\n",
    "    max_trials: int = 500,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns new_val_df, new_test_df\n",
    "    - Group-aware (by group_col)\n",
    "    - Approx stratified (choose the split that best matches overall class mix)\n",
    "    - Enforces at least `min_evasive_test` items in test\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    assert y_col in df and group_col in df, \"Missing y_col or group_col.\"\n",
    "    y_all   = df[y_col].values\n",
    "    groups  = df[group_col].values\n",
    "\n",
    "    best = None\n",
    "    best_score = np.inf\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    for _ in range(max_trials):\n",
    "        rs = int(rng.randint(0, 1e9))\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=rs)\n",
    "        idx_tr, idx_te = next(gss.split(np.zeros(len(df)), y_all, groups))\n",
    "        y_te = y_all[idx_te]\n",
    "\n",
    "        # ensure enough evasives in test\n",
    "        evasive_count = np.sum(y_te == evasive_label)\n",
    "        if evasive_count < min_evasive_test:\n",
    "            continue\n",
    "\n",
    "        score = _score_stratification(y_all, y_te)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best = (idx_tr, idx_te)\n",
    "\n",
    "    if best is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not satisfy min_evasive_test={min_evasive_test}. \"\n",
    "            \"Increase test_size, lower min_evasive_test, or pool more data.\"\n",
    "        )\n",
    "\n",
    "    idx_tr, idx_te = best\n",
    "    new_val  = df.iloc[idx_tr].reset_index(drop=True)\n",
    "    new_test = df.iloc[idx_te].reset_index(drop=True)\n",
    "\n",
    "    # quick report\n",
    "    def _counts(d):\n",
    "        c = Counter(d[y_col]); return dict(sorted(c.items(), key=lambda x: str(x[0])))\n",
    "    print(\"=== New Split Summary ===\")\n",
    "    print(f\"Total: {len(df)} | Val: {len(new_val)} | Test: {len(new_test)}\")\n",
    "    print(\"Overall:\", _counts(df))\n",
    "    print(\"Val:    \", _counts(new_val))\n",
    "    print(\"Test:   \", _counts(new_test))\n",
    "    print(\"Stratification score (abs diff sum):\", best_score)\n",
    "\n",
    "    return val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec9ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== New Split Summary ===\n",
      "Total: 215 | Val: 143 | Test: 72\n",
      "Overall: {'Direct': 173, 'Evasive': 42}\n",
      "Val:     {'Direct': 115, 'Evasive': 28}\n",
      "Test:    {'Direct': 58, 'Evasive': 14}\n",
      "Stratification score (abs diff sum): 0.0018087855297157507\n"
     ]
    }
   ],
   "source": [
    "# Pool validation + test\n",
    "dev = pd.concat([jpm_val_qa_labelled, jpm_test_qa_labelled], ignore_index=True)\n",
    "\n",
    "# Build a group key so all answers to the same Q stay together\n",
    "dev['group_id'] = dev['year'].astype(str) + \"_\" + dev['quarter'].astype(str) + \"_\" + dev['question_number'].astype(str)\n",
    "\n",
    "# Run split\n",
    "jpm_val_qa_labelled, jpm_test_qa_labelled = stratified_group_shuffle_split(\n",
    "    dev,\n",
    "    y_col='label',          # your ground-truth label column\n",
    "    group_col='group_id',   # <- key!\n",
    "    test_size=0.30,\n",
    "    min_evasive_test=12,\n",
    "    evasive_label='Evasive',\n",
    "    random_state=42,\n",
    "    max_trials=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e8784",
   "metadata": {},
   "source": [
    "### **continue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bbed1bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>role_normalised</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>label</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, we've told you that we're kind of pencil...</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>So, as you think about all of what you've just...</td>\n",
       "      <td>Okay. Let's take a crack. Let's see what the b...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hi, good morning. I guess, maybe one question,...</td>\n",
       "      <td>Yeah, so Ebrahim let me sort of respond narrow...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hi, good morning. I guess, maybe one question,...</td>\n",
       "      <td>Yeah. And then in terms of the office space, a...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year quarter  question_number  answer_number  \\\n",
       "0           0  2023      Q1              1.0            1.0   \n",
       "1           2  2023      Q1              1.0            3.0   \n",
       "2          10  2023      Q1              7.0            2.0   \n",
       "3          19  2023      Q1             13.0            1.0   \n",
       "4          21  2023      Q1             13.0            3.0   \n",
       "\n",
       "                                            question  \\\n",
       "0  So, Jamie, I was actually hoping to get your p...   \n",
       "1  So, Jamie, I was actually hoping to get your p...   \n",
       "2  So, as you think about all of what you've just...   \n",
       "3  Hi, good morning. I guess, maybe one question,...   \n",
       "4  Hi, good morning. I guess, maybe one question,...   \n",
       "\n",
       "                                              answer   speaker_name  \\\n",
       "0  Well, I think you were already kind of complet...    Jamie Dimon   \n",
       "1  Well, we've told you that we're kind of pencil...    Jamie Dimon   \n",
       "2  Okay. Let's take a crack. Let's see what the b...  Jeremy Barnum   \n",
       "3  Yeah, so Ebrahim let me sort of respond narrow...  Jeremy Barnum   \n",
       "4  Yeah. And then in terms of the office space, a...  Jeremy Barnum   \n",
       "\n",
       "                                 role role_normalised  \\\n",
       "0  Chairman & Chief Executive Officer          banker   \n",
       "1  Chairman & Chief Executive Officer          banker   \n",
       "2             Chief Financial Officer          banker   \n",
       "3             Chief Financial Officer          banker   \n",
       "4             Chief Financial Officer          banker   \n",
       "\n",
       "                                          source_pdf   label      group_id  \n",
       "0  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct   2023_Q1_1.0  \n",
       "1  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct   2023_Q1_1.0  \n",
       "2  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct   2023_Q1_7.0  \n",
       "3  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct  2023_Q1_13.0  \n",
       "4  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct  2023_Q1_13.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>role_normalised</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>label</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie â€“ there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hi. Good morning. My first question is you men...</td>\n",
       "      <td>Yeah. So, Erika, as you know, we take â€“ not go...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>2023_Q1_6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year quarter  question_number  answer_number  \\\n",
       "0           4  2023      Q1              2.0            1.0   \n",
       "1           5  2023      Q1              3.0            1.0   \n",
       "2           6  2023      Q1              4.0            1.0   \n",
       "3           7  2023      Q1              5.0            1.0   \n",
       "4           8  2023      Q1              6.0            1.0   \n",
       "\n",
       "                                            question  \\\n",
       "0  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "1  Yeah, and as a follow-up on the point about ra...   \n",
       "2  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "3  Okay. And then I wanted to ask Jamie â€“ there's...   \n",
       "4  Hi. Good morning. My first question is you men...   \n",
       "\n",
       "                                              answer   speaker_name  \\\n",
       "0  Yeah, sure. So let me just summarize the drive...  Jeremy Barnum   \n",
       "1  Well first of all, I don't quite believe it. S...    Jamie Dimon   \n",
       "2  Yeah. John, it's a really good question, and w...  Jeremy Barnum   \n",
       "3  Yeah. I wouldn't use the word credit crunch if...    Jamie Dimon   \n",
       "4  Yeah. So, Erika, as you know, we take â€“ not go...  Jeremy Barnum   \n",
       "\n",
       "                                 role role_normalised  \\\n",
       "0             Chief Financial Officer          banker   \n",
       "1  Chairman & Chief Executive Officer          banker   \n",
       "2             Chief Financial Officer          banker   \n",
       "3  Chairman & Chief Executive Officer          banker   \n",
       "4             Chief Financial Officer          banker   \n",
       "\n",
       "                                          source_pdf    label     group_id  \n",
       "0  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   Direct  2023_Q1_2.0  \n",
       "1  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   Direct  2023_Q1_3.0  \n",
       "2  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   Direct  2023_Q1_4.0  \n",
       "3  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...   Direct  2023_Q1_5.0  \n",
       "4  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Evasive  2023_Q1_6.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the labelled datasets.\n",
    "display(jpm_test_qa_labelled.head())\n",
    "display(jpm_val_qa_labelled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd1968",
   "metadata": {},
   "source": [
    "### **LLM Model Set-up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98557e44",
   "metadata": {},
   "source": [
    "- Initial testing three LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "16d6267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model name checkpoints.\n",
    "roberta_name = 'roberta-large-mnli'\n",
    "deberta_name = 'microsoft/deberta-large-mnli'\n",
    "zs_deberta_name = 'MoritzLaurer/deberta-v3-large-zeroshot-v2.0'\n",
    "\n",
    "# Load tokenizers and models.\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_name)\n",
    "roberta = AutoModelForSequenceClassification.from_pretrained(roberta_name)\n",
    "\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(deberta_name)\n",
    "deberta = AutoModelForSequenceClassification.from_pretrained(deberta_name)\n",
    "\n",
    "zs_deberta_tokenizer = AutoTokenizer.from_pretrained(zs_deberta_name)\n",
    "zs_deberta = AutoModelForSequenceClassification.from_pretrained(zs_deberta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9599a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta id2label: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n",
      "deberta id2label: {0: 'CONTRADICTION', 1: 'NEUTRAL', 2: 'ENTAILMENT'}\n",
      "zs_deberta id2label: {0: 'entailment', 1: 'not_entailment'}\n"
     ]
    }
   ],
   "source": [
    "# Verify label order per model.\n",
    "print(\"roberta id2label:\", roberta.config.id2label)\n",
    "print(\"deberta id2label:\", deberta.config.id2label)\n",
    "print(\"zs_deberta id2label:\", zs_deberta.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84565ed2",
   "metadata": {},
   "source": [
    "- Roberta and deberta have the standard 3 MNLI labels whereas zero shot deberta is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dcde65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add models and tokenizers to dictionary.\n",
    "models_and_tokenizers = {\n",
    "        'roberta': (roberta, roberta_tokenizer),\n",
    "        'deberta': (deberta, deberta_tokenizer),\n",
    "        'zs_deberta': (zs_deberta, zs_deberta_tokenizer)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e540367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # no MPS\n",
    "for model, tok in models_and_tokenizers.values():\n",
    "    model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c60cf",
   "metadata": {},
   "source": [
    "### **Baseline Evasion Score Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e9c22283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of evasive phrases\n",
    "EVASIVE_PHRASES = [\n",
    "    r\"\\btoo early\\b\",\n",
    "    r\"\\bcan't (?:comment|share|discuss)\\b\",\n",
    "    r\"\\bwon't (?:comment|share|provide)\\b\",\n",
    "    r\"\\bno (?:update|comment)\\b\",\n",
    "    r\"\\bwe (?:don't|do not) (?:break out|provide guidance)\\b\",\n",
    "    r\"\\bnot (?:going to|able to) (?:comment|share|provide)\\b\",\n",
    "    r\"\\bwe'll (?:come back|circle back)\\b\",\n",
    "    r\"\\bnot something we disclose\\b\",\n",
    "    r\"\\bas (?:we|I) (?:said|mentioned)\\b\",\n",
    "    r\"\\bgenerally speaking\\b\",\n",
    "    r\"\\bit's premature\\b\",\n",
    "    r\"\\bit's difficult to say\\b\",\n",
    "    r\"\\bI (?:wouldn't|won't) want to (?:speculate|get into)\\b\",\n",
    "    r\"\\bI (?:think|guess|suppose)\\b\",\n",
    "    r\"\\bkind of\\b\",\n",
    "    r\"\\bsort of\\b\",\n",
    "    r\"\\baround\\b\",\n",
    "    r\"\\broughly\\b\",\n",
    "    r\"\\bwe (?:prefer|plan) not to\\b\",\n",
    "    r\"\\bwe're not prepared to\\b\",\n",
    "]\n",
    "\n",
    "# List of words that suggest the answer needs specific financial numbers to properly answer the question.\n",
    "SPECIFICITY_TRIGGERS = [\n",
    "    \"how much\",\"how many\",\"what is\",\"what are\",\"when\",\"which\",\"where\",\"who\",\"why\",\n",
    "    \"range\",\"guidance\",\"margin\",\"capex\",\"opex\",\"revenue\",\"sales\",\"eps\",\"ebitda\",\n",
    "    \"timeline\",\"date\",\"target\",\"growth\",\"update\",\"split\",\"dividend\",\"cost\",\"price\",\n",
    "    \"units\",\"volumes\",\"gross\",\"net\",\"tax\",\"percentage\",\"utilization\",\"order book\"\n",
    "]\n",
    "\n",
    "NUMERIC_PATTERN = r\"(?:\\d+(?:\\.\\d+)?%|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b|Â£|\\$|â‚¬)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "70af0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between question and answers.\n",
    "def cosine_sim(q, a):\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform([q, a]) # converts text to vectors \n",
    "    sim = float(cosine_similarity(vec[0], vec[1])[0, 0]) # calculate the cosine similarity between the two vectors\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a9a484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute baseline evasion score.\n",
    "def baseline_evasion_score(q, a):\n",
    "    # 1. Cosine similarity\n",
    "    sim = cosine_sim(q, a) # calculates cosine similarity using previous function\n",
    "    sim_component = (1 - sim) * 45 # less similar the answer is, the bigger the contribution to the evasion score, scaled by 45\n",
    "\n",
    "    # 2. Numerical specificity- Does the question require and answer with financial figures/ a specific answer?\n",
    "    needs_num = any(t in q.lower() for t in SPECIFICITY_TRIGGERS) # true if the question requires a numeric/ specific answer\n",
    "    has_num = bool(re.search(NUMERIC_PATTERN, a)) # true if the answer includes a number \n",
    "    numeric_component = 25 if needs_num and not has_num else 0 # score of 25 if the question needs a number but the answer doesn't give one\n",
    "\n",
    "    # 3. Evasive phrases- does the answer contain evasive phrases?\n",
    "    phrase_hits = sum(len(re.findall(p, a.lower())) for p in EVASIVE_PHRASES) # counts how many times an evasive phrase appears in the answer\n",
    "    phrase_component = min(3, phrase_hits) * 8 # max of 3 hits counted, each hit = 8 points \n",
    "\n",
    "    # Final evasion score.\n",
    "    score = min(100, sim_component + numeric_component + phrase_component) # adds components together and caps score at 100\n",
    "    \n",
    "    return score, sim, phrase_hits, needs_num, has_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f5e81",
   "metadata": {},
   "source": [
    "### **LLM and Blended Evasion Score Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "04b61a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the premise for the model (question + answer).\n",
    "def build_premise(q, a):\n",
    "    return f'[QUESTION] {q} [ANSWER] {a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3f1159fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_max_len(tokenizer, model):\n",
    "    m = getattr(tokenizer, \"model_max_length\", None)\n",
    "    if m is None or m == int(1e30):\n",
    "        m = getattr(getattr(model, \"config\", None), \"max_position_embeddings\", 512)\n",
    "    return int(m or 512)\n",
    "\n",
    "def token_len(tokenizer, text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def compute_answer_budget(tokenizer, model, question, hyp_max_tokens, q_cap=128, safety_margin=12):\n",
    "    max_len = model_max_len(tokenizer, model)            # usually 512\n",
    "    specials = tokenizer.num_special_tokens_to_add(pair=True)\n",
    "    q_tokens = min(token_len(tokenizer, question), q_cap)\n",
    "    budget = max_len - specials - q_tokens - hyp_max_tokens - safety_margin\n",
    "    return max(32, budget)\n",
    "\n",
    "def chunk_answer_for_pair(tokenizer, answer, answer_budget, stride_tokens=128):\n",
    "    \"\"\"\n",
    "    Chunk the ANSWER using tokenizer.tokenize (no model max-length checks),\n",
    "    then stitch back to text with convert_tokens_to_string.\n",
    "    \"\"\"\n",
    "    toks = tokenizer.tokenize(answer)  # <-- avoids the max-length warning\n",
    "    if len(toks) <= answer_budget:\n",
    "        return [answer]\n",
    "\n",
    "    chunks, i = [], 0\n",
    "    while i < len(toks):\n",
    "        window_tokens = toks[i:i+answer_budget]\n",
    "        window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "        chunks.append(window_text)\n",
    "        if i + answer_budget >= len(toks):\n",
    "            break\n",
    "        i += max(1, answer_budget - stride_tokens)\n",
    "    return chunks\n",
    "\n",
    "def pair_logits_chunks(model, tokenizer, device, premise, hypothesis, max_length=None, stride=128):\n",
    "    if max_length is None:\n",
    "        max_length = model_max_len(tokenizer, model)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        premise,\n",
    "        hypothesis,\n",
    "        return_tensors='pt',\n",
    "        truncation='only_first',          # split/truncate Q+A only\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding='max_length'              # <-- add this\n",
    "    )\n",
    "\n",
    "    # keep only keys the model expects\n",
    "    input_names = set(getattr(tokenizer, \"model_input_names\",\n",
    "                              [\"input_ids\", \"attention_mask\", \"token_type_ids\"]))\n",
    "\n",
    "    def to_batch(enc_dict, i=None):\n",
    "        batch = {}\n",
    "        for k, v in enc_dict.items():\n",
    "            if k in input_names and isinstance(v, torch.Tensor):\n",
    "                batch[k] = (v[i:i+1] if i is not None else v).to(device)\n",
    "        return batch\n",
    "\n",
    "    # single chunk\n",
    "    if enc[\"input_ids\"].shape[0] == 1:\n",
    "        batch = to_batch(enc)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch).logits\n",
    "        return [logits.squeeze(0)]\n",
    "\n",
    "    # multiple overflowed chunks\n",
    "    logits_list = []\n",
    "    n = enc[\"input_ids\"].shape[0]\n",
    "    for i in range(n):\n",
    "        batch = to_batch(enc, i)\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch).logits\n",
    "        logits_list.append(out.squeeze(0))\n",
    "    return logits_list\n",
    "\n",
    "def get_label_idx(model, name, default):\n",
    "    id2label = getattr(model.config, \"id2label\", {})\n",
    "    if id2label:\n",
    "        for k, v in id2label.items():\n",
    "            if name in str(v).lower():\n",
    "                return int(k)\n",
    "    return default\n",
    "\n",
    "def p_entail_from_logits(logits, model, temperature=1.0):\n",
    "    nlab = logits.shape[-1]\n",
    "    ent_i = get_label_idx(model, \"entail\", 2 if nlab==3 else 1)\n",
    "    probs = torch.softmax(logits / float(temperature), dim=-1)\n",
    "    return float(probs[ent_i])\n",
    "\n",
    "# --- your templates (unchanged) ---\n",
    "DIRECT_TEMPLATES = [\n",
    "    \"The answer gives a direct and specific response to the question.\",\n",
    "    \"The answer addresses the question explicitly and concretely.\",\n",
    "    \"The answer responds directly with actionable specifics.\",\n",
    "]\n",
    "EVASIVE_TEMPLATES = [\n",
    "    \"The answer avoids giving a direct response to the question.\",\n",
    "    \"The answer is evasive or deflects without specifics.\",\n",
    "    \"The answer sidesteps the question and withholds details.\",\n",
    "]\n",
    "\n",
    "def llm_evasion_score(question, answer, model, tokenizer, device, temperature=2.0, stride=128):\n",
    "    max_len = model_max_len(tokenizer, model)\n",
    "    n_dir, n_eva = len(DIRECT_TEMPLATES), len(EVASIVE_TEMPLATES)\n",
    "\n",
    "    p_ent_direct_list, p_ent_evasive_list = [], []\n",
    "\n",
    "    premise = f\"Q: {question}\\nA: {answer}\"\n",
    "\n",
    "    # Collect P(entailment) for DIRECT hypotheses (over chunks), then mean over templates\n",
    "    for h in DIRECT_TEMPLATES:\n",
    "        logits_chunks = pair_logits_chunks(model, tokenizer, device, premise, h, max_length=max_len, stride=stride)\n",
    "        # For each chunk, compute P(entail); take the max across chunks (recall-friendly)\n",
    "        pents = [p_entail_from_logits(log, model, temperature) for log in logits_chunks]\n",
    "        p_ent_direct_list.append(max(pents))\n",
    "\n",
    "    # Same for EVASIVE hypotheses\n",
    "    for h in EVASIVE_TEMPLATES:\n",
    "        logits_chunks = pair_logits_chunks(model, tokenizer, device, premise, h, max_length=max_len, stride=stride)\n",
    "        pents = [p_entail_from_logits(log, model, temperature) for log in logits_chunks]\n",
    "        p_ent_evasive_list.append(max(pents))\n",
    "\n",
    "    # Mean over templates\n",
    "    p_ent_direct  = float(torch.tensor(p_ent_direct_list).mean())\n",
    "    p_ent_evasive = float(torch.tensor(p_ent_evasive_list).mean())\n",
    "\n",
    "    # Neutral-aware normalization (donâ€™t force a 2-class softmax over logits)\n",
    "    denom = p_ent_evasive + p_ent_direct + 1e-9\n",
    "    p_evasive = float(p_ent_evasive / denom)\n",
    "    p_direct  = 1.0 - p_evasive\n",
    "\n",
    "    return {\n",
    "        'p_direct': p_direct,\n",
    "        'p_evasive': p_evasive,\n",
    "        'p_ent_direct': p_ent_direct,\n",
    "        'p_ent_evasive': p_ent_evasive\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9aa9911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute blended evasion score and return all scores.\n",
    "def compute_all_evasion_scores(q, a, *, models_and_tokenizers=models_and_tokenizers, device, LLM_WEIGHT=0.30):\n",
    "    \n",
    "    # Compute baseline evasion score.\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # Individual LLM scores.\n",
    "    llm_scores = {}\n",
    "    for name, (m, t) in models_and_tokenizers.items():\n",
    "        scores = llm_evasion_score(q, a, m, t, device)\n",
    "        llm_scores[name] = float(100.0 * scores['p_evasive'])\n",
    "\n",
    "    # Ensemble LLM score.\n",
    "    llm_avg = float(np.mean(list(llm_scores.values()))) if llm_scores else 0.0\n",
    "\n",
    "    # Compute blended score.\n",
    "    blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * llm_avg, 0.0, 100.0))\n",
    "\n",
    "    return {\n",
    "        'baseline': base_score,\n",
    "        'llm_individual': llm_scores,\n",
    "        'llm_avg': llm_avg,\n",
    "        'blended': blended_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1fd47",
   "metadata": {},
   "source": [
    "### **Main Pipeline v1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c51f5",
   "metadata": {},
   "source": [
    "- v1 tests three LLM models, an average of these (ensemble) vs a baseline (rule-based) with a blended score of avg + baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a7618e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label 'Direct' or 'Evasive' based on the score.\n",
    "def label_from_score(score, threshold):\n",
    "    return 'Evasive' if score >= threshold else 'Direct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "81ea294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evasion Pipeline.\n",
    "def evasion_pipeline(df, models_and_tokenizers, device, LLM_WEIGHT, EVASION_THRESHOLD_BASE, EVASION_THRESHOLD_LLM, EVASION_THRESHOLD_BLENDED):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row['question']), str(row['answer'])\n",
    "        output = compute_all_evasion_scores(q=q, a=a, LLM_WEIGHT=LLM_WEIGHT, models_and_tokenizers=models_and_tokenizers, device=device)\n",
    "\n",
    "        pred_base = label_from_score(output['baseline'], EVASION_THRESHOLD_BASE)\n",
    "        pred_llm_avg = label_from_score(output['llm_avg'], EVASION_THRESHOLD_LLM)\n",
    "        pred_blended = label_from_score(output['blended'], EVASION_THRESHOLD_BLENDED)\n",
    "\n",
    "        record = {\n",
    "            'question_number': row.get('question_number'),\n",
    "            'question': q,\n",
    "            'answer': a,\n",
    "\n",
    "            # Evasion Scores\n",
    "            'evasion_score_baseline': int(output['baseline']),\n",
    "            'evasion_score_llm_avg': int(output['llm_avg']),\n",
    "            \"evasion_score_blended\": int(output['blended']),\n",
    "\n",
    "            # Predicted labels.\n",
    "            'prediction_baseline': pred_base,\n",
    "            'prediction_llm_avg': pred_llm_avg,\n",
    "            'prediction_blended': pred_blended,\n",
    "        }\n",
    "\n",
    "        for model_name, score in output['llm_individual'].items():\n",
    "            record[f'evasion_score_{model_name}'] = int(score)\n",
    "            record[f'prediction_{model_name}'] = label_from_score(score, EVASION_THRESHOLD_LLM)\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c0cfa",
   "metadata": {},
   "source": [
    "### **Fine-Tune Score Thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1e9f61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an initial run with preliminary threshold values.\n",
    "LLM_WEIGHT = 0.30\n",
    "EVASION_THRESHOLD_BASE = 30.0\n",
    "EVASION_THRESHOLD_LLM = 30.0\n",
    "EVASION_THRESHOLD_BLENDED = 30.0\n",
    "\n",
    "jpm_val_qa_scores = evasion_pipeline(\n",
    "    jpm_val_qa_labelled, \n",
    "    models_and_tokenizers, \n",
    "    device, \n",
    "    LLM_WEIGHT, \n",
    "    EVASION_THRESHOLD_BASE, \n",
    "    EVASION_THRESHOLD_LLM, \n",
    "    EVASION_THRESHOLD_BLENDED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "edc85f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_llm_avg</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_llm_avg</th>\n",
       "      <th>prediction_blended</th>\n",
       "      <th>evasion_score_roberta</th>\n",
       "      <th>prediction_roberta</th>\n",
       "      <th>evasion_score_deberta</th>\n",
       "      <th>prediction_deberta</th>\n",
       "      <th>evasion_score_zs_deberta</th>\n",
       "      <th>prediction_zs_deberta</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Hey, thanks. Good morning. Hey, Jeremy, I was ...</td>\n",
       "      <td>Yeah, sure. So let me just summarize the drive...</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>45</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>50</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>25</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Yeah, and as a follow-up on the point about ra...</td>\n",
       "      <td>Well first of all, I don't quite believe it. S...</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>40</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>81</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>70</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Hi, thanks. Jeremy, wanted to follow up again ...</td>\n",
       "      <td>Yeah. John, it's a really good question, and w...</td>\n",
       "      <td>78</td>\n",
       "      <td>67</td>\n",
       "      <td>74</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>54</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>84</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>63</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Okay. And then I wanted to ask Jamie â€“ there's...</td>\n",
       "      <td>Yeah. I wouldn't use the word credit crunch if...</td>\n",
       "      <td>55</td>\n",
       "      <td>67</td>\n",
       "      <td>59</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>58</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>69</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>74</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Hi. Good morning. My first question is you men...</td>\n",
       "      <td>Yeah. So, Erika, as you know, we take â€“ not go...</td>\n",
       "      <td>44</td>\n",
       "      <td>60</td>\n",
       "      <td>49</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>36</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>84</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>59</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0              2.0  Hey, thanks. Good morning. Hey, Jeremy, I was ...   \n",
       "1              3.0  Yeah, and as a follow-up on the point about ra...   \n",
       "2              4.0  Hi, thanks. Jeremy, wanted to follow up again ...   \n",
       "3              5.0  Okay. And then I wanted to ask Jamie â€“ there's...   \n",
       "4              6.0  Hi. Good morning. My first question is you men...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Yeah, sure. So let me just summarize the drive...                      80   \n",
       "1  Well first of all, I don't quite believe it. S...                      40   \n",
       "2  Yeah. John, it's a really good question, and w...                      78   \n",
       "3  Yeah. I wouldn't use the word credit crunch if...                      55   \n",
       "4  Yeah. So, Erika, as you know, we take â€“ not go...                      44   \n",
       "\n",
       "   evasion_score_llm_avg  evasion_score_blended prediction_baseline  \\\n",
       "0                     40                     68             Evasive   \n",
       "1                     64                     47             Evasive   \n",
       "2                     67                     74             Evasive   \n",
       "3                     67                     59             Evasive   \n",
       "4                     60                     49             Evasive   \n",
       "\n",
       "  prediction_llm_avg prediction_blended  evasion_score_roberta  \\\n",
       "0            Evasive            Evasive                     45   \n",
       "1            Evasive            Evasive                     40   \n",
       "2            Evasive            Evasive                     54   \n",
       "3            Evasive            Evasive                     58   \n",
       "4            Evasive            Evasive                     36   \n",
       "\n",
       "  prediction_roberta  evasion_score_deberta prediction_deberta  \\\n",
       "0            Evasive                     50            Evasive   \n",
       "1            Evasive                     81            Evasive   \n",
       "2            Evasive                     84            Evasive   \n",
       "3            Evasive                     69            Evasive   \n",
       "4            Evasive                     84            Evasive   \n",
       "\n",
       "   evasion_score_zs_deberta prediction_zs_deberta    label  \n",
       "0                        25                Direct   Direct  \n",
       "1                        70               Evasive   Direct  \n",
       "2                        63               Evasive   Direct  \n",
       "3                        74               Evasive   Direct  \n",
       "4                        59               Evasive  Evasive  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the results and reappend the label.\n",
    "jpm_val_qa_scores['label'] = jpm_val_qa_labelled['label'].values\n",
    "jpm_val_qa_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "426bbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth (1 = Evasive, 0 = Direct)\n",
    "def extract_y_true(df):\n",
    "    return (df['label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0c6d0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculate metrics for each threshold.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def tune_threshold(df, score_col, thr_grid):\n",
    "    y_true = extract_y_true(df)                     # get true labels\n",
    "    scores = df[score_col].astype(float).values     # get raw evasion scores \n",
    "\n",
    "    rows = []\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (scores >= thr).astype(int) # label response evasive (1) if score is higher than threshold\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        rows.append({\n",
    "            'threshold': float(thr),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    results = pd.DataFrame(rows).sort_values(\n",
    "        by=['f1', 'recall'],\n",
    "        ascending=[False, False]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "07ebd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold ranges around current thresholds.\n",
    "thr_base_grid = np.arange(40, 85, 5)\n",
    "thr_llm_grid = np.arange(35, 85, 5)\n",
    "thr_blend_grid = np.arange(40, 85, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e2db390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline / blended / avg LLM \n",
    "base_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_baseline', thr_base_grid)\n",
    "llm_avg_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_llm_avg', thr_llm_grid)\n",
    "blend_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_blended', thr_blend_grid)\n",
    "\n",
    "# Individual LLM models\n",
    "roberta_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_roberta', thr_llm_grid)\n",
    "deberta_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_deberta', thr_llm_grid)\n",
    "zs_deberta_results = tune_threshold(jpm_val_qa_scores, 'evasion_score_zs_deberta', thr_llm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b41c00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Baseline Threshold: 40.0\n",
      "Best avg LLM Threshold: 50.0\n",
      "Best Blended Threshold 40.0\n",
      "Best roberta Threshold: 35.0\n",
      "Best deberta Threshold: 60.0\n",
      "Best zs deberta Threshold 55.0\n"
     ]
    }
   ],
   "source": [
    "# Extract the best thresholds based on recall.\n",
    "best_base_thr = base_results.loc[0, 'threshold']\n",
    "best_avg_llm_thr = llm_avg_results.loc[0, 'threshold']\n",
    "best_blend_thr = blend_results.loc[0, 'threshold']\n",
    "\n",
    "best_roberta_thr = roberta_results.loc[0, 'threshold']\n",
    "best_deberta_thr = deberta_results.loc[0, 'threshold']\n",
    "best_zs_derberta_thr = zs_deberta_results.loc[0, 'threshold']\n",
    "\n",
    "print('Best Baseline Threshold:', best_base_thr)\n",
    "print('Best avg LLM Threshold:', best_avg_llm_thr)\n",
    "print('Best Blended Threshold', best_base_thr)\n",
    "\n",
    "print('Best roberta Threshold:', best_roberta_thr)\n",
    "print('Best deberta Threshold:', best_deberta_thr)\n",
    "print('Best zs deberta Threshold', best_zs_derberta_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "da6045e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 baseline configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       40.0   0.208955  1.000000  0.345679  0.258741\n",
      "1       45.0   0.183486  0.714286  0.291971  0.321678\n",
      "2       65.0   0.209677  0.464286  0.288889  0.552448\n",
      "3       70.0   0.236842  0.321429  0.272727  0.664336\n",
      "4       55.0   0.177778  0.571429  0.271186  0.398601\n",
      "\n",
      "Top 5 llm configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       50.0   0.211538  0.785714  0.333333  0.384615\n",
      "1       35.0   0.198529  0.964286  0.329268  0.230769\n",
      "2       40.0   0.198413  0.892857  0.324675  0.272727\n",
      "3       55.0   0.215190  0.607143  0.317757  0.489510\n",
      "4       45.0   0.198198  0.785714  0.316547  0.335664\n",
      "\n",
      "Top 5 blended configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       40.0   0.201439  1.000000  0.335329  0.223776\n",
      "1       65.0   0.250000  0.464286  0.325000  0.622378\n",
      "2       45.0   0.193548  0.857143  0.315789  0.272727\n",
      "3       50.0   0.183486  0.714286  0.291971  0.321678\n",
      "4       55.0   0.184783  0.607143  0.283333  0.398601\n",
      "\n",
      "Top 5 roberta configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       35.0   0.212389  0.857143  0.340426  0.349650\n",
      "1       40.0   0.195876  0.678571  0.304000  0.391608\n",
      "2       45.0   0.200000  0.500000  0.285714  0.510490\n",
      "3       55.0   0.250000  0.285714  0.266667  0.692308\n",
      "4       50.0   0.204545  0.321429  0.250000  0.622378\n",
      "\n",
      "Top 5 deberta configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       60.0   0.236559  0.785714  0.363636  0.461538\n",
      "1       65.0   0.232877  0.607143  0.336634  0.531469\n",
      "2       40.0   0.200000  1.000000  0.333333  0.216783\n",
      "3       35.0   0.197183  1.000000  0.329412  0.202797\n",
      "4       45.0   0.188406  0.928571  0.313253  0.202797\n",
      "\n",
      "Top 5 zs deberta configs:\n",
      "    threshold  precision    recall        f1  accuracy\n",
      "0       55.0   0.222222  0.714286  0.338983  0.454545\n",
      "1       70.0   0.245902  0.535714  0.337079  0.587413\n",
      "2       50.0   0.208333  0.714286  0.322581  0.412587\n",
      "3       60.0   0.214286  0.642857  0.321429  0.468531\n",
      "4       45.0   0.200000  0.750000  0.315789  0.363636\n"
     ]
    }
   ],
   "source": [
    "# Inspect trade-offs.\n",
    "print('\\nTop 5 baseline configs:\\n', base_results.head())\n",
    "print('\\nTop 5 llm configs:\\n', llm_avg_results.head())\n",
    "print('\\nTop 5 blended configs:\\n', blend_results.head())\n",
    "\n",
    "print('\\nTop 5 roberta configs:\\n', roberta_results.head())\n",
    "print('\\nTop 5 deberta configs:\\n', deberta_results.head())\n",
    "print('\\nTop 5 zs deberta configs:\\n', zs_deberta_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57694f63",
   "metadata": {},
   "source": [
    "- Best balanced performance of LLM models was the deberta model with threshold = 60, giving 79% recall, 46% accuracy and F1 = 0.36.\n",
    "- Use baseline threshold = 0.40 as baseline detector as this gave the highest F1 score across the grid search, giving the most balanced model and so is the fairest comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9314b",
   "metadata": {},
   "source": [
    "### **Main Pipeline v2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10a6dd",
   "metadata": {},
   "source": [
    "- This pipeline incorperates the results from the validation threshold tuning and best model performances \n",
    "- Updates some of the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5d70b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_all_evasion_scores_v2(q, a, *, models_and_tokenizers, device, LLM_WEIGHT=0.30):\n",
    "#     # Baseline score\n",
    "#     base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "#     # Get RoBERTa from dict.\n",
    "#     if 'roberta' in models_and_tokenizers:\n",
    "#         roberta_model, roberta_tok = models_and_tokenizers['roberta']\n",
    "#     else:\n",
    "#         candidates = [k for k in models_and_tokenizers.keys() if 'roberta' in k.lower()]\n",
    "#         if not candidates:\n",
    "#             raise ValueError(\"RoBERTa model not found in models_and_tokenizers. Expected a key containing 'roberta'.\")\n",
    "#         roberta_model, roberta_tok = models_and_tokenizers[candidates[0]]\n",
    "\n",
    "#     # RoBERTa LLM score\n",
    "#     r_scores = llm_evasion_score(q, a, roberta_model, roberta_tok, device)\n",
    "#     roberta_score = float(100.0 * r_scores['p_evasive'])\n",
    "\n",
    "#     # Blended (Baseline <-> RoBERTa)\n",
    "#     blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * roberta_score, 0.0, 100.0))\n",
    "\n",
    "#     return {\n",
    "#         'baseline': base_score,\n",
    "#         'roberta': roberta_score,\n",
    "#         'blended': blended_score\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entailment_logit(model, tokenizer, device, premise, hypothesis):\n",
    "#     model.eval()\n",
    "#     enc = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, max_length=512)\n",
    "#     enc = {k: v.to(device) for k, v in enc.items()}\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(**enc).logits.squeeze(0)\n",
    "#     n = logits.shape[-1]\n",
    "#     if n == 3:\n",
    "#         return float(logits[2])  # index 2 = entailment\n",
    "#     elif n == 2:\n",
    "#         return float(logits[1])  # index 1 ~ entailment\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected num_labels={n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def llm_evasion_score_v2(question, answer, model, tokenizer, device):\n",
    "#     premise   = f\"[QUESTION] {question} [ANSWER] {answer}\"\n",
    "#     H_DIRECT  = \"The answer gives a direct and specific response to the question.\"\n",
    "#     H_EVASIVE = \"The answer avoids giving a direct response to the question.\"\n",
    "\n",
    "#     s_direct  = entailment_logit(model, tokenizer, device, premise, H_DIRECT)\n",
    "#     s_evasive = entailment_logit(model, tokenizer, device, premise, H_EVASIVE)\n",
    "\n",
    "#     # Pairwise softmax over entailment logits\n",
    "#     s = torch.tensor([s_direct, s_evasive])\n",
    "#     p = F.softmax(s, dim=0).tolist()\n",
    "#     return {'p_direct': p[0], 'p_evasive': p[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ee7e2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_evasion_scores_v2(q, a, *, models_and_tokenizers, device, LLM_WEIGHT=0.30):\n",
    "    base_score, _, _, _, _ = baseline_evasion_score(q, a)\n",
    "\n",
    "    # pick deberta\n",
    "    if 'deberta' in models_and_tokenizers:\n",
    "        deberta_model, deberta_tok = models_and_tokenizers['deberta']\n",
    "    else:\n",
    "        k = next(k for k in models_and_tokenizers if 'deberta' in k.lower())\n",
    "        deberta_model, deberta_tok = models_and_tokenizers[k]\n",
    "\n",
    "    r = llm_evasion_score(q, a, deberta_model, deberta_tok, device)\n",
    "    deberta_score = float(100.0 * r['p_evasive'])\n",
    "\n",
    "    blended_score = float(np.clip((1.0 - LLM_WEIGHT) * base_score + LLM_WEIGHT * deberta_score, 0.0, 100.0))\n",
    "    return {'baseline': base_score, 'deberta': deberta_score, 'blended': blended_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5b4ca14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evasion pipeline v2 \n",
    "def evasion_pipeline_v2(df, models_and_tokenizers, device, LLM_WEIGHT, EVASION_THRESHOLD_BASE, EVASION_THRESHOLD_DEBERTA, EVASION_THRESHOLD_BLENDED):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        q, a = str(row['question']), str(row['answer'])\n",
    "        output = compute_all_evasion_scores_v2(\n",
    "            q=q, a=a,\n",
    "            LLM_WEIGHT=LLM_WEIGHT,\n",
    "            models_and_tokenizers=models_and_tokenizers,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        pred_base    = label_from_score(output['baseline'], EVASION_THRESHOLD_BASE)\n",
    "        pred_deberta = label_from_score(output['deberta'], EVASION_THRESHOLD_DEBERTA)\n",
    "        pred_blended = label_from_score(output['blended'], EVASION_THRESHOLD_BLENDED)\n",
    "\n",
    "        record = {\n",
    "            'question_number': row.get('question_number'),\n",
    "            'question': q,\n",
    "            'answer': a,\n",
    "\n",
    "            # Scores\n",
    "            'evasion_score_baseline': int(output['baseline']),\n",
    "            'evasion_score_deberta': int(output['deberta']),\n",
    "            'evasion_score_blended': int(output['blended']),\n",
    "\n",
    "            # Predictions\n",
    "            'prediction_baseline': pred_base,\n",
    "            'prediction_deberta': pred_deberta,\n",
    "            'prediction_blended': pred_blended,\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "04a43fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>role_normalised</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>label</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, we've told you that we're kind of pencil...</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>So, as you think about all of what you've just...</td>\n",
       "      <td>Okay. Let's take a crack. Let's see what the b...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hi, good morning. I guess, maybe one question,...</td>\n",
       "      <td>Yeah, so Ebrahim let me sort of respond narrow...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>2023</td>\n",
       "      <td>Q1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hi, good morning. I guess, maybe one question,...</td>\n",
       "      <td>Yeah. And then in terms of the office space, a...</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>banker</td>\n",
       "      <td>data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2023_Q1_13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year quarter  question_number  answer_number  \\\n",
       "0           0  2023      Q1              1.0            1.0   \n",
       "1           2  2023      Q1              1.0            3.0   \n",
       "2          10  2023      Q1              7.0            2.0   \n",
       "3          19  2023      Q1             13.0            1.0   \n",
       "4          21  2023      Q1             13.0            3.0   \n",
       "\n",
       "                                            question  \\\n",
       "0  So, Jamie, I was actually hoping to get your p...   \n",
       "1  So, Jamie, I was actually hoping to get your p...   \n",
       "2  So, as you think about all of what you've just...   \n",
       "3  Hi, good morning. I guess, maybe one question,...   \n",
       "4  Hi, good morning. I guess, maybe one question,...   \n",
       "\n",
       "                                              answer   speaker_name  \\\n",
       "0  Well, I think you were already kind of complet...    Jamie Dimon   \n",
       "1  Well, we've told you that we're kind of pencil...    Jamie Dimon   \n",
       "2  Okay. Let's take a crack. Let's see what the b...  Jeremy Barnum   \n",
       "3  Yeah, so Ebrahim let me sort of respond narrow...  Jeremy Barnum   \n",
       "4  Yeah. And then in terms of the office space, a...  Jeremy Barnum   \n",
       "\n",
       "                                 role role_normalised  \\\n",
       "0  Chairman & Chief Executive Officer          banker   \n",
       "1  Chairman & Chief Executive Officer          banker   \n",
       "2             Chief Financial Officer          banker   \n",
       "3             Chief Financial Officer          banker   \n",
       "4             Chief Financial Officer          banker   \n",
       "\n",
       "                                          source_pdf   label      group_id  \n",
       "0  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct   2023_Q1_1.0  \n",
       "1  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct   2023_Q1_1.0  \n",
       "2  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct   2023_Q1_7.0  \n",
       "3  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct  2023_Q1_13.0  \n",
       "4  data/raw/jpm/.ipynb_checkpoints/jpm-1q23-earni...  Direct  2023_Q1_13.0  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View test dataset.\n",
    "jpm_test_qa_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a687e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evasion pipeline v2 with test dataset. \n",
    "LLM_WEIGHT = 0.70\n",
    "EVASION_THRESHOLD_BASE = 40.0\n",
    "EVASION_THRESHOLD_DEBERTA = 60.0\n",
    "EVASION_THRESHOLD_BLENDED = 40.0\n",
    "\n",
    "jpm_test_qa_scores = evasion_pipeline_v2(\n",
    "    jpm_test_qa_labelled,\n",
    "    models_and_tokenizers,\n",
    "    device,\n",
    "    LLM_WEIGHT,\n",
    "    EVASION_THRESHOLD_BASE,\n",
    "    EVASION_THRESHOLD_DEBERTA,\n",
    "    EVASION_THRESHOLD_BLENDED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73432c72",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a03e5648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evasion_score_baseline</th>\n",
       "      <th>evasion_score_deberta</th>\n",
       "      <th>evasion_score_blended</th>\n",
       "      <th>prediction_baseline</th>\n",
       "      <th>prediction_deberta</th>\n",
       "      <th>prediction_blended</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, I think you were already kind of complet...</td>\n",
       "      <td>73</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>So, Jamie, I was actually hoping to get your p...</td>\n",
       "      <td>Well, we've told you that we're kind of pencil...</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>So, as you think about all of what you've just...</td>\n",
       "      <td>Okay. Let's take a crack. Let's see what the b...</td>\n",
       "      <td>35</td>\n",
       "      <td>88</td>\n",
       "      <td>72</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.0</td>\n",
       "      <td>Hi, good morning. I guess, maybe one question,...</td>\n",
       "      <td>Yeah, so Ebrahim let me sort of respond narrow...</td>\n",
       "      <td>82</td>\n",
       "      <td>53</td>\n",
       "      <td>62</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>Hi, good morning. I guess, maybe one question,...</td>\n",
       "      <td>Yeah. And then in terms of the office space, a...</td>\n",
       "      <td>72</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Evasive</td>\n",
       "      <td>Direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number                                           question  \\\n",
       "0              1.0  So, Jamie, I was actually hoping to get your p...   \n",
       "1              1.0  So, Jamie, I was actually hoping to get your p...   \n",
       "2              7.0  So, as you think about all of what you've just...   \n",
       "3             13.0  Hi, good morning. I guess, maybe one question,...   \n",
       "4             13.0  Hi, good morning. I guess, maybe one question,...   \n",
       "\n",
       "                                              answer  evasion_score_baseline  \\\n",
       "0  Well, I think you were already kind of complet...                      73   \n",
       "1  Well, we've told you that we're kind of pencil...                      49   \n",
       "2  Okay. Let's take a crack. Let's see what the b...                      35   \n",
       "3  Yeah, so Ebrahim let me sort of respond narrow...                      82   \n",
       "4  Yeah. And then in terms of the office space, a...                      72   \n",
       "\n",
       "   evasion_score_deberta  evasion_score_blended prediction_baseline  \\\n",
       "0                     58                     62             Evasive   \n",
       "1                     62                     58             Evasive   \n",
       "2                     88                     72              Direct   \n",
       "3                     53                     62             Evasive   \n",
       "4                     58                     62             Evasive   \n",
       "\n",
       "  prediction_deberta prediction_blended   label  \n",
       "0             Direct            Evasive  Direct  \n",
       "1            Evasive            Evasive  Direct  \n",
       "2            Evasive            Evasive  Direct  \n",
       "3             Direct            Evasive  Direct  \n",
       "4             Direct            Evasive  Direct  "
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results.\n",
    "jpm_test_qa_scores['label'] = jpm_test_qa_labelled['label'].values\n",
    "jpm_test_qa_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fd9831d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the evasion scores vs true labels.\n",
    "def evaluate_evasion_scores(df):\n",
    "\n",
    "    # True labels: 1 = Evasive, 0 = Direct (using 'human_label').\n",
    "    y_true = (df['label'].astype(str).str.strip().str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "    # Convert predicted label strings to binary (1 = Evasive, 0 = Direct).\n",
    "    def to_binary(pred_series):\n",
    "        return (pred_series.astype(str).str.strip().str.lower() == 'evasive').astype(int).values\n",
    "\n",
    "    # Convert predicted labels to binary.\n",
    "    y_pred_base  = to_binary(df['prediction_baseline'])\n",
    "    y_pred_deberta   = to_binary(df['prediction_deberta'])\n",
    "    y_pred_blend = to_binary(df['prediction_blended'])\n",
    "\n",
    "    return {\n",
    "        'baseline': {\n",
    "            'classification_report': classification_report(y_true, y_pred_base, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_base)\n",
    "        },\n",
    "        'deberta': {\n",
    "            'classification_report': classification_report(y_true, y_pred_deberta, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_deberta)\n",
    "        },\n",
    "        'blended': {\n",
    "            'classification_report': classification_report(y_true, y_pred_blend, target_names=[\"Direct\", \"Evasive\"], digits=3, zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred_blend) \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "adf27b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results.\n",
    "eval_dict = evaluate_evasion_scores(jpm_test_qa_scores)\n",
    "baseline_eval, deberta_eval, blended_eval = eval_dict['baseline'], eval_dict['deberta'], eval_dict['blended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4d02cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      1.000     0.155     0.269        58\n",
      "     Evasive      0.222     1.000     0.364        14\n",
      "\n",
      "    accuracy                          0.319        72\n",
      "   macro avg      0.611     0.578     0.316        72\n",
      "weighted avg      0.849     0.319     0.287        72\n",
      "\n",
      "[[ 9 49]\n",
      " [ 0 14]]\n"
     ]
    }
   ],
   "source": [
    "# View baseline results.\n",
    "base_cr, base_cm = baseline_eval['classification_report'], baseline_eval['confusion_matrix']\n",
    "\n",
    "print(base_cr)\n",
    "print(base_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "47f021d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      0.857     0.414     0.558        58\n",
      "     Evasive      0.227     0.714     0.345        14\n",
      "\n",
      "    accuracy                          0.472        72\n",
      "   macro avg      0.542     0.564     0.451        72\n",
      "weighted avg      0.735     0.472     0.517        72\n",
      "\n",
      "[[24 34]\n",
      " [ 4 10]]\n"
     ]
    }
   ],
   "source": [
    "# View deberta results.\n",
    "deberta_cr, deberta_cm = deberta_eval['classification_report'], deberta_eval['confusion_matrix']\n",
    "\n",
    "print(deberta_cr)\n",
    "print(deberta_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b919c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Direct      1.000     0.017     0.034        58\n",
      "     Evasive      0.197     1.000     0.329        14\n",
      "\n",
      "    accuracy                          0.208        72\n",
      "   macro avg      0.599     0.509     0.182        72\n",
      "weighted avg      0.844     0.208     0.091        72\n",
      "\n",
      "[[ 1 57]\n",
      " [ 0 14]]\n"
     ]
    }
   ],
   "source": [
    "# View blended results.\n",
    "blended_cr, blended_cm = blended_eval['classification_report'], blended_eval['confusion_matrix']\n",
    "\n",
    "print(blended_cr)\n",
    "print(blended_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
