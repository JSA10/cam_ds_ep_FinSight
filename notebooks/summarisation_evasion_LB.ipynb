{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b820ac6",
   "metadata": {},
   "source": [
    "# **Summarisation & Evasion Notebook (LB)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79cb78c",
   "metadata": {},
   "source": [
    "# **1. Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b76a8",
   "metadata": {},
   "source": [
    "- Summarise banker answers into short, PRA relevant insights.\n",
    "- Generate an evasion score to tag summaries.\n",
    "- RAG pipeline to bring in relevant external documents (e.g. PRA risk definitions, regulatory news).\n",
    "- Optional extension: validate flagged risks with external/regulatory news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94670282",
   "metadata": {},
   "source": [
    "# **2. Set up Workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f5884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/nlp-evasion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Core python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any \n",
    "import csv\n",
    "\n",
    "# NLP & Summarisation\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import spacy\n",
    "from llama_cpp import Llama \n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "from bert_score import score as bertscore \n",
    "\n",
    "# Retrieval\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import faiss\n",
    "import chromadb\n",
    "import langchain\n",
    "import llama_index\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91bb8c",
   "metadata": {},
   "source": [
    "# **3. Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743f6bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "jpm_2025_df = pd.read_csv('../data/processed/jpm/all_jpm_2025.csv')\n",
    "\n",
    "# View the data.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75628c",
   "metadata": {},
   "source": [
    "# **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc49b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.', 'Okay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View speaker roles.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec70c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>And then some. Theres a lot of value added.</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah. And obviously, I mean, we're not going t...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Chief Financial Officer, JPMorganChase</td>\n",
       "      <td>Okay</td>\n",
       "      <td>there you have it.</td>\n",
       "      <td>But it's not like I thought it would do badly,...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q2</td>\n",
       "      <td>data/raw/jpm/jpm-2q25-earnings-call-transcript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_number  answer_number                            speaker_name  \\\n",
       "201               35            5.0  Chief Financial Officer, JPMorganChase   \n",
       "205               36            3.0  Chief Financial Officer, JPMorganChase   \n",
       "\n",
       "                                            role             company  \\\n",
       "201  And then some. Theres a lot of value added.       JPMorganChase   \n",
       "205                                         Okay  there you have it.   \n",
       "\n",
       "                                               content  year quarter  \\\n",
       "201  Yeah. And obviously, I mean, we're not going t...  2025      Q2   \n",
       "205  But it's not like I thought it would do badly,...  2025      Q2   \n",
       "\n",
       "                                            source_pdf  \n",
       "201  data/raw/jpm/jpm-2q25-earnings-call-transcript...  \n",
       "205  data/raw/jpm/jpm-2q25-earnings-call-transcript...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View rows with invalid roles.\n",
    "valid_roles = 'analyst', 'Chief Financial Officer', 'Chairman & Chief Executive Officer'\n",
    "invalid_roles_df = jpm_2025_df[~jpm_2025_df['role'].isin(valid_roles)]\n",
    "\n",
    "# Number of rows with invalid roles.\n",
    "print('Number of rows:', invalid_roles_df.shape[0])\n",
    "\n",
    "# View the rows.\n",
    "invalid_roles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e31cc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyst', 'Chief Financial Officer',\n",
       "       'Chairman & Chief Executive Officer',\n",
       "       'And then some. Theres a lot of value added.'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the correct role information.\n",
    "jpm_2025_df.at[205, 'role'] = 'Chief Financial Officer'\n",
    "jpm_2025_df.at[209, 'role'] = 'Chief Financial Officer'\n",
    "\n",
    "# Verify the roles have been updated.\n",
    "jpm_2025_df['role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role mapping.\n",
    "role_map = {\n",
    "    'analyst': 'analyst',\n",
    "    'Chief Financial Officer': 'banker',\n",
    "    'Chairman & Chief Executive Officer': 'banker'\n",
    "}\n",
    "\n",
    "# Apply to dataset.\n",
    "jpm_2025_df['role_normalised'] = jpm_2025_df['role'].map(role_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755f26e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_number</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>role</th>\n",
       "      <th>company</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>source_pdf</th>\n",
       "      <th>role_normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Good morning, Jeremy. Wondering if you could s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Sure, Ken. So I mean, at a high level, I would...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ken Usdin</td>\n",
       "      <td>analyst</td>\n",
       "      <td>Autonomous Research</td>\n",
       "      <td>Yeah. And just one question on the NII ex. Mar...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jeremy Barnum</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>Yeah, that's a good question, Ken. You're righ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jamie Dimon</td>\n",
       "      <td>Chairman &amp; Chief Executive Officer</td>\n",
       "      <td>JPMorganChase</td>\n",
       "      <td>In the curve basically.</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q1</td>\n",
       "      <td>data/raw/jpm/jpm-1q25-earnings-call-transcript...</td>\n",
       "      <td>banker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_number  answer_number   speaker_name  \\\n",
       "0                1            NaN      Ken Usdin   \n",
       "1                1            1.0  Jeremy Barnum   \n",
       "2                2            NaN      Ken Usdin   \n",
       "3                2            1.0  Jeremy Barnum   \n",
       "4                2            2.0    Jamie Dimon   \n",
       "\n",
       "                                 role              company  \\\n",
       "0                             analyst  Autonomous Research   \n",
       "1             Chief Financial Officer        JPMorganChase   \n",
       "2                             analyst  Autonomous Research   \n",
       "3             Chief Financial Officer        JPMorganChase   \n",
       "4  Chairman & Chief Executive Officer        JPMorganChase   \n",
       "\n",
       "                                             content  year quarter  \\\n",
       "0  Good morning, Jeremy. Wondering if you could s...  2025      Q1   \n",
       "1  Sure, Ken. So I mean, at a high level, I would...  2025      Q1   \n",
       "2  Yeah. And just one question on the NII ex. Mar...  2025      Q1   \n",
       "3  Yeah, that's a good question, Ken. You're righ...  2025      Q1   \n",
       "4                            In the curve basically.  2025      Q1   \n",
       "\n",
       "                                          source_pdf role_normalised  \n",
       "0  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "1  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "2  data/raw/jpm/jpm-1q25-earnings-call-transcript...         analyst  \n",
       "3  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  \n",
       "4  data/raw/jpm/jpm-1q25-earnings-call-transcript...          banker  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset.\n",
    "jpm_2025_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf565",
   "metadata": {},
   "source": [
    "# **5. Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401764b",
   "metadata": {},
   "source": [
    "## **5.1 Baseline**\n",
    "- Model = BART (not instruction tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the w\n"
     ]
    }
   ],
   "source": [
    "# Filter data to banker answers only.\n",
    "banker_answers = jpm_2025_df[jpm_2025_df['role_normalised'] == 'banker']['content'].tolist()\n",
    "print(banker_answers[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54559054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: The main thing that we see there, what would appear to be a certain amount of frontloading of spending ahead of people expecting price increases from tariffs. So ironically, that's actually somewhat supportive, all else equal. In terms of our corporate clients, obviously, they've been reacting to the changes in tariff policy.\n"
     ]
    }
   ],
   "source": [
    "# Summarisation baseline (BART)\n",
    "summariser = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "\n",
    "sample_text = banker_answers[0]\n",
    "summary = summariser(sample_text, max_length=80, min_length=30, do_sample=False)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024bd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sure, Ken. So I mean, at a high level, I would say that obviously, some of the salient news flow is quite recent. So, we've done some soundings and some checking both on the consumer side and on the wholesale side. I think on the consumer side, the thing to check is the spending data. And to be honest, the main thing that we see there, what would appear to be a certain amount of frontloading of sp\n",
      "Summary: Corporates are taking a wait-and-see approach to tariff policy. Some sectors are going to be much more exposed than others. Small business and smaller corporates are probably a little more challenged.\n"
     ]
    }
   ],
   "source": [
    "# Prompt conditioning to make PRA relevant.\n",
    "prompt = \"Summarise this answer, focusing on risk, capital and evasion of detail: \" + sample_text\n",
    "summary = summariser(prompt, max_length=80, min_length=30)\n",
    "print('Original:', sample_text[:400])\n",
    "print('Summary:', summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813acc2",
   "metadata": {},
   "source": [
    "## **5.2 Adding Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1b7a",
   "metadata": {},
   "source": [
    "Retrieve PRA risk categories to give greater PRA focus to summaries (local RAG loop).\n",
    "- measure cosine similarity between transcript chunks and PRA risk categories (vectors)\n",
    "- retrieve the top 2-3 most relevant risk categories \n",
    "- prepend them to the summarisation prompt to make summaries PRA-aligned instead of just summarised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2bd1",
   "metadata": {},
   "source": [
    "- Attempting to use BART resulted in prompt echoing.\n",
    "- New attempt using Mistral-7B-Instruct.\n",
    "- Using sentence-BERT vs TF-IDF for vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b05ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove whitespace in text.\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f9ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the transcript into smaller chunks.\n",
    "def chunk_text(text: str, max_chars: int = 6000):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split into sentences \n",
    "    chunks, current_chunk, current_len = [], [], 0 # list of chunks, sentences collecting for current chunk, character count for current chunk\n",
    "\n",
    "    for s in sentences:\n",
    "        if current_len + len(s) + 1 <= max_chars: # if the characters of current chunk + new sentence is below the limit:\n",
    "            current_chunk.append(s) # add sentence to current chunk \n",
    "            current_len += len(s) + 1 # update running character count \n",
    "        \n",
    "        else: # if the characters is above the limit:\n",
    "            chunks.append(' '.join(current_chunk)) # add the current chunk to the final chunk list\n",
    "            current_chunk, current_len = [s], len(s) # start a new chunk containing the sentence and update current len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk)) # add any sentences in current chunk after loop ends \n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PRA categories and definitions from CSV.\n",
    "def load_pra_categories(path: Path):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            (row.get('category', '').strip(), [row.get('definition', '').strip()])\n",
    "            for row in reader if row.get('category')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sentence-BERT embedding index for PRA categories.\n",
    "def build_embedding_index(pra_categories):\n",
    "    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    docs = [f\"{name} {' '.join(defs)}\" for name, defs in pra_categories]\n",
    "    pra_risk_embeddings = embedder.encode(docs, batch_size=32, normalize_embeddings=True)\n",
    "\n",
    "    return embedder, np.asarray(pra_risk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the relevant PRA categories to the transcript chunks.\n",
    "def find_rel_categories(chunk, pra_categories, embedder, pra_risk_embeddings, top_k=2):\n",
    "    query_vec = embedder.encode([chunk], normalize_embeddings=True) # turns chunk into embedding\n",
    "    sims = cosine_similarity(query_vec, pra_risk_embeddings).ravel() # compares the chunk to each category doc \n",
    "    top_indices = np.argsort(-sims)[:top_k] # sorts scores descending and selected top k cateogories \n",
    "\n",
    "    return [pra_categories[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7d85bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_json_block(model_output: str) -> str:\n",
    "#     \"\"\"If the model adds extra text, grab the first JSON object.\"\"\"\n",
    "#     a = model_output.find(\"{\")\n",
    "#     b = model_output.rfind(\"}\")\n",
    "#     return model_output[a:b+1] if a != -1 and b != -1 and b > a else model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d0fa31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON instructions for the output file.\n",
    "JSON_INSTRUCTIONS = \"\"\"\\\n",
    "You are a careful data extraction model.\n",
    "Return ONLY valid minified JSON with EXACT keys: summary, evidence, pra_categories.\n",
    "Schema:\n",
    "{\n",
    "  \"summary\": \"string (4-6 sentences, neutral tone)\",\n",
    "  \"evidence\": [\"string\", ...],               // up to N quotes/facts\n",
    "  \"pra_categories\": [                        // 1-3 items\n",
    "    {\"name\": \"string\", \"why\": \"string\"},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "Rules:\n",
    "- Use standard double quotes; no trailing commas; no markdown or prose.\n",
    "- If something is unknown, return an empty string or [] (do NOT omit keys).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ead73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PLACEHOLDER]\n",
    "\n",
    "def _repair_json_string(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = s.strip().replace(\"\\r\\n\", \"\\n\").lstrip(\"\\ufeff\")\n",
    "\n",
    "    # Prefer <json>...</json> blocks if present\n",
    "    m = re.search(r\"<json>\\s*(\\{[\\s\\S]*?\\})\\s*</json>\", t, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        t = m.group(1).strip()\n",
    "    else:\n",
    "        # If fenced code exists, prefer ```json ... ``` then ``` ... ```\n",
    "        m = re.search(r\"```json\\s*([\\s\\S]*?)```\", t, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            m = re.search(r\"```\\s*([\\s\\S]*?)```\", t)\n",
    "        if m:\n",
    "            t = m.group(1).strip()\n",
    "\n",
    "        # As a last resort, grab the *first* minimal JSON object\n",
    "        if not m:\n",
    "            m = re.search(r\"\\{[\\s\\S]*?\\}\", t)\n",
    "            if m: t = m.group(0).strip()\n",
    "\n",
    "    # Normalise quotes and remove trailing commas before } or ]\n",
    "    t = t.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    t = re.sub(r\",\\s*([}\\]])\", r\"\\1\", t)\n",
    "    return t\n",
    "\n",
    "def _coerce_schema(obj: dict, max_evidence: int) -> dict:\n",
    "    \"\"\"Ensure keys exist and types are consistent.\"\"\"\n",
    "    out = {\n",
    "        \"summary\": obj.get(\"summary\") or \"\",\n",
    "        \"evidence\": obj.get(\"evidence\") or [],\n",
    "        # accept both pra_categories and \"pra categories\"\n",
    "        \"pra_categories\": obj.get(\"pra_categories\") or obj.get(\"pra categories\") or [],\n",
    "    }\n",
    "\n",
    "    # evidence -> list[str], capped\n",
    "    if not isinstance(out[\"evidence\"], list):\n",
    "        out[\"evidence\"] = [str(out[\"evidence\"])]\n",
    "    out[\"evidence\"] = [str(x).strip() for x in out[\"evidence\"] if str(x).strip()][:max_evidence]\n",
    "\n",
    "    # pra_categories -> list[{name, why}]\n",
    "    pcs = []\n",
    "    for c in out[\"pra_categories\"]:\n",
    "        if isinstance(c, dict):\n",
    "            pcs.append({\"name\": str(c.get(\"name\") or \"\"), \"why\": str(c.get(\"why\") or \"\")})\n",
    "        else:\n",
    "            pcs.append({\"name\": str(c), \"why\": \"\"})\n",
    "    out[\"pra_categories\"] = pcs\n",
    "    return out\n",
    "\n",
    "def _parse_llm_json(text: str, max_evidence: int) -> dict:\n",
    "    t = _repair_json_string(text)\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        return _coerce_schema(obj, max_evidence)\n",
    "    except Exception:\n",
    "        # Never return raw; return a well-formed empty structure\n",
    "        return _coerce_schema({}, max_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7ff3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PLACEHOLDER]\n",
    "def summarise_chunk(model, chunk, relevant_categories, chunk_idx, max_evidence=5):\n",
    "    # Build PRA notes block\n",
    "    lines = []\n",
    "    for name, definition in relevant_categories:\n",
    "        lines.append(f\"- {name}:\")\n",
    "        for d in list(definition)[:2]:\n",
    "            lines.append(f\"- {d}\")\n",
    "    notes_block = \"\\n\".join(lines)\n",
    "\n",
    "    system_prompt = (\n",
    "        JSON_INSTRUCTIONS\n",
    "        + \"\\nYou are a Data Scientist producing PRA-aligned summaries. \"\n",
    "          \"Use only the transcript text as evidence. Map content to the PRA categories provided.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "TRANSCRIPT:\n",
    "{clean_text(chunk)}\n",
    "\n",
    "PRA NOTES:\n",
    "{clean_text(notes_block)}\n",
    "\n",
    "TASK:\n",
    "Return JSON ONLY, wrapped exactly like this:\n",
    "<json>{\"summary\": \"...\", \"evidence\": [\"...\"], \"pra_categories\": [{\"name\":\"...\",\"why\":\"...\"}]}</json>\n",
    "\n",
    "RULES:\n",
    "- 4-6 sentence neutral summary.\n",
    "- Up to {max_evidence} evidence bullets (quotes/facts).\n",
    "- 1-3 pra_categories objects.\n",
    "- If evidence is lacking, use a single bullet \"Insufficient evidence\".\n",
    "- Only choose categories supported by the evidence.\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = model.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": clean_text(user_prompt)},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=700,   # keep headroom to avoid truncation\n",
    "        repeat_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    text = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    return _parse_llm_json(text, max_evidence, debug_prefix=f'chunk_{chunk_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77206958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to summarise the chunk.\n",
    "# def summarise_chunk(model, chunk, relevant_categories, max_evidence=5):\n",
    "#     lines = []\n",
    "#     for name, definition in relevant_categories:\n",
    "#         lines.append(f'- {name}:')\n",
    "#         for d in definition:\n",
    "#             lines.append(f'- {d}')\n",
    "#     notes_block = '\\n'.join(lines)\n",
    "\n",
    "#     system_prompt = (\n",
    "#         JSON_INSTRUCTIONS\n",
    "#         + '\\nYou are a Data Scientist producing PRA-aligned summaries.'\n",
    "#         'Use only the transcript text as evidence. Map content to the PRA categories provided.'\n",
    "#     )\n",
    "\n",
    "#     user_prompt = f\"\"\"\n",
    "# TRANSCRIPT:\n",
    "# {chunk}\n",
    "\n",
    "# PRA NOTES:\n",
    "# {notes_block}\n",
    "\n",
    "# TASK:\n",
    "# Return STRICT JSON with keys:\n",
    "# - evidence: list of up to {max_evidence} bullet strings\n",
    "# - pra_categories: list of objects {{ 'name': '...', 'why': '...' }}\n",
    "# - summary: 4-6 sentences, neutral tone\n",
    "\n",
    "# RULES:\n",
    "# - Do not invent facts. If evidence is lacking, use a single bullet 'Insufficient evidence'.\n",
    "# - Only choose categories supported by the evidence.\n",
    "# - No markdown. Output JSON only.\n",
    "# \"\"\"\n",
    "    \n",
    "#     response = model.create_chat_completion(\n",
    "#         messages=[\n",
    "#             {'role': 'system', 'content': system_prompt},\n",
    "#             {'role': 'user', 'content': clean_text(user_prompt)}\n",
    "#         ],\n",
    "#         temperature=0.2,\n",
    "#         max_tokens=700,\n",
    "#         repeat_penalty=1.1\n",
    "#     ) \n",
    "\n",
    "#     text = response['choices'][0]['message']['content'].strip()\n",
    "#     try:\n",
    "#         return json.loads(extract_json_block(text))\n",
    "#     except Exception:\n",
    "#         return {'raw': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e398a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables.\n",
    "MODEL_PATH = '/Users/laurenbrixey/Documents/Data Science Career Accelerator/Project Submissions/Course 3/topic_project_4.1/mistral-7b-instruct-v0.1.Q4_K_M.gguf'\n",
    "PRA_NOTES_PATH = '../data/RAG-resources/PRA_risk_categories.csv'\n",
    "TRANSCRIPT_PATH = '../data/processed/jpm/all_jpm_2025.csv'\n",
    "OUTPUT_PATH = \"jpm_mistral_pra_summary.json\"\n",
    "TOP_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3c7cff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pra_risk_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Runner code.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pra_categories \u001b[38;5;241m=\u001b[39m load_pra_categories(Path(PRA_NOTES_PATH)) \u001b[38;5;66;03m# load pra category file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m embedder, category_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_embedding_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpra_categories\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# embed pra categories\u001b[39;00m\n\u001b[1;32m      5\u001b[0m transcript_text \u001b[38;5;241m=\u001b[39m Path(TRANSCRIPT_PATH)\u001b[38;5;241m.\u001b[39mread_text() \u001b[38;5;66;03m# load the transcript text\u001b[39;00m\n\u001b[1;32m      6\u001b[0m transcript_chunks \u001b[38;5;241m=\u001b[39m chunk_text(transcript_text) \u001b[38;5;66;03m# split the transcript into smaller chunks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36mbuild_embedding_index\u001b[0;34m(pra_categories)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_embedding_index\u001b[39m(pra_categories):\n\u001b[1;32m      3\u001b[0m     embedder \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     pra_risk_embeddings \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39mencode(\u001b[43mpra_risk_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedder, np\u001b[38;5;241m.\u001b[39masarray(pra_risk_embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pra_risk_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Runner code.\n",
    "pra_categories = load_pra_categories(Path(PRA_NOTES_PATH)) # load pra category file\n",
    "embedder, category_embeddings = build_embedding_index(pra_categories) # embed pra categories\n",
    "\n",
    "transcript_text = Path(TRANSCRIPT_PATH).read_text() # load the transcript text\n",
    "transcript_chunks = chunk_text(transcript_text) # split the transcript into smaller chunks\n",
    "\n",
    "n_threads = max(4, (os.cpu_count() or 8) - 2)\n",
    "\n",
    "model = Llama(\n",
    "    model_path=str(MODEL_PATH), \n",
    "    n_ctx=4096, \n",
    "    n_gpu_layers=24, \n",
    "    chat_format='mistral-instruct', \n",
    "    n_threads=n_threads)\n",
    "\n",
    "results = []\n",
    "for i, chunk in enumerate(transcript_chunks, 1):\n",
    "    # top_categories = find_rel_categories(chunk, pra_categories, embedder, category_embeddings, top_k=TOP_K)\n",
    "    # summary_result = summarise_chunk(model, chunk, top_categories)\n",
    "    # results.append({'chunk': i, 'result': summary_result})\n",
    "    try:\n",
    "        # Use your intended TOP_K here\n",
    "        top_categories = find_rel_categories(\n",
    "            chunk, pra_categories, embedder, category_embeddings, top_k=TOP_K\n",
    "        )\n",
    "\n",
    "        summary_result = summarise_chunk(model, chunk, top_categories, i, max_evidence=5)\n",
    "\n",
    "        results.append({'chunk': i, 'result': summary_result})\n",
    "\n",
    "    except Exception as e:\n",
    "        # Hard fail-safe: still emit a valid empty structure for this chunk\n",
    "        results.append({\n",
    "            'chunk': i,\n",
    "            'result': {\n",
    "                'summary': '',\n",
    "                'evidence': ['Insufficient evidence'],\n",
    "                'pra_categories': []\n",
    "            },\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "combined_summary = ' '.join(r['result'].get('summary', '') for r in results)\n",
    "final_output = {'chunks': results, 'combined_summary': combined_summary.strip()}\n",
    "\n",
    "Path(OUTPUT_PATH).write_text(json.dumps(final_output, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1aadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Post processing script.\n",
    "# INPUT_JSON = '../notebooks/jpm_mistral_pra_summary.json' # input path\n",
    "# INPUT_PATH = pathlib.Path(INPUT_JSON)\n",
    "\n",
    "# # Output paths \n",
    "# OUTPUT_JSON = INPUT_PATH.with_suffix('.clean.json')\n",
    "# OUTPUT_CSV = INPUT_PATH.with_suffix('.csv')\n",
    "\n",
    "# # Load the raw file.\n",
    "# with open(INPUT_PATH, 'r', encoding='utf-8') as f:\n",
    "#     raw_file = json.load(f)\n",
    "\n",
    "# chunks = raw_file.get('chunks', []) # get the list of chunks\n",
    "\n",
    "# rows = [] # empty list to store cleaned rows\n",
    "\n",
    "# for row in chunks:\n",
    "#     result= row.get('result', {})\n",
    "\n",
    "#     # Handle inconsistent key names.\n",
    "#     pra_categories = result.get('pra_categories') or result.get('pra categories')\n",
    "#     evidence = result.get('evidence') or result.get('raw')\n",
    "\n",
    "#     rows.append({\n",
    "#         'chunk_id': row.get('chunk'),\n",
    "#         'summary': result.get('summary'), \n",
    "#         'pra_categories': pra_categories,\n",
    "#         'supporting_evidence': evidence\n",
    "#     })\n",
    "\n",
    "# # Save clean JSON.\n",
    "# with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # Save clean csv\n",
    "# df = pd.DataFrame(rows)\n",
    "# df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# print(\"Clean JSON saved to:\", OUTPUT_JSON)\n",
    "# print(\"CSV saved to:\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_JSON = pathlib.Path('../notebooks/jpm_mistral_pra_summary.json')\n",
    "# OUTPUT_JSON = INPUT_JSON.with_suffix(\".clean.v2.json\")\n",
    "# OUTPUT_CSV  = INPUT_JSON.with_suffix(\".v2.csv\")\n",
    "\n",
    "# def safe_loads(s: str):\n",
    "#     try:\n",
    "#         return json.loads(s)\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def pick(d, *keys):\n",
    "#     if not isinstance(d, dict): return None\n",
    "#     for k in keys:\n",
    "#         v = d.get(k)\n",
    "#         if v is not None:\n",
    "#             return v\n",
    "#     return None\n",
    "\n",
    "# raw = json.load(open(INPUT_JSON, \"r\", encoding=\"utf-8\"))\n",
    "# rows = []\n",
    "# for ch in raw.get(\"chunks\", []):\n",
    "#     res = ch.get(\"result\", {}) or {}\n",
    "#     raw_parsed = safe_loads(res.get(\"raw\")) if isinstance(res.get(\"raw\"), str) else None\n",
    "\n",
    "#     summary = pick(res, \"summary\") or pick(raw_parsed or {}, \"summary\")\n",
    "#     pra_categories = (\n",
    "#         pick(res, \"pra_categories\", \"pra categories\")\n",
    "#         or pick(raw_parsed or {}, \"pra_categories\", \"pra categories\")\n",
    "#     )\n",
    "#     evidence = pick(res, \"evidence\") or pick(raw_parsed or {}, \"evidence\")\n",
    "\n",
    "#     rows.append({\n",
    "#         \"chunk_id\": ch.get(\"chunk\"),\n",
    "#         \"summary\": summary,\n",
    "#         \"pra_categories\": pra_categories,\n",
    "#         \"supporting_evidence\": evidence,\n",
    "#     })\n",
    "\n",
    "# # Save files\n",
    "# with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# pd.DataFrame(rows).to_csv(OUTPUT_CSV, index=False)\n",
    "# print(\"Wrote:\", OUTPUT_JSON)\n",
    "# print(\"Wrote:\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9d93f",
   "metadata": {},
   "source": [
    "# **6. Evasion Scoring**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-evasion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
